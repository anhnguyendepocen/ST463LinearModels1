[
["index.html", "Notes for ST463/ST683 Linear Models 1 1 Module Preliminaries 1.1 Recommended texts 1.2 Software 1.3 R Scripts", " Notes for ST463/ST683 Linear Models 1 Katarina Domijan 2018-11-12 1 Module Preliminaries 1.1 Recommended texts There are many good resources, e.g. Weisberg (2005), Fox (2005), Fox (2016), Ramsey and Schafer (2002), Draper and Smith (1966). 1.2 Software We will use Minitab and R (R Core Team 2017). To create this document, I am using the bookdown package (Xie 2018) which was built on top of R Markdown and knitr (Xie 2015). Other packages used are: rgl (Adler, Murdoch, and others 2018), plyr (Wickham 2011), ggplot2 (Wickham 2016), scatterplot3d (Ligges and Mächler 2003), MASS (Venables and Ripley 2002), car (Fox and Weisberg 2011), datasets (R Core Team 2018), datasauRus (Locke and D’Agostino McGowan 2018) and Sleuth3 (F.L. Ramsey et al. 2016). 1.3 R Scripts R scripts for the notes can be found here References "],
["intro.html", "2 Introduction 2.1 Introductory Examples", " 2 Introduction Modeling = development of mathematical expressions that describe the behavior of a random variable of interest. This variable is called the response (or dependent) variable and denoted with \\(Y\\). Other variables which are thought to provide information on the behavior of \\(Y\\) are incorporated into the model as predictor or explanatory variables (also called the independent variables). We denote them with \\(X\\). Data consist of information taken from \\(n\\) units. Subscripts \\(i = 1,..., n\\) identify the particular unit from which the observations were taken. Additional subscripts can be used to identify different predictors. All models involve unknown constants, called parameters, which control the behavior of the model. These parameters are denoted by Greek letters (e.g. \\(\\beta\\)) and are to be estimated from the data. We denote estimates using hat notation, e.g. \\(\\hat{\\beta}\\). In this module we will study linear models. Here the parameters enter the model as simple coefficients on the \\(X\\)s or functions of \\(X\\)s. 2.1 Introductory Examples A first look at how \\(Y\\) changes as \\(X\\) is varied is seen in a scatterplot. 2.1.1 Mother and daughter heights Data from Pearson and Lee (1903). \\(\\bar{y}=\\) 63.75, sd(\\(y\\)) = 2.6 Taller mothers have taller daughters. Since most points fall above line \\(y=x\\) most daughters are taller. Does the data follow a linear pattern? If so we can use the linear regression line to summarise the data. We can use the regression line to predict a daughters height based on her mother’s height. \\(\\hat{y}=\\) 29.92 +0.54 \\(x\\) 2.1.2 Bacterial count and storage temperature Points are jittered to avoid overprinting. It does not appear to be a linear relationship. Consider a transformation? Log transformed bacteria counts appear to have a linear relationship with temperature. 2.1.3 Yield and Rainfall The dataset is from Ramsey and Schafer (2002). The data on corn yields and rainfall are in `ex0915’ in library(Sleuth3). Variables: Yield: corn yield (bushels/acre) Rainfall: rainfall (inches/year) Year: year. You must enable Javascript to view this page properly. 2.1.4 Driving Example from: Weisberg (2005). Study how fuel consumption varies over 50 US states and the District of Columbia and the effect of state gasoline tax on the consuption. Variable: FuelC: Gasoline sold for road use, thousands of gallons Drivers: Number of licensed drivers in the state Income: Per person personal income for the year 2000, in thousands of dollars Miles: Miles of Federal-aid highway miles in the state Pop: 2001 population age 16 and over Tax: Gasoline state tax rate, cents per gallon State: State name We will use a scatterplot matrix. Both Drivers and FuelC are state totals so will be larger in more populous states. Income is per person, we want to make variables comparable. Transform variables: FuelC2:FuelC/Pop Drivers2: Drivers/Pop Miles2:log\\(_2\\)(Miles) FuelC decreases as tax increases but there is a lot of variation. Fuel is weakly related to a number of other variables. Other graphical representations of the dataset: library(MASS) parcoord(driving2[, c(2,6, 1,3,4,5)]) 2.1.5 Fuel Consumption Information was recorded on fuel usage and average temperature (\\(^oF\\)) over the course of one week for eight office complexes of similar size. Data from Bowerman and Schafer (1990). We expect fuel use to be driven by weather conditions. Fuel use: response or dependent variable. Denoted by \\(Y\\). Temperature: Explanatory or predictor variable. Denoted by \\(X\\). We observe n=8 pairs: \\((x_{i}, y_{i}), i =1,...,8\\). Temp Fuel 28.0 12.4 28.0 11.7 32.5 12.4 39.0 10.8 45.9 9.4 57.8 9.5 58.1 8.0 62.5 7.5 The scatterplot shows that fuel use decreases roughly linearly as temperature increases. We assume there’s an underlying true line: \\[\\mbox{Fuel} =\\beta_{0} + \\beta_{1}\\mbox{Temp} + \\epsilon\\] or, more generally: \\(y =\\beta_{0} + \\beta_{1}x + \\epsilon.\\) The intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)), are unknown parameters and \\(\\epsilon\\) is the random error component. For each observation we have: \\(y_i =\\beta_{0} + \\beta_{1}x_i + \\epsilon_i\\). We can estimate \\(\\beta_0\\) and \\(\\beta_1\\) from the available data. One method that can be used to do this is the method of ordinary least squares. NOTE: other models are possible: References "],
["SLR.html", "3 Simple Linear regression 3.1 Ordinary least squares 3.2 The formal simple linear regression model 3.3 Simple linear regression models in R and Minitab 3.4 Statistical inference 3.5 Analysis of variance (for s.l.r.) 3.6 Sample correlation 3.7 Assessing the simple linear regression model assumptions 3.8 A note on the Galton paradox", " 3 Simple Linear regression 3.1 Ordinary least squares We have seen some introductory examples in Section 2.1.5. Fuel consumption example, what is the `best fitting line’ to summarise the linear trend? \\[y_i =\\beta_{0} + \\beta_{1}x_i + \\epsilon_i.\\] The method of ordinary least squares chooses \\(\\beta_{0}\\), \\(\\beta_{1}\\) to minimise: \\[\\begin{align*} S(\\beta_{0}, \\beta_{1}) &amp; =\\sum_{i=1}^{n}\\epsilon_i^2 \\\\ &amp; = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\\\ \\end{align*}\\] The least squares estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) must satisfy: \\(\\frac{\\delta S}{\\delta \\beta_0} = 0\\) and \\(\\frac{\\delta S}{\\delta \\beta_1} = 0\\). \\[\\begin{align*} \\frac{\\delta S}{\\delta \\beta_0} &amp; = - 2\\sum_{i=1}^{n} (y_i-\\beta_0-\\beta_1x_i) \\\\ \\frac{\\delta S}{\\delta \\beta_1} &amp; = - 2\\sum_{i=1}^{n} x_i(y_i-\\beta_0-\\beta_1x_i). \\end{align*}\\] Setting these to 0 at \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) gives: \\[\\begin{align} \\sum_{i=1}^{n} (y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i) &amp; =0 \\tag{3.1}\\\\ \\sum_{i=1}^{n} x_i(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_i)&amp; =0. \\tag{3.2} \\end{align}\\] These equations ((3.1) and (3.2)) are called the normal equations. From (3.1): \\[\\begin{align*} \\sum_{i=1}^{n} y_i-n\\hat{\\beta}_0-\\hat{\\beta}_1\\sum_{i=1}^{n}x_i &amp; =0\\\\ \\hat{\\beta}_0&amp; =\\bar{y}-\\hat{\\beta}_1\\bar{x}. \\end{align*}\\] Substitute into (3.2): \\[\\begin{align*} \\sum_{i=1}^{n}x_i( y_i-\\bar{y}+\\hat{\\beta}_1\\bar{x}-\\hat{\\beta}_1x_i) &amp; =0\\\\ \\sum_{i=1}^{n}x_i( y_i-\\bar{y}) &amp; =\\hat{\\beta}_1\\sum_{i=1}^{n}x_i(x_i-\\bar{x})\\\\ \\hat{\\beta}_1&amp; = \\frac{\\sum_{i=1}^{n}x_i( y_i-\\bar{y})}{\\sum_{i=1}^{n}x_i(x_i-\\bar{x})}\\\\ &amp; =\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})( y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\\\ &amp; =\\frac{S_{xy}}{S_{xx}}. \\end{align*}\\] Some notation: \\[\\begin{align*} S_{xx} &amp; =\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2} =\\sum_{i=1}^{n}x_{i}^{2} - n\\bar{x}^2 \\\\ S_{yy} &amp; =\\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2} =\\sum_{i=1}^{n}y_{i}^{2} - n\\bar{y}^2 \\\\ S_{xy} &amp; =\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y}) = \\sum_{i=1}^{n}x_{i}y_{i} - n\\bar{x}\\bar{y} \\end{align*}\\] So, the equation of the OLS fitted line is given by: \\[\\hat{y} =\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x,\\] where \\[\\hat{\\beta}_{1} = \\frac{S_{xy}}{S_{xx}}\\] and \\[\\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_1\\bar{x}.\\] 3.1.1 Residuals The fitted value at each observation is: \\[\\hat{y}_i =\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_i\\] The residuals are computed as: \\[e_i = y_i-\\hat{y}_i\\] 3.1.2 Some algebraic implications of the OLS fit \\(\\sum_{i=1}^n e_i = \\sum_{i=1}^n (y_i - \\hat{y}_i) = 0\\) (residuals sum to 0) \\(\\sum_{i=1}^n x_i e_i = \\sum_{i=1}^n x_i(y_i - \\hat{y}_i) = 0\\) \\(\\sum_{i=1}^n y_i = \\sum_{i=1}^n \\hat{y}_i\\) (from (3.1)) \\(\\bar{y} = \\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x}\\) (OLS line always goes through the mean of the sample) \\(\\sum_{i=1}^n \\hat{y}_ie_i = 0\\) (from (3.1) and (3.2)). 3.1.3 OLS Estimates for the Fuel Consumption Example x &lt;- Temp y &lt;- Fuel n &lt;- 8 cbind(x, y, xsq = x^2, ysq = y^2, xy = x * y) ## x y xsq ysq xy ## [1,] 28.0 12.4 784.00 153.76 347.20 ## [2,] 28.0 11.7 784.00 136.89 327.60 ## [3,] 32.5 12.4 1056.25 153.76 403.00 ## [4,] 39.0 10.8 1521.00 116.64 421.20 ## [5,] 45.9 9.4 2106.81 88.36 431.46 ## [6,] 57.8 9.5 3340.84 90.25 549.10 ## [7,] 58.1 8.0 3375.61 64.00 464.80 ## [8,] 62.5 7.5 3906.25 56.25 468.75 sum(x) ## [1] 351.8 sum(y) ## [1] 81.7 mean(x) ## [1] 43.975 mean(y) ## [1] 10.2125 sum(x^2) ## [1] 16874.76 sum(y^2) ## [1] 859.91 sum(x * y) ## [1] 3413.11 Sxx &lt;- sum(x^2) - n * mean(x)^2 Sxx ## [1] 1404.355 Syy &lt;- sum(y^2) - n * mean(y)^2 Syy ## [1] 25.54875 Sxy &lt;- sum(x * y) - n * mean(x) * mean(y) Sxy ## [1] -179.6475 Calculate \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\): \\[\\begin{align*} \\hat{\\beta}_{1} &amp; = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^{n}x_{i}y_{i} - n\\bar{x}\\bar{y}}{\\sum_{i=1}^{n}x_{i}^{2} - n\\bar{x}^{2}} \\\\ &amp; =\\frac{-179.65}{1404.355} = -0.128 \\end{align*}\\] \\(\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x} = 10.212 - ( - 0.128)(43.98) =15.84\\) The equation of the fitted line is \\(\\hat{y}= 15.84 - 0.128 x\\). 3.1.4 Interpretation of the fitted simple linear regression line: Parameter estimates -0.128 is the estimated change in mean fuel use for a 1\\(^oF\\) increase in temperature. In theory, 15.84 is the estimated mean fuel use at a temperature of 0\\(^oF\\). However, we have no reason to believe this is a good estimate because our data contains no information about the fuel-temperature relationship below 28\\(^oF\\). 3.1.5 Predicting The fitted line allows us to predict fuel use at any temperature within the range of the data. For example, at \\(x=30^oF\\): \\[\\hat{y}_i = 15.84 - 0.128 \\times 30 = 12.\\] 12 units of fuel is the estimated fuel use at \\(30^oF\\). E.g. at \\(x=40\\); \\(\\hat{y} = 10.721\\), at \\(x=50\\); \\(\\hat{y} = 9.442\\). 3.2 The formal simple linear regression model The SLR model tries to capture two features: a linear trend and fluctuations (scatter about that trend). Because of random variations in experimental conditions we do not expect to get the same value of \\(y\\) even if we keep repeating the experiment at various fixed \\(x\\) values. SLR model tries to model the scatter about the regression line. We will have to make some assumptions about the behaviour of these chance fluctuations. 3.2.1 Model The SLR model is of the form: \\[y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i}, \\hspace{0.5cm} \\epsilon_{i} \\sim N(0, \\sigma^{2}), \\hspace{0.5cm} i=1,...,n. \\] \\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters \\(y\\) and \\(\\epsilon\\) are random \\(x\\) is assumed non-random We use errors \\(\\epsilon_{i}\\) to model the chance fluctuations about the regression line (i.e. the underlying true line). So the SLR model assumes that these errors, i.e. vertical distances from the observed point to the regression line, are, on average, equal to zero. It also assumes that they are normally distributed. Another assumption is that the \\(\\epsilon_{i}\\) values are independent and identically distributed (IID). 3.2.2 Assumptions \\(\\mathbb{E}[\\epsilon_{i}] = 0\\), so \\(\\mathbb{E}[y_{i}] = \\beta_0 + \\beta_1x_i+ \\mathbb{E}[\\epsilon_i] = \\beta_0 + \\beta_1x_i\\). Var(\\(\\epsilon_i\\)) = \\(\\sigma^2\\). Equivalently Var(\\(y_{i}\\)) = \\(\\sigma^2\\). \\(\\epsilon_i\\) are independent (therefore \\(y_{i}\\) also are). \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Equivalently \\(y_i \\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\). NOTE: if \\(x_i\\) are random then the model says that \\(\\mathbb{E}[y_{i}|x_i] = \\beta_0 + \\beta_1x_i\\) and Var(\\(y_{i}|x_i\\)) = \\(\\sigma^2\\). 3.2.3 Estimation of \\(\\sigma^2\\) NOTE: \\(\\sigma^2\\) = Var(\\(\\epsilon_i\\)) The errors \\(\\epsilon_i\\) are not observable, but the residuals, \\(e_i\\) should have similar properties. We estimate \\(\\sigma^2\\) by \\[\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-2}.\\] \\(n-2\\) is the degrees of freedom and \\(\\sum_{i=1}^n e_i^2\\) is called the residual sum of squares, denoted \\(\\mbox{SSE}\\). 3.2.4 Review of some probability results Let \\(U\\), \\(W\\) and \\(Z\\) be three random variables: \\(\\mathbb{E}[U]\\) = mean of the distribution of \\(U\\) Var \\((U) = \\mathbb{E}[U^2] - (\\mathbb{E}[U])^2\\) Cov(\\(U,U\\)) = Var(\\(U\\)) Cov(\\(U, W\\)) = \\(\\mathbb{E}[UW] - \\mathbb{E}[U]\\mathbb{E}[W]\\) If \\(U\\) and \\(W\\) are uncorrelated or independent then Cov(\\(U,W\\)) = 0 \\(\\mbox{Corr}(U,W) = \\frac{\\mbox{Cov}(U,W)}{\\sqrt{\\mbox{Var}(U)\\mbox{Var}(W)}}\\) For constants \\(a\\) and \\(b\\): \\(\\mathbb{E}[aU+bW] = a\\mathbb{E}[U] + b\\mathbb{E}[W]\\) Var(\\(aU \\pm bW\\)) = \\(a^2\\)Var[\\(U\\)] + \\(b^2\\)Var[\\(W\\)] \\(\\pm\\) \\(2ab\\)Cov(\\(U\\),\\(W\\)) Cov(\\(aU+bW, cZ\\)) = \\(ac\\)Cov(\\(U\\),\\(Z\\)) + \\(bc\\)Cov(\\(W\\),\\(Z\\)) 3.2.5 Properties of the estimates When the model holds: \\(\\hat{\\beta}_1 \\sim N\\left(\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right)\\) \\(\\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right)\\) Cov\\((\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\sigma^2\\frac{\\bar{x}}{S_{xx}}\\) \\(\\hat{y} \\sim N\\left(\\beta_0 + \\beta_1x, \\sigma^2\\left(\\frac{1}{n}+ \\frac{(x-\\bar{x})^2}{S_{xx}}\\right)\\right)\\) \\((n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)}\\) \\(\\mathbb{E}[\\hat{\\sigma}^2] = \\sigma^2\\) Proof of (1): First show that \\(\\mathbb{E}[\\hat{\\beta}_1] = \\beta_1\\). \\[\\begin{align*} \\hat{\\beta}_1 &amp; = \\frac{S_{xy}}{S_{xx}} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})( y_i-\\bar{y})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})y_i}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\\\ &amp; = \\sum_{i = 1}^n a_iy_i \\end{align*}\\] where \\(a_i\\) depend only on \\(x\\) and are NOT random. By linearity of expectation: \\[\\begin{align*} \\mathbb{E}[\\hat{\\beta}_1] &amp; = \\sum_{i = 1}^n a_i\\mathbb{E}[y_i]\\\\ &amp; = \\sum_{i = 1}^n a_i (\\beta_0+\\beta_1x_i) \\mbox{ (from the model assumptions)}\\\\ &amp; = \\beta_0\\sum_{i = 1}^n a_i+\\beta_1\\sum_{i = 1}^n a_i x_i \\end{align*}\\] But \\[\\sum_{i = 1}^n a_i = \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} = 0,\\] And \\[\\begin{align*} \\sum_{i = 1}^n a_i x_i &amp; = \\frac{\\sum_{i=1}^{n}x_i(x_i-\\bar{x})}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}x_i^2-n\\bar{x}^2}{S_{xx}} \\\\ &amp; = \\frac{S_{xx}}{S_{xx}} = 1. \\end{align*}\\] So \\(\\mathbb{E}[\\hat{\\beta}_1] = \\beta_1\\) as required. Second, show that Var(\\(\\hat{\\beta}_1\\)) = \\(\\frac{\\sigma^2}{S_{xx}}\\) \\[\\begin{align*} \\mbox{Var}(\\hat{\\beta}_1) &amp; = \\mbox{Var} \\left( \\sum_{i = 1}^n a_i y_i \\right)\\\\ &amp; = \\sum_{i = 1}^n a_i^2 \\mbox{Var}(y_i) \\mbox{(since $y_i$s are independent)}\\\\ &amp; = \\sigma^2 \\sum_{i = 1}^n a_i^2\\\\ &amp; = \\sigma^2 \\sum_{i = 1}^n \\left( \\frac{x_i-\\bar{x}}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2} \\right)^2\\\\ &amp; = \\sigma^2 \\frac{\\sum_{i = 1}^n (x_i-\\bar{x})^2}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2)^2} \\\\ &amp; = \\sigma^2 \\frac{S_{xx}}{(S_{xx})^2} \\\\ &amp; = \\frac{ \\sigma^2}{S_{xx}} \\mbox{ (as required)} \\end{align*}\\] Finally, the normality assumption follows as \\(\\hat{\\beta}_1\\) is a linear combination of normal random variables (\\(y_i\\)s). Proof of (2): First show that \\(\\mathbb{E}[\\hat{\\beta}_0] = \\beta_0\\). \\[\\begin{align*} \\mathbb{E}[\\hat{\\beta}_0] &amp; = \\mathbb{E}[\\bar{y} - \\hat{\\beta}_1\\bar{x}]\\\\ &amp; = \\mathbb{E}[\\bar{y}] - \\beta_1 \\bar{x}\\\\ &amp; = \\frac{1}{n}\\sum_{i = 1}^n\\mathbb{E}[y_i] - \\beta_1 \\bar{x}\\\\ &amp; = \\frac{1}{n}\\sum_{i = 1}^n (\\beta_0+ \\beta_1 x_i) - \\beta_1 \\bar{x}\\\\ &amp; = \\frac{1}{n}( n\\beta_0 + \\beta_1 \\sum_{i = 1}^n x_i) - \\beta_1 \\bar{x}\\\\ &amp; = \\beta_0 + \\beta_1 \\bar{x} - \\beta_1 \\bar{x}\\\\ &amp; = \\beta_0 \\mbox{ (as required)} \\end{align*}\\] Second, show that Var(\\(\\hat{\\beta}_0\\)) = \\(\\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\) \\[\\begin{align*} \\mbox{Var}(\\hat{\\beta}_0) &amp; =\\mbox{Var}(\\bar{y} - \\hat{\\beta}_1\\bar{x}) \\\\ &amp; = \\mbox{Var}(\\bar{y}) + \\bar{x}^2 \\mbox{Var}(\\hat{\\beta}_1) - 2\\bar{x}\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) \\end{align*}\\] \\[\\begin{align*} \\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) &amp; =\\mbox{Cov}\\left( \\frac{1}{n}\\sum_{i = 1}^n y_i, \\sum_{i = 1}^n a_iy_i \\right)\\\\ &amp; =\\sum_{i = 1}^n \\sum_{j = 1}^n \\frac{1}{n} a_i\\mbox{Cov}(y_i, y_j)\\\\ &amp; =\\frac{1}{n} \\sum_{i = 1}^n \\sum_{j = 1}^n a_i\\mbox{Cov}(y_i, y_j)\\\\ &amp; =\\frac{1}{n} \\sum_{i = 1}^n a_i\\mbox{Cov}(y_i, y_i) \\mbox{ (since $y_i$ are indep.)} \\\\ &amp; =\\frac{\\sigma^2}{n} \\sum_{i = 1}^n a_i\\\\ &amp; = 0 \\end{align*}\\] \\[\\begin{align*} \\mbox{Var}(\\hat{\\beta}_0) &amp; = \\mbox{Var}(\\bar{y}) + \\bar{x}^2 \\mbox{Var}(\\hat{\\beta}_1) \\\\ &amp; = \\frac{1}{n^2} \\sum_{i = 1}^n \\mbox{Var}(y_i) + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}}\\\\ &amp; = \\frac{1}{n^2} n\\sigma^2 + \\bar{x}^2 \\frac{\\sigma^2}{S_{xx}}\\\\ &amp; = \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right) \\mbox{ (as required)} \\end{align*}\\] Finally, the normality assumption follows as \\(\\hat{\\beta}_0\\) is a linear combination of normal random variables (\\(y_i\\)s and \\(\\hat{\\beta}_1\\)). Proof of (3): \\[\\begin{align*} \\mbox{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; =\\mbox{Cov}(\\bar{y} - \\hat{\\beta}_1\\bar{x}, \\hat{\\beta}_1)\\\\ &amp; =\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1) - \\mbox{Cov}(\\hat{\\beta}_1\\bar{x}, \\hat{\\beta}_1) \\\\ &amp; = 0 - \\bar{x}\\mbox{Cov}(\\hat{\\beta}_1, \\hat{\\beta}_1) \\\\ &amp; = -\\bar{x} \\frac{\\sigma^2}{S_{xx}} \\end{align*}\\] Proof of (4): First show that \\(\\mathbb{E}[\\hat{y}] = \\beta_0 + \\beta_1 x\\). \\[\\begin{align*} \\mathbb{E}[\\hat{y}] &amp; = \\mathbb{E}[\\hat{\\beta}_0 + \\hat{\\beta}_1x]\\\\ &amp; = \\mathbb{E}[\\hat{\\beta}_0] + \\mathbb{E}[\\hat{\\beta}_1]x\\\\ &amp; = \\beta_0 + \\beta_1 x \\mbox{ (as required)} \\end{align*}\\] Second, show that Var(\\(\\hat{y}\\)) = \\(\\sigma^2\\left(\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{S_{xx}}\\right)\\) \\[\\begin{align*} \\mbox{Var}(\\hat{y}) &amp; =\\mbox{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\\\ &amp; =\\mbox{Var}(\\bar{y} - \\hat{\\beta}_1\\bar{x} + \\hat{\\beta}_1 x) \\\\ &amp; =\\mbox{Var}(\\bar{y} + \\hat{\\beta}_1(x - \\bar{x})) \\\\ &amp; =\\mbox{Var}(\\bar{y}) + (x - \\bar{x})^2 \\mbox{Var}(\\hat{\\beta}_1) + 2(x - \\bar{x})\\mbox{Cov}(\\bar{y}, \\hat{\\beta}_1)\\\\ &amp; =\\frac{\\sigma^2}{n} + (x - \\bar{x})^2 \\frac{\\sigma^2}{S_{xx}}\\\\ &amp; =\\sigma^2\\left(\\frac{1}{n} + \\frac{(x - \\bar{x})^2}{S_{xx}}\\right) \\quad \\mbox{ (as required)}. \\end{align*}\\] Finally, the normality assumption follows as \\(\\hat{y}\\) is a linear combination of \\(y_i\\)s. 3.2.6 Special cases At \\(x = 0\\), \\(\\hat{y} = \\hat{\\beta}_0\\). At \\(x = x_i\\), \\(\\hat{y} = \\hat{y}_i\\). \\[\\begin{align*} \\mbox{Var}(\\hat{y}_i) &amp; =\\sigma^2\\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\\\ &amp; =\\sigma^2h_{ii} \\end{align*}\\] NOTE: \\(h_{ii} = \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\) is a distance value (see later!) 3.3 Simple linear regression models in R and Minitab 3.3.1 Minitab Regression Analysis: Fuel versus Temp Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value Regression 1 22.9808 22.9808 53.69 0.000 Temp 1 22.9808 22.9808 53.69 0.000 Error 6 2.5679 0.4280 Lack-of-Fit 5 2.3229 0.4646 1.90 0.500 Pure Error 1 0.2450 0.2450 Total 7 25.5488 Model Summary S R-sq R-sq(adj) R-sq(pred) 0.654209 89.95% 88.27% 81.98% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 15.838 0.802 19.75 0.000 Temp -0.1279 0.0175 -7.33 0.000 1.00 Regression Equation Fuel = 15.838 -0.1279Temp 3.3.2 R fit &lt;- lm(Fuel~Temp) summary(fit) ## ## Call: ## lm(formula = Fuel ~ Temp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5663 -0.4432 -0.1958 0.2879 1.0560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.83786 0.80177 19.754 1.09e-06 *** ## Temp -0.12792 0.01746 -7.328 0.00033 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6542 on 6 degrees of freedom ## Multiple R-squared: 0.8995, Adjusted R-squared: 0.8827 ## F-statistic: 53.69 on 1 and 6 DF, p-value: 0.0003301 anova(fit) ## Analysis of Variance Table ## ## Response: Fuel ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Temp 1 22.9808 22.981 53.695 0.0003301 *** ## Residuals 6 2.5679 0.428 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.4 Statistical inference 3.4.1 R simulation: ParamInference.R Reminder: the linear relationship \\(\\mathbb{E}[y_{i}] = \\beta_{0} + \\beta_{1}x_{i}\\) is the underlying true line and the values of its parameters (intercept \\(\\beta_0\\) and slope \\(\\beta_1\\)) are unknown. We estimate the parameters with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). These parameter estimates have sampling distributions. #~~~~~~~~~~~~~~~ Simulation #~~~~~~~~~~~~~~~ True model parameters: beta_0 &lt;- 1 beta_1 &lt;- 2 sigma_sq &lt;- 2 # Simulate a dataset: n &lt;- 100 #number of observations in the sample x &lt;- runif(n, -5, 5) e &lt;- rnorm(n, 0, sqrt(sigma_sq)) y &lt;- beta_0 + x * beta_1 + e plot(x, y) abline(beta_0, beta_1, col = 2 ) # true model points(mean(x), mean(y), col = 3) fit &lt;- lm(y ~ x) #fit the model abline(coef(fit)[1], coef(fit)[2], col = 3) # estimated coefficients coef(fit) ## (Intercept) x ## 0.8221711 2.0056686 s &lt;- sqrt(sum(residuals(fit)^2)/(n-2)) s^2 ## [1] 2.28467 # some useful calculations for dist of estimates x_bar &lt;- mean(x) Sxx &lt;- sum(x^2) - n * (x_bar^2) var_beta_0 &lt;- sigma_sq * (1/n + x_bar^2/Sxx) var_beta_1 &lt;- sigma_sq /Sxx est_cov &lt;- -sigma_sq * x_bar/Sxx # estimated covariance from one sample se_fit &lt;- sqrt(sigma_sq * (1/n + (x - x_bar)^2/Sxx)) #~~~~~~~~~~~~~~~~~~~~ Repeat the simulation N &lt;- 500 #number of simulations estimates &lt;- matrix(0, N, 3) # to save param estimates for (i in 1:N) { x &lt;- runif(n, -5, 5) e &lt;- rnorm(n, 0, sqrt(sigma_sq)) y &lt;- beta_0 + x * beta_1 + e fit &lt;- lm(y ~ x) estimates[i, 1] &lt;- coef(fit)[1] estimates[i, 2] &lt;- coef(fit)[2] estimates[i, 3] &lt;- anova(fit)[[&quot;Mean Sq&quot;]][2] # sigamsq } plot(x,y) for (i in 1:50)abline(estimates[i, 1], estimates[i, 2], col = &quot;grey&quot;) # N is too many points #~~~~~~~~~~~~~~ plot the estimates #beta_0 hist(estimates[,1], breaks = 50, freq = FALSE, xlab = expression(hat(beta)[0]), main = expression(hat(beta)[0])) curve(dnorm(x, beta_0, sqrt(var_beta_0) ), col = 2, add = TRUE) #beta_1 hist(estimates[,2], breaks = 50, freq = FALSE, xlab = expression(hat(beta)[1]), main = expression(hat(beta)[1])) curve( dnorm(x, beta_1, sqrt(var_beta_1)) , col = 2, add = TRUE) #sigma_sq hist((n-2)/sigma_sq *estimates[,3], breaks = 50, freq = FALSE, xlab = expression(hat(sigma)^2), main = expression(hat(sigma)^2)) curve( dchisq(x, df = n-2), col = 2, add = TRUE) NOTE: superimposed curves are calculated from one (first) sample of the data and the known parameters. The distributions are from 3.2.5: \\(\\hat{\\beta}_1 \\sim N\\left(\\beta_1,\\frac{\\sigma^2}{S_{xx}}\\right)\\) \\(\\hat{\\beta}_0 \\sim N\\left(\\beta_0, \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)\\right)\\) \\((n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{(n-2)}\\) 3.4.2 Inference for \\(\\beta_1\\) 3.4.2.1 Confidence Interval \\(\\hat{\\beta}_1\\) estimates \\(\\beta_1\\), for example the change in fuel use for a 1\\(^oF\\) increase in temperature. We would like to construct a confidence interval for \\(\\beta_1\\). This will give us an interval where we are confident the true \\(\\beta_1\\) lies. The key to obtaining a C.I. is the fact that: \\[\\hat{\\beta}_1 \\sim N(\\beta_1, \\frac{\\sigma^2}{S_{xx}}).\\] Equivalently \\[\\frac{\\hat{\\beta}_{1} - \\beta_{1}}{\\sigma/\\sqrt{S_{xx}}} \\sim N(0,1).\\] And, when we replace \\(\\sigma\\) by \\(\\hat{\\sigma}\\) we have: \\[\\frac{\\hat{\\beta}_{1} - \\beta_{1}}{\\hat{\\sigma}/\\sqrt{S_{xx}}} \\sim t_{n-2}.\\] The df is \\(n-2\\) because this is the df associated with the estimate of \\(\\sigma\\). In general, when: \\[\\frac{\\mbox{Est - parameter}}{\\mbox{S.E.(Est)}} \\sim \\mbox{distribution}.\\] A C.I. for the parameter is given by: \\[\\mbox{Est} \\pm \\mbox{(quantile from distribution)} \\times \\mbox{S.E.(Est)}.\\] A \\((1-\\alpha)\\times 100\\%\\) confidence interval for \\(\\beta_{1}\\): \\[\\hat{\\beta}_{1} \\pm t_{n-2}(\\alpha/2) \\times \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}.\\] For the fuel use data, a \\(95\\%\\) C.I. for \\(\\beta_1\\) is: \\[\\begin{align*} \\hat{\\beta}_1 &amp;\\pm t_6(0.025) \\times \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}\\\\ &amp; = -0.128 \\pm 2.45 \\times 0.018\\\\ &amp; = (-0.171, -0.085) \\end{align*}\\] We are \\(95\\%\\) confident that the average fuel use drop is between 0.085 and 0.171. 3.4.2.2 Hypothesis test In some settings we may wish to test: \\(H_{0}: \\beta_{1} = 0\\) versus \\(H_{A}: \\beta_{1} \\ne 0\\). The null hypothesis here is that \\(\\mathbb{E}[y] = \\beta_0\\) i.e. \\(\\mathbb{E}[y]\\) is not linearly related to \\(x\\). Under \\(H_0\\): \\[t_{obs} = \\frac{\\hat{\\beta}_{1} - 0}{\\hat{\\sigma}/\\sqrt{S_{xx}}} \\sim t_{n-2}.\\] P-value = \\(P[T_{n-2} \\geq |t_{obs}|]\\) Reject \\(H_0\\) for small p-values, typically \\(p&lt; 0.05\\). In the fuel use example: \\[t_{obs} = \\frac{-0.128-0}{0.018} = -7.33\\] and p-value \\(&lt; 0.001\\), so we reject \\(H_0\\) and conclude that \\(\\beta_{1} \\ne 0\\). We could also test \\(H_0: \\beta_1=b\\) by computing: \\[\\frac{\\hat{\\beta}_{1} - b}{\\mbox{S.E}.(\\hat{\\beta}_{1})}.\\] 3.4.3 Inference for \\(\\beta_0\\) A \\(95\\%\\) C.I. for \\(\\beta_0\\) is: \\[\\hat{\\beta}_{0} \\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}(\\hat{\\beta}_{0}).\\] Note: \\(\\mbox{S.E.}(\\hat{\\beta}_0) = \\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)}\\) For the fuel use data: \\[\\begin{align*} &amp; = 15.84 \\pm 2.45 \\times 0.8018\\\\ &amp; = (13.88, 17.80) \\end{align*}\\] We can also test for a particular value of \\(\\beta_0\\), e.g. \\(H_0: \\beta_0 = 0\\) vs. \\(H_A: \\beta_0 \\neq 0\\) The null hypothesis here is that \\(\\mathbb{E}[y] = \\beta_1 x\\) i.e. the line passes through the origin. The test statistic is \\[\\frac{\\hat{\\beta}_{0} - \\beta_0}{\\mbox{S.E.}(\\hat{\\beta}_{0})} = 19.75.\\] for the fuel data. P-value \\(= 2P[T_6 \\geq 19.75] &lt;0.001.\\) Note: This is for illustration, in practice with this particular data we would not do this. Why? 3.4.4 Inference for mean response Suppose we want to estimate \\(\\mu=\\mathbb{E}[y]\\) at a particular value of \\(x\\). At \\(x_0\\) let: \\[\\mu_0 = \\mathbb{E}[y_0] = \\beta_0 + \\beta_1 x_0\\] We can estimate \\(\\mu_0\\) by: \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0.\\] e.g. we estimate the mean fuel use at temperature \\(50^oF\\) by \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 50 =15.84-0.128 \\times 50 =9.44.\\] A \\(95\\%\\) C.I. for \\(\\mu_0\\) is given by \\[\\begin{align*} \\mbox{Est} &amp;\\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.(Est)}\\\\ \\hat{y}_0 &amp;\\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0) \\end{align*}\\] where \\[\\begin{align*} \\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0)&amp; =\\hat{\\sigma}\\sqrt{h_{00}}\\\\ &amp; = \\hat{\\sigma}\\sqrt{\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}}\\\\ &amp; = 0.65 \\times \\sqrt{0.15}=0.254 \\end{align*}\\] The \\(95\\%\\) C.I. is \\[\\begin{align*} &amp; = 9.44 \\pm 2.45 \\times 0.254\\\\ &amp; = (8.8, 10.1) \\end{align*}\\] This interval contains the true mean fuel use at \\(50^oF\\) with \\(95\\%\\) confidence. 3.4.5 Inference for prediction Suppose we want to predict the response at a particular value of \\(x\\). At \\(x_0\\), let \\(y_0\\) be the unobserved response. From our model: \\[y_0 =\\mu_0 + \\epsilon= \\beta_0 + \\beta_1 x_0 + \\epsilon\\] and we estimate (predict) it by: \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0.\\] Note: estimation of a random variable is called prediction. In our example, we predict that the fuel use at temp \\(50^oF\\) as: \\[\\hat{y}_0 = 15.84-0.128\\times 50=9.44\\] Note that the prediction of future response equals the estimate of the mean response, however the associated standard errors (and hence confidence intervals) are different. Confidence intervals for random variables are called prediction intervals (PIs). In our prediction of \\(y_0\\) by \\(\\hat{y}_0\\) the prediction error \\(y_0-\\hat{y}_0\\) is with variance: \\[\\begin{align*} \\mbox{Var}(y_0-\\hat{y}_0)&amp; = \\mbox{Var}(y_0)+ \\mbox{Var}(\\hat{y}_0)\\mbox{ (indep as $y_0$ is out of sample)}\\\\ &amp; =\\sigma^2 + \\sigma^2\\times h_{00} \\\\ &amp; = \\sigma^2(1+h_{00})\\\\ &amp; = \\sigma^2\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right) \\end{align*}\\] The \\(\\mbox{S.E.}\\) of the prediction is then: \\[\\mbox{S.E.}_{\\mbox{pred}} (\\hat{y}_0) = \\hat{\\sigma}\\sqrt{1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}}\\] The \\(95\\%\\) prediction interval is \\[\\hat{y}_0 \\pm t_{n-2}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{pred}} (\\hat{y}_0)\\] For \\(x_0\\) = 50, \\(\\hat{y}_0 =9.442\\): \\[\\mbox{S.E.}_{\\mbox{pred}}= \\sqrt{1+0.15} \\times 0.654 = 0.702\\] So the \\(95\\%\\) P.I. is: \\[\\begin{align*} &amp; = 9.44 \\pm 2.45 \\times 0.702\\\\ &amp; = (7.72, 11.16) \\end{align*}\\] We are \\(95\\%\\) sure that the interval contains the actual fuel use on a week with temp = \\(50^oF\\). 3.5 Analysis of variance (for s.l.r.) The analysis of variance is a method for comparing the fit of two or more models to the same dataset. It is particularly useful in multiple regression. 3.5.1 ANOVA decomposition \\[\\begin{align*} \\mbox{data} &amp; = \\mbox{fit} + \\mbox{residual} \\\\ y_i &amp; = \\hat{y}_i +e_i\\\\ y_i - \\bar{y} &amp; = \\hat{y}_i - \\bar{y} +e_i \\\\ \\sum_{i = 1}^n ( y_i - \\bar{y})^2 &amp; =\\sum_{i = 1}^n(\\hat{y}_i - \\bar{y} +e_i)^2\\\\ &amp; =\\sum_{i = 1}^n(\\hat{y}_i - \\bar{y})^2 +\\sum_{i = 1}^ne_i^2 + 2\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})e_i \\end{align*}\\] The last term is zero because (from normal equations): \\(\\sum_{i = 1}^n\\hat{y}_ie_i = 0\\) and \\(\\sum_{i = 1}^ne_i = 0\\). The decomposition: \\[\\sum_{i = 1}^n ( y_i - \\bar{y})^2 =\\sum_{i = 1}^n(\\hat{y}_i - \\bar{y})^2 +\\sum_{i = 1}^ne_i^2\\] is called the ANOVA decomposition. These calculations are summarised in the ANOVA table. 3.5.2 ANOVA table In general, an ANOVA table is a method for partitioning variability in a response variable into what is explained by the model fitted and what is left over. The exact form of the ANOVA table will depend on the model that has been fitted. The hypothesis being tested by the model will also depend on the model that has been fitted. An ANOVA table for the simple linear regression model: SOURCE df SS MS F Regression 1 SSR MSR = SSR/1 MSR/MSE Error n-2 SSE MSE = SSE/(n-2) Total n-1 SST NOTE: The total sum of squares, \\(\\mbox{SST} = \\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2}\\) is the sum of squares of \\(y\\) about the mean. The total sum of squares does not depend on \\(x\\). (NB: this is \\(S_{yy}\\)) The regression sum of squares, \\(\\mbox{SSR} = \\sum_{i=1}^{n}(\\hat{y}_{i} - \\bar{y})^{2}\\). Note that \\(\\hat{y}_{i}\\) depends on \\(x\\). The residual/error sum of squares, \\(\\mbox{SSE} = \\sum_{i=1}^{n}e_{i}^{2} = \\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}\\). \\(\\mbox{SST} = \\mbox{SSR} + \\mbox{SSE}\\). The sums of squares have associated degrees of freedom (df). MS = SS/df The mean squared error \\(\\mbox{MSE}\\) estimates \\(\\sigma^{2}\\). The coefficient of determination is: \\[R^{2} = \\frac{\\mbox{SSR}}{\\mbox{SST}} = 1 - \\frac{\\mbox{SSE}}{\\mbox{SST}}.\\] \\(R^{2}\\) is always between 0 and 1 and it measures the proportion of variation in \\(y\\) that is explained by regression with \\(x\\). 3.5.3 Special cases \\(R^{2}=1\\), \\(\\mbox{SSR} = \\mbox{SST}\\), \\(\\mbox{SSE}\\) = 0. \\(e_{i}=0\\), \\(i = 1, \\dots,n\\), data fall on a straight line. \\(R^{2}=0\\), \\(\\mbox{SSR} = 0\\), \\(\\mbox{SSE} = \\mbox{SST}\\). \\(\\hat{y}_{i}=\\bar{y}\\), \\(\\hat{\\beta}_1=0\\). 3.5.4 Does regression on x explain y? In simple linear regression this amounts to testing: \\[\\begin{align*} H_0&amp;:&amp; \\beta_1 = 0\\\\ H_A&amp;:&amp; \\beta_1 \\ne 0 \\end{align*}\\] We can use a t-test for this, but there is an equivalent test based on the F distribution. As we will see later, F-tests have a wide range of applications. If \\(H_0\\) holds, then \\(\\mbox{SSR}\\) is small and \\(\\mbox{SSE}\\) large. Therefore large values of \\(\\mbox{SSR}\\) relative to \\(\\mbox{SSE}\\) provide evidence against \\(H_0\\). The F-statistic is: \\[F=\\frac{\\mbox{SSR}/df_R}{\\mbox{SSE}/df_E}.\\] where \\(df_R=1\\) is the degrees of freedom of \\(\\mbox{SSR}\\) and \\(df_E=n-2\\) is the degrees of freedom of \\(\\mbox{SSE}\\). Under \\(H_0\\),\\(F \\sim F_{1,n-2}\\). By dividing each SS by the \\(df\\) we put them on a common scale, so that if \\(H_0\\) is true: \\[\\mbox{SSR}/1 \\approx \\mbox{SSE}/(n-2)\\] and \\[F_{obs}\\approx 1.\\] Large values of \\(F_{obs}\\) provide evidence against \\(H_0\\). P-value: \\(P( F_{1,n-2} \\geq F_{obs})\\). 3.5.5 Notes on the ANOVA table \\(\\mathbb{E}[\\mbox{MSR}] = \\sigma^2 + \\beta_1^2 S_{xx}\\). Proof: \\[\\begin{align*} \\mbox{MSR} &amp; = \\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2/1\\\\ &amp; = \\sum_{i = 1}^n (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i - \\bar{y})^2\\\\ &amp; = \\sum_{i = 1}^n (\\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1x_i - \\bar{y})^2\\\\ &amp; =\\hat{\\beta}_1^2\\sum_{i = 1}^n (x_i - \\bar{x})^2\\\\ &amp; =\\hat{\\beta}_1^2 S_{xx} \\end{align*}\\] \\[\\begin{align*} \\mathbb{E}[\\mbox{MSR}] &amp; = \\mathbb{E}[\\hat{\\beta}_1^2 S_{xx}]\\\\ &amp; =S_{xx} \\mathbb{E}[\\hat{\\beta}_1^2]\\\\ &amp; = S_{xx} \\left(\\mbox{Var}(\\hat{\\beta}_1) + \\mathbb{E}[\\hat{\\beta}_1]^2 \\right)\\\\ &amp; = S_{xx} \\left(\\frac{\\sigma^2}{S_{xx}} + \\beta_1^2 \\right)\\\\ &amp; = \\sigma^2 + \\beta_1^2 S_{xx} \\end{align*}\\] \\(\\mathbb{E}[\\mbox{MSE}] = \\sigma^2\\). Proof: \\[\\begin{align*} \\mbox{MSE} &amp; = \\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n-2}\\\\ &amp; = \\frac{\\sum_{i = 1}^ne_i^2}{n-2} \\end{align*}\\] \\[\\begin{align*} \\mathbb{E}[\\mbox{MSE}] &amp; = \\frac{1}{n-2} \\mathbb{E}\\left[\\sum_{i = 1}^ne_i^2 \\right]\\\\ &amp; = \\frac{1}{n-2} \\sum_{i = 1}^n\\mathbb{E}[e_i^2]\\\\ &amp; = \\frac{1}{n-2} \\sum_{i = 1}^n\\left( \\mbox{Var}(e_i) + \\mathbb{E}[e_i]^2 \\right) \\end{align*}\\] NOTE: \\(\\mathbb{E}[\\epsilon_i] = 0\\), \\(\\mbox{Var}(\\epsilon_i) = \\sigma^2\\), but \\(\\mathbb{E}[e_i] = 0\\), \\(\\mbox{Var}(e_i)= \\sigma^2(1- h_{ii})\\). We will revisit this later in the course. \\[\\begin{align*} \\mathbb{E}[\\mbox{MSE}] &amp; = \\frac{1}{n-2} \\sum_{i = 1}^n \\left( \\sigma^2( 1 - h_{ii}) + 0 \\right)\\\\ &amp; =\\frac{1}{n-2} \\sum_{i = 1}^n \\sigma^2 \\left(1-\\left(\\frac{1}{n} +\\frac{(x_i - \\bar{x})^2}{S_{xx}}\\right)\\right)\\\\ &amp; =\\frac{1}{n-2} \\sum_{i = 1}^n \\left(\\sigma^2 - \\frac{\\sigma^2}{n} -\\frac{\\sigma^2 (x_i - \\bar{x})^2}{S_{xx}}\\right)\\\\ &amp; =\\frac{1}{n-2} \\left(\\sigma^2 n- \\sigma^2 -\\frac{\\sigma^2 }{S_{xx}}\\sum_{i = 1}^n(x_i - \\bar{x})^2\\right)\\\\ &amp; = \\frac{1}{n-2} \\left((n-2) \\sigma^2\\right)\\\\ &amp; =\\sigma^2 \\end{align*}\\] Under the \\(H_0\\), \\(\\beta_1 = 0\\) and then \\(\\mathbb{E}[\\mbox{MSE}] = \\mathbb{E}[\\mbox{MSR}]\\). \\(\\mbox{MSE} = \\hat{\\sigma}^2\\) can be computed using the formula: \\[\\hat{\\sigma}^2 = \\frac{S_{yy} - \\hat{\\beta}_1^2S_{xx}}{n-2}.\\] 3.6 Sample correlation The relationship between \\(X\\) and \\(Y\\) can be examined using a scatterplot \\((x_i, y_i)\\). Sample correlation measures the strength and direction of the linear association between \\(X\\) and \\(Y\\). It is defined as: \\[r = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\] \\(r\\) is the estimate of the population correlation (\\(\\rho\\)) between \\(X\\) and \\(Y\\). 3.6.0.1 Connection between correlation and regression: \\(\\hat{\\beta}_1=\\sqrt{\\frac{\\mbox{SST}}{S_{xx}}}r=\\frac{s_y}{s_x}r\\) where \\(s_y\\) and \\(s_x\\) are the standard deviations of the \\(y_i\\)’s and \\(x_i\\)’s. Note that if you change the role of \\(X\\) and \\(Y\\) the resulting regression line would have slope \\(\\frac{s_x}{s_y}r\\). \\(r^2 =R^2\\) the coefficient of determination. In the SLR model variable \\(x\\) is treated as fixed and \\(y\\) and \\(\\epsilon\\) are random. It is convenient to think of the predictor variable as fixed even if it israndom as we are interested in the behaviour of \\(y\\) at various fixed \\(x\\) values. One can calculate \\(r\\) for any pair of variables (see next page), but correlation assumes that variables are bivariately normally distributed. Whereas correlation treats both variables symmetrically, in regression, the exploratory variable is used to explain or predict the response variable. Father and son heights (Galton data) 3.6.1 Examples of correlation Anscombe data In all graphs below, correlation is \\(r = -0.06\\). 3.6.2 Comparison of the correlation and coefficient of determination for two data sets. cor(X1,Y1)^2 ## [1] 0.6699889 summary(lm(Y1 ~X1))[8] ## $r.squared ## [1] 0.6699889 cor(X2,Y2)^2 ## [1] 0.6895371 summary(lm(Y2 ~X2))[8] ## $r.squared ## [1] 0.6895371 3.7 Assessing the simple linear regression model assumptions 3.7.1 Assumptions In the SLR model, we assume that \\(y_{i} \\sim\\) N(\\(\\beta_{0} + \\beta_{1}x_{i}, \\sigma^{2}\\)) and that the \\(y_{i}\\)’s are independent. Equivalently, since \\(\\epsilon_{i} = y_{i} - \\beta_{0} - \\beta_{1}x_{i}\\), the SLR model assumes that \\(\\epsilon_{i} \\sim\\) N(0, \\(\\sigma^{2}\\)) and the \\(\\epsilon_{i}\\)’s are independent and identically distributed. We want to check the following: There is a linear relationship, i.e. \\(\\mathbb{E}\\)[\\(y_{i}\\)] = \\(\\beta_{0} + \\beta_{1}x_{i}\\). If the data do not follow a linear relationship then the simple linear regression model is not appropriate. The \\(\\epsilon_{i}\\)’s have a constant variance, i.e. Var(\\(\\epsilon_{i}\\)) = \\(\\sigma^{2}\\) for all \\(i\\). If there is not constant variance, the line will summarise the data okay but the parameter estimate standard errors, estimates of \\(\\sigma\\) etc, are all based on incorrect assumptions. The \\(\\epsilon_{i}\\)’s are independent. The \\(\\epsilon_{i}\\)’s are normally distributed (with mean 0). 3.7.2 Violations and consequences Linearity: A straight linear relationship may be inadequate. A straight linear relationship may only be appropriate for most of the data. When linearity is violated least squares estimates can be biased and standard errors may be inaccurate. Constant variance: When the variance is not constant least squares estimate are unbiased but standard errors are inaccurate. Independence: When there is a lack of independence least squares estimates are unbiased but standard errors are seriously affected. Normality: Violations of normality do not have much impact on estimates and standard errors. Tests and C.I.’s are not usually seriously affected because of the C.L.T. 3.7.3 Graphical tools for assessment Plot of \\(y_i\\) versus \\(x_i\\). If satisfactory use simple linear regression. Sometimes the patterns in the plot of \\(y_i\\) versus \\(x_i\\) are difficult to detect because of the total variability of the response variable is much larger than the variability around the regression line. Scatterplots of residuals vs fits are better at finding patterns because the linear component of the variation in the responses has been removed, leaving a clearer picture about curvature and spread. The plot alerts the user of nonlinearity, non-constant variance and the presence of outliers. Plot of \\(e_i\\) versus \\(\\hat{y}_i\\) (or \\(x_i\\)). If satisfactory use simple linear regression. If linearity is violated but \\(\\mathbb{E}[y]\\) is monotonic in \\(x\\) and \\(\\mbox{Var}(y)\\) is constant, try transforming \\(x\\) and then use simple linear regression. If linearity is violated and \\(\\mathbb{E}[y]\\) is not monotonic, try quadratic regression \\(\\mathbb{E}[y] = \\beta_0+\\beta_1 x+\\beta_2 x^2\\) (we will look at this later). If linearity is violated and \\(\\mbox{Var}(y)\\) increases with \\(\\mathbb{E}[y]\\), try transforming y and then use simple linear regression. If the distribution of \\(y\\) about \\(\\mathbb{E}[y]\\) is skewed, i.e. non-normal, then use simple linear regression but report skewness. If linearity is not violated but \\(\\mbox{Var}(y)\\) increases with \\(\\mathbb{E}[y]\\), use weighted regression (we will look at this later). Normal probability plot The model assumes normality of \\(y\\) about \\(\\mathbb{E}[y]\\), or, equivalently, normality of \\(\\epsilon\\). The residuals \\(e_i\\) approximate \\(\\epsilon\\) and should therefore have a normal distribution. The normal probability (quantile) plot is a plot of \\(z_i\\) versus \\(e_i\\), where \\(z_i\\) are quantiles from the standard normal distribution. This plot should roughly follow a straight line pattern. Residuals vs. time order If the data are collected over time, serial correlation or a general time trend may occur. A plot of \\(e_i\\) vs. \\(t_i\\) (time of the i\\(^{th}\\) observation) may be examined for patterns. Everytime you use SLR you should also draw graphs 1) to 3). Also plot 4) when appropriate. 3.7.4 Cigarette Data FDA data on cigarettes, response is carbon monoxide, predictor is nicotine. Data from McIntyre (1994). summary(fit) ## ## Call: ## lm(formula = carbon.monoxide ~ nicotine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3273 -1.2228 0.2304 1.2700 3.9357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6647 0.9936 1.675 0.107 ## nicotine 12.3954 1.0542 11.759 3.31e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.828 on 23 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8512 ## F-statistic: 138.3 on 1 and 23 DF, p-value: 3.312e-11 #studres(fit) To assess the fit construct residual plots Plot 1: residuals increasing with the fit, non constant variance. Plot 2: no indication that the assumption of Normality is unreasonable. There are 3 unusual observations: 3, 19, 25. Obs 3 has a large negative residual. It is the point on the upper right of the fitted line plot. It is a high influence point, meaning it has a big effect on the fitted line obtained. Obs 19 and 25 have large positive residuals. We could attempt to improve the fit, refit the model without observation 3. Diagnostic plots: Plot 1: no linear pattern, but some hint of non-constant variance. 3.7.4.1 Transformations: How can we pick the best transformation? Examine the fitted line plot: linearity, constant variance. Examine the residual vs fit plot: no relationship, constant variance, no outliers. Check the normality of residuals. Check for sensitivity: whether fit would change substantially if extreme points are removed. One can also compare \\(R^2\\) as long as the \\(y\\) values are on the same scale. Furthermore, \\(R^2\\) doesn’t follow a distribution, so we can’t compare \\(R^2\\) in two models and know that one is meaningfully better. Note: interpretation changes after transformations. Row 1: delete outlier Row 2: use a square root transformation for the predictor. This diminishes the influence of the outlier. The residual plot hints at a small amount of bias. Row 3: take square root transformations of both the response and the predictor. Row 4: take log transformations of both the response and the predictor. 3.8 A note on the Galton paradox 3.8.1 The Galton paradox Sons are on average taller than their fathers (by 1 inch approx) apply(father.son, 2, mean) ## father son ## 67.68683 68.68423 Taller than average fathers have taller than average sons. Regression towards the mean: although the above is true, for these tall people, the son’s height was on average less than the father’s. The suggestion is that each generation would have offspring more near average than the previous generation and that over many generations the offspring would be of uniform heigth. However, the observations showed the sons as variable as the fathers. apply(father.son, 2, sd) ## father son ## 2.745827 2.816194 An apparent paradox? 3.8.2 Two regressions Regressing \\(y\\) on \\(x\\), treats \\(x\\) variable as fixed and only vertical distances are minimized. Howevever, regressing \\(x\\) on \\(y\\), i.e. trying to predict the fathers’ heights from their sons’ treats the sons’ heights \\(y\\) as fixed and the least squares criterion minimizes the horizontal distances. 3.8.3 Regression vs orthogonal regression References "],
["multiple-regression.html", "4 Multiple regression 4.1 Introductory examples 4.2 Least squares estimation for multiple regression 4.3 Prediction from multiple linear regression model 4.4 Regression models in matrix notation: examples 4.5 The formal multiple regression model and properties 4.6 The hat matrix 4.7 ANOVA for multiple regression 4.8 1-way ANOVA model 4.9 One way ANOVA in regression notation 4.10 Confidence intervals and hypothesis tests for linear combinations of \\(\\boldsymbol{\\beta}\\)", " 4 Multiple regression 4.1 Introductory examples Setup: response variable \\(y\\), predictors \\(x_1\\), \\(x_2\\), …, \\(x_k\\). 4.1.1 Example 1: Fuel Use Example from Section 2. Information was recorded on fuel usage and average temperature (\\(^oF\\)) over the course of one week for eight office complexes of similar size. Data are from Bowerman and Schafer (1990). \\(y\\) = fuel use, \\(x_1\\) = temperature, \\(x_2\\) = chill index. Data: Temp Fuel Chill 28.0 12.4 18 28.0 11.7 14 32.5 12.4 24 39.0 10.8 22 45.9 9.4 8 57.8 9.5 16 58.1 8.0 1 62.5 7.5 0 We wish to use \\(x_1\\) and \\(x_2\\) to predict \\(y\\). This should give more accurate predictions than either \\(x_1\\) or \\(x_2\\) alone. A multiple linear regression model is: fuel use \\(\\approx\\) a linear function of temperature and chill index. More precisely: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon.\\] As before, \\(\\epsilon\\) is the unobserved error, \\(\\beta_0, \\beta_1, \\beta_2\\) are the unknown parameters. When \\(\\mathbb{E}[\\epsilon ] = 0\\) we have \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2.\\] In SLR we can check model appropriateness by plotting \\(y\\) vs \\(x\\) and observing whether the points fall close to a line. Here we could construct a 3-d plot of \\(y\\), \\(x_1\\), \\(x_2\\) and points should fall close to a plane. For a given set of values of \\(x_1\\) and \\(x_2\\), say \\(x_1 = 45.9\\) and \\(x_2 = 8\\), the model says that the mean fuel use is: \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 \\times 45.9 + \\beta_2 \\times 8.\\] If \\(x_1 = x_2 = 0\\) then \\(\\mathbb{E}[y] = \\beta_0\\), the model intercept. To interpret \\(\\beta_1\\) suppose \\(x_1 = t\\) and \\(x_2 = c\\). Then \\[\\mathbb{E}[y]=\\beta_0 + \\beta_1 \\times t + \\beta_2 \\times c.\\] Now suppose \\(x_1\\) increases by \\(1\\) and \\(x_2\\) stays fixed: \\[\\mathbb{E}[y]=\\beta_0 + \\beta_1 \\times (t + 1) + \\beta_2 \\times c.\\] Substracting these we find that \\(\\beta_1\\) is the increase in \\(\\mathbb{E}[y]\\) associated with 1 unit increase in \\(x_1\\) for a fixed \\(x_2\\). I.e. two weeks having the same chill index but whose temperature differed by \\(1^o\\) would have a mean fuel use difference of \\(\\beta_1\\). 4.1.2 Example 2: Categorical predictors Suppose we wish to predict the fuel efficiency of different car types. Data are from Cryer and Miller (1991). We have data on: \\(y\\) = gallons per mile (gpm), \\(x_1\\) = car weight (w), \\(x_2\\) = transmission type (ttype): 1 = automatic or 0 = manual. We use the model \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2.\\] \\(\\beta_0\\) = the mean gpm for cars of weight \\(w = 0\\) and ttype = manual. \\(\\beta_1\\) = change in mean gpm when weight increases by 1 for the same ttype. \\(\\beta_2\\) = change in mean gpm when the car of the same weight is changed from manual to automatic. The model says that: \\[\\begin{align*} \\mathbb{E}[y] &amp; = \\beta_0 + \\beta_1 x_1 \\quad \\mbox{ for manual}\\\\ &amp; = \\beta_0 + \\beta_2 + \\beta_1 x_1 \\quad \\mbox{ for automatic.} \\end{align*}\\] Therefore we are fitting two lines with different intercepts but the same slope. The data should look like: Suppose the data look like this: This suggests we should fit two lines with different intercepts and different slopes. We introduce a third predictor: \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2,\\] giving: \\[\\begin{align*} \\mathbb{E}[y] &amp; = \\beta_0 + \\beta_1 x_1 \\quad \\mbox{ for manual}\\\\ &amp; = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 \\quad \\mbox{ for automatic.} \\end{align*}\\] The term \\(x_1x_2\\) is called an interaction term. Here: \\(\\beta_2\\) = difference in intercept \\(\\beta_3\\) = difference in slope. 4.1.3 Example 3: Polynomials We have one predictor \\(x\\) but the plot of \\(y\\) vs \\(x\\) exhibits a quadratic pattern. Then we can fit a multiple regression model: \\[\\mathbb{E}[y] = \\beta_0 + \\beta_1 x + \\beta_2 x^2.\\] This is also called a quadratic regression model or, more generally, a polynomial regression model. Higher order polynomial regression models can also be used if needed. You must enable Javascript to view this page properly. Link: http://www.rpubs.com/kdomijan/333155 4.1.4 Example 4: Nonlinear relationships For example, \\[y = \\alpha x_1 ^{\\beta x_2} \\epsilon.\\] Nonlinear models can sometimes be linearized, for example: \\[log(y) = log(\\alpha) + \\beta x_2 log(x_1) + log(\\epsilon).\\] Here: \\(x = x_2 log(x_1)\\). NOTE: the term linear refers to the linearity of regression parameters. A general form for multiple linear regression model (with two explanatory variables): \\[y = \\beta_0 f_0(x_1, x_2) + \\beta_1 f_1(x_1, x_2) + \\beta_2 f_2(x_1, x_2) + \\dots\\] where \\(f_j(x_1, x_2)\\) are known functions of explanatory variables. The extension to more than two explanatory variables is straightforward. 4.1.5 Cigarette Data continued Data from 3.7.4. Consider a second predictor (weight): Regression (nicotine only) summary(fit) ## ## Call: ## lm(formula = carbon.monoxide ~ nicotine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3273 -1.2228 0.2304 1.2700 3.9357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6647 0.9936 1.675 0.107 ## nicotine 12.3954 1.0542 11.759 3.31e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.828 on 23 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8512 ## F-statistic: 138.3 on 1 and 23 DF, p-value: 3.312e-11 Regression (weight only) summary(fit2) ## ## Call: ## lm(formula = carbon.monoxide ~ weight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.524 -2.533 0.622 2.842 7.268 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11.795 9.722 -1.213 0.2373 ## weight 25.068 9.980 2.512 0.0195 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.289 on 23 degrees of freedom ## Multiple R-squared: 0.2153, Adjusted R-squared: 0.1811 ## F-statistic: 6.309 on 1 and 23 DF, p-value: 0.01948 Regression (both predictors) fit3 &lt;- lm(carbon.monoxide ~ weight + nicotine) summary(fit3) ## ## Call: ## lm(formula = carbon.monoxide ~ weight + nicotine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.3304 -1.2249 0.2314 1.2677 3.9371 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.61398 4.44663 0.363 0.720 ## weight 0.05883 5.02395 0.012 0.991 ## nicotine 12.38812 1.24473 9.952 1.32e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.87 on 22 degrees of freedom ## Multiple R-squared: 0.8574, Adjusted R-squared: 0.8444 ## F-statistic: 66.13 on 2 and 22 DF, p-value: 4.966e-10 Regression (quadratic) nicotine.sq &lt;- nicotine^2 fit4 &lt;- lm(carbon.monoxide ~ nicotine + nicotine.sq) summary(fit4) ## ## Call: ## lm(formula = carbon.monoxide ~ nicotine + nicotine.sq) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9857 -1.1052 0.1834 0.8654 3.4145 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.784 1.453 -1.227 0.23264 ## nicotine 20.111 2.775 7.248 2.92e-07 *** ## nicotine.sq -3.730 1.267 -2.945 0.00749 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.583 on 22 degrees of freedom ## Multiple R-squared: 0.8977, Adjusted R-squared: 0.8884 ## F-statistic: 96.53 on 2 and 22 DF, p-value: 1.284e-11 4.2 Least squares estimation for multiple regression Our model states that: \\[y = \\beta_0 + \\beta_1x_{1} + \\beta_2x_{2} + ... + \\beta_kx_k + \\epsilon,\\] where \\(k&lt;n\\). For each observation we have: \\[y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + ... + \\beta_kx_{ik} + \\epsilon_i.\\] We can write this more compactly using matrix notation. Let \\(\\mathbf{Y}\\) be the response vector: \\[\\mathbf{Y} =\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots\\\\ y_{n} \\end{bmatrix}\\] Let \\(\\mathbf{X}\\) be the \\(n \\times p\\) matrix, where \\(p = k+1\\): \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; \\dots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; \\dots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \\dots &amp; x_{nk} \\end{bmatrix}\\] Let \\(\\boldsymbol{\\beta}\\) be the \\(p\\)-dim parameter vector: \\[\\boldsymbol{\\beta} =\\begin{bmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots\\\\ \\beta_{k} \\end{bmatrix}\\] Let \\(\\boldsymbol{\\epsilon}\\) be the \\(n\\)-dim error vector: \\[\\boldsymbol{\\epsilon} =\\begin{bmatrix} \\epsilon_{1} \\\\ \\epsilon_{2} \\\\ \\vdots\\\\ \\epsilon_{n} \\end{bmatrix}\\] The model states that: \\[\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\] The vector of fitted values is: \\[\\hat{\\mathbf{Y}}=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}.\\] The corresponding residual values are: \\[\\mathbf{e}=\\mathbf{Y}-\\hat{\\mathbf{Y}}.\\] The OLS estimates minimise: \\[S(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik})^2\\] over \\(\\boldsymbol{\\beta}\\). Therefore the OLS estimates satisfy: \\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_j} = 0, \\quad \\forall j\\] and as before we evaluate at \\(\\hat{\\boldsymbol{\\beta}}\\) \\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_0} = -2 \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik})\\] \\[\\frac{\\delta S(\\boldsymbol{\\beta})}{\\delta \\beta_j} = -2 \\sum_{i=1}^{n} x_{ij}(y_i-\\beta_0-\\beta_1x_{i1}- ... - \\beta_kx_{ik}), \\quad \\forall j = 1,...,k.\\] The OLS estimates of \\(\\boldsymbol{\\beta}\\) satisfy: \\[\\sum_{i=1}^{n}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}- ... - \\hat{\\beta}_kx_{ik}) = 0\\] and \\[\\sum_{i=1}^{n}(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}- ... - \\hat{\\beta}_kx_{ik})x_{ij} = 0, \\quad \\forall j = 1,...,k.\\] These normal equations (see (3.1) and (3.2)) can be written as: \\[\\sum_{i=1}^{n}e_i = 0\\] and \\[\\sum_{i=1}^{n}x_{ij}e_i = 0, \\quad \\forall j = 1,...,k.\\] We can combine this into one matrix equation: \\[\\mathbf{X}^T\\mathbf{e}= \\mathbf{0}\\] or equivalently: \\[\\mathbf{X}^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})= \\mathbf{0}\\] Therefore the OLS estimator \\(\\hat{\\boldsymbol{\\beta}}\\) satisfies: \\[\\begin{align} \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} &amp;= \\mathbf{X}^T\\mathbf{Y}\\\\ \\hat{\\boldsymbol{\\beta}} &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\tag{4.1} \\end{align}\\] Matrix \\(\\mathbf{X}^T\\mathbf{X}\\) is non-singular (i.e has an inverse) iff \\(rank(\\mathbf{X}) =p\\), i.e. \\(\\mathbf{X}\\) has full rank and the columns of \\(\\mathbf{X}\\) are linearly independent. 4.2.1 Estimation of \\(\\sigma^2\\) = Var\\((\\epsilon)\\) A point estimate of \\(\\sigma^2\\) is the mean squared error: \\[\\hat{\\sigma}^2 = \\mbox{MSE} = \\frac{\\mbox{SSE}}{n-p} = \\frac{\\sum_{i=1}^n e_i^2}{n-p}.\\] 4.2.2 Estimation of Var\\((\\hat{\\beta})\\) \\[\\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^T\\mathbf{X})^{-1} \\sigma^2.\\] \\[\\widehat{\\mbox{Var}(\\hat{\\boldsymbol{\\beta}})} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\hat{\\sigma}^2.\\] 4.3 Prediction from multiple linear regression model As we have seen already, to predict from a multiple regression model we use: \\[\\hat{y}_i = \\hat{\\beta}_0+ \\hat{\\beta}_1x_{i1}+ \\cdots+\\hat{\\beta}_kx_{ik}\\] or \\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\] At a particular set of \\(x_0\\) values we predict the response \\(y_0\\) by: \\[\\hat{y}_0 = \\mathbf{x}_0^T\\hat{\\boldsymbol{\\beta}}\\] where \\(\\mathbf{x}_0^T = ( 1, x_{01},x_{02},..., x_{0k})\\). We also use \\(\\hat{y}_0\\) to estimate \\(\\mathbb{E}(y_0)\\), the mean of \\(y_0\\) at a given set of \\(x_0\\) values. The \\(\\mbox{S.E.}\\) for the estimate of the mean \\(\\mathbb{E}(y_0)\\) is: \\[\\mbox{S.E.}_{\\mbox{fit}} (\\hat{y}_0)= \\hat{\\sigma}\\sqrt{\\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0}.\\] A \\(1-\\alpha\\) confidence interval for the expected response at \\(\\mathbf{x}_0\\) is given by: \\[\\hat{y}_0 \\pm t_{n-p}(\\alpha/2) \\times \\mbox{S.E.}_{\\mbox{fit}} (\\hat{y}_0).\\] The \\(\\mbox{S.E.}\\) for the predicted \\(y_0\\): \\[\\mbox{S.E.}_{\\mbox{pred}}(\\hat{y}_0) = \\hat{\\sigma}\\sqrt{1+ \\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0}.\\] Note: \\[\\mbox{S.E.}_{\\mbox{pred}}(\\hat{y}_0)= \\sqrt{\\hat{\\sigma}^2+\\mbox{S.E.}_{\\mbox{fit}}(\\hat{y}_0)^2}\\] 4.4 Regression models in matrix notation: examples 4.4.1 Example 1: SLR The \\(\\mathbf{X}\\) matrix is: \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1}\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp;x_{n} \\end{bmatrix}\\] To estimate the coefficients \\(\\hat{\\boldsymbol{\\beta}}\\): \\[\\begin{align*} \\mathbf{X}^T\\mathbf{X} &amp;= \\begin{bmatrix} n &amp; \\sum x_{i}\\\\ \\sum x_{i}&amp; \\sum x_{i}^2 \\end{bmatrix}\\\\ (\\mathbf{X}^T\\mathbf{X})^{-1} &amp; = \\frac{1}{n \\sum x_{i}^2 - (\\sum x_{i})^2}\\begin{bmatrix} \\sum x_{i}^2&amp; -\\sum x_{i}\\\\ -\\sum x_{i} &amp; n \\end{bmatrix} \\\\ &amp; = \\frac{1}{n (\\sum x_{i}^2 - n\\bar{x}^2)}\\begin{bmatrix} \\sum x_{i}^2 &amp; -n\\bar{x} \\\\ -n\\bar{x} &amp; n \\end{bmatrix} \\\\ &amp; = \\frac{1}{S_{xx}}\\begin{bmatrix} \\sum x_{i}^2/n &amp; -\\bar{x} \\\\ -\\bar{x} &amp; 1 \\end{bmatrix} \\\\ \\mathbf{X}^T\\mathbf{Y} &amp;= \\begin{bmatrix} \\sum y_{i} \\\\ \\sum x_{i}y_{i} \\end{bmatrix} = \\begin{bmatrix} n\\bar{y} \\\\ \\sum x_{i}y_{i} \\end{bmatrix}\\\\ \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\ &amp; = \\frac{1}{S_{xx}}\\begin{bmatrix} \\bar{y}\\sum x_{i}^2 -\\bar{x} \\sum x_{i}y_i \\\\ -n \\bar{x} \\bar{y} + \\sum x_{i}y_i \\end{bmatrix} \\end{align*}\\] With some algebra, this gives: \\[\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}\\] and \\[\\hat{\\beta}_0= \\bar{y} - \\hat{\\beta}_1\\bar{x}\\] as before, and \\[\\begin{align*} \\mbox{Var}(\\hat{\\boldsymbol{\\beta}})&amp; = (\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2\\\\ &amp; = \\frac{\\sigma^2}{S_{xx}} \\begin{bmatrix} \\sum x_{i}^2/n&amp; -\\bar{x} \\\\ -\\bar{x}&amp; 1 \\end{bmatrix} \\end{align*}\\] which gives \\[\\mbox{Var}(\\hat{\\beta}_0) = \\sigma^2\\left(\\frac{1}{n}+ \\frac{\\bar{x}^2}{S_{xx}}\\right),\\] \\[\\mbox{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{S_{xx}},\\] \\[\\mbox{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\bar{x}\\frac{\\sigma^2}{S_{xx}}.\\] 4.4.2 Example 2 Example from Stapleton (2009), Problem 3.1.1, pg 81. A scale has 2 pans. The measurement given by the scale is the difference between the weight in pan 1 and pan 2, plus a random error \\(\\epsilon\\). Suppose that \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mbox{Var}(\\epsilon) = \\sigma^2\\) and the \\(\\epsilon_i\\) are independent. Suppose also that two objects have weight \\(\\beta_1\\) and \\(\\beta_2\\) and that 4 measurements are taken: Pan 1: object 1, Pan 2: empty Pan 1: empty, Pan 2: object 2 Pan 1: object 1, Pan 2: object 2 Pan 1: object 1 and 2, Pan 2: empty Let \\(y_1\\), \\(y_2\\), \\(y_3\\) and \\(y_4\\) be the four observations. Then: \\[\\begin{align*} y_1 &amp; = \\beta_1 + \\epsilon_1\\\\ y_2 &amp; =- \\beta_2 + \\epsilon_2\\\\ y_3 &amp; = \\beta_1 - \\beta_2 + \\epsilon_3\\\\ y_4 &amp; = \\beta_1 + \\beta_2 + \\epsilon_4\\\\ \\end{align*}\\] \\(\\mathbf{X} = \\begin{bmatrix} 1 &amp;0 \\\\ 0 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix}\\) \\(\\mathbf{Y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}\\) \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix}\\) \\(\\boldsymbol{\\epsilon} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\end{bmatrix}\\) The model is: \\[\\mathbf{Y} = \\begin{bmatrix} 1 &amp;0 \\\\ 0 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1 \\end{bmatrix} \\times\\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\boldsymbol{\\epsilon} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] The OLS estimates of \\(\\boldsymbol{\\beta}\\) are given by: \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\] \\[\\begin{align*} \\mathbf{X}^T\\mathbf{X} &amp; = \\begin{bmatrix} 1 &amp; 0 &amp; 1 &amp; 1\\\\ 0 &amp; -1 &amp; -1 &amp; 1\\\\ \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1\\\\ \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 3 \\\\ \\end{bmatrix}\\\\ &amp; = 3\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix}\\\\ \\mathbf{X}^T\\mathbf{Y} &amp;= \\begin{bmatrix} y_1 + y_3 + y_4\\\\ -y_2 - y_3 + y_4\\\\ \\end{bmatrix}\\\\ \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}\\\\ &amp; = \\frac{1}{3}\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} y_1 + y_3 + y_4\\\\ -y_2 - y_3 + y_4\\\\ \\end{bmatrix}\\\\ &amp; = \\frac{1}{3}\\begin{bmatrix} y_1 + y_3 + y_4\\\\ -y_2 - y_3 + y_4\\\\ \\end{bmatrix}\\\\ \\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1} \\sigma^2 = \\frac{1}{3}\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{bmatrix} \\sigma^2.\\\\ \\end{align*}\\] Can we improve the experiment so that the 4 measurements yield estimates of \\(\\boldsymbol{\\beta}\\) with smaller variance? present design: \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{3}\\) Let \\(\\mathbf{X} = \\begin{bmatrix} 1 &amp; -1 \\\\ 1 &amp; -1 \\\\ 1 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix}\\), \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{4}\\) Let \\(\\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}\\), \\(\\mbox{Var}(\\hat{\\beta}_i) = \\frac{\\sigma^2}{2}\\). 4.5 The formal multiple regression model and properties 4.5.1 Concepts: random vectors, covariance matrix, multivariate normal distribution (MVN). Let \\(Y_1,...,Y_n\\) be r.v.s defined on a common probability space. Then \\(\\mathbf{Y}\\) is a random vector. Let \\(\\mu_i = \\mathbb{E}[y_i]\\) and \\(\\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_n \\end{bmatrix}\\). Then \\(\\boldsymbol{\\mu}\\) is a mean vector and we write: \\[\\mathbb{E}[\\mathbf{Y}] = \\boldsymbol{\\mu}.\\] Let \\(\\sigma_{ij} = Cov(y_i, y_j)\\). Then \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix of \\(\\mathbf{Y}\\), where \\(\\boldsymbol{\\Sigma}_{ij} = [\\sigma_{ij}].\\) We write: \\[\\mbox{Var}(\\mathbf{Y}) = \\boldsymbol{\\Sigma}.\\] Aside: \\(\\mbox{Cov}(Y_i,Y_j) = \\mathbb{E}[(Y_i-\\mathbb{E}[Y_i])(Y_j-\\mathbb{E}[Y_j])]\\) \\(\\mbox{Cov}(Y_i,Y_i) = \\mbox{Var}(Y_i)\\) If \\(Y_i,Y_j\\) are independent then \\(\\mbox{Cov}(Y_i,Y_j) = 0\\). When \\(Y_i,Y_j\\) have bivariate normal distribution, if \\(\\mbox{Cov}(Y_i,Y_j) = 0\\), then \\(Y_i,Y_j\\) are independent. Fact: Suppose \\(\\mathbf{Y}\\) has mean \\(\\boldsymbol{\\mu}\\) and variance \\(\\boldsymbol{\\Sigma}\\). Then for a vector of constants \\(\\mathbf{b}\\) and matrix of constants \\(\\mathbf{C}\\): \\[\\mathbb{E}[\\mathbf{C}\\mathbf{Y} + \\mathbf{b}] = \\mathbf{C}\\boldsymbol{\\mu} + \\mathbf{b}\\] and \\[\\mbox{Var}( \\mathbf{C}\\mathbf{Y} + \\mathbf{b} ) = \\mathbf{C}\\boldsymbol{\\Sigma} \\mathbf{C}^T.\\] Defn: A random \\(n\\) - dim vector \\(\\mathbf{Y}\\) is said to have a MVN distribution if \\(\\mathbf{Y}\\) can be written as \\[\\mathbf{Y} = \\mathbf{A}\\mathbf{Z} + \\boldsymbol{\\mu}\\] where: \\(Z_1, Z_2, ..., Z_p\\) are iid N(0,1), \\(\\mathbf{A}\\) is an \\(n \\times p\\) matrix of constants and \\(\\boldsymbol{\\mu}\\) is an \\(n\\) vector of constants. Notes: Random vector \\(\\mathbf{Z}\\) is multivariate normal with mean \\(\\mathbf{0}\\) and covariance \\(\\mathbf{I}_p\\) since \\(Z_i\\)s are independent so their covariances are 0. We write: \\[\\mathbf{Z} \\sim N_p(\\mathbf{0}, \\mathbf{I}_p).\\] \\(\\mathbb{E}[\\mathbf{Y}] = \\mathbb{E}[\\mathbf{A}\\mathbf{Z} + \\boldsymbol{\\mu}] = \\boldsymbol{\\mu}\\), \\(\\mbox{Var}(\\mathbf{Y}) = \\mathbf{A}\\mathbf{A}^T\\), \\(\\mathbf{Y} \\sim N_n (\\boldsymbol{\\mu}, \\mathbf{A}\\mathbf{A}^T)\\). 4.5.2 Multiple regression model \\[\\mathbf{Y}=\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\\] \\(\\mathbf{Y}=\\) \\(n\\) - dimensional response random vector. \\(\\boldsymbol{\\beta}=\\) unknown \\(p\\) - dimensional parameter vector. \\(\\mathbf{X}=\\) an \\(n \\times p\\) matrix of constants. \\(\\boldsymbol{\\epsilon}=\\) \\(n\\) - dimensional error vector. Assumptions: Linearity: \\(\\mathbb{E}[\\boldsymbol{\\epsilon} ] =\\mathbf{0}\\), hence \\(\\mathbb{E}[\\mathbf{Y}] = \\mathbf{X}\\boldsymbol{\\beta}\\). Constant variance and 0 covariances \\(\\mbox{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 I_n\\) and \\(\\mbox{Var}(\\mathbf{Y}) = \\sigma^2 I_n\\). MVN distribution: \\(\\boldsymbol{\\epsilon} \\sim N_n(\\mathbf{0},\\sigma^2 I_n )\\) and \\(\\mathbf{Y} \\sim N_n(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2 I_n )\\) Notes: When the off diagonal entries of the covariance matrix of a MVN distribution are 0, the \\(Y_1, ..., Y_n\\) are independent. Theorem 4.1 Let \\(\\hat{\\boldsymbol{\\beta}}\\) be the OLS estimator of \\(\\boldsymbol{\\beta}\\). When the model assumptions hold: \\[\\hat{\\boldsymbol{\\beta}} \\sim N_p(\\boldsymbol{\\beta}, (\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2)\\] Corollary: \\(\\hat{\\beta}_j \\sim N(\\beta_j, c_{jj}\\sigma^2)\\), where \\(c_{jj}\\) is the \\(jj\\) entry of \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) for \\(j = 0, ..., k\\). Theorem 4.2 Let \\(\\hat{\\sigma}^2 = \\frac{\\mbox{SSE}}{n-p}\\). When the model assumptions hold: \\[(n-p)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi ^2_{(n-p)}\\] and \\[\\mathbb{E}[\\hat{\\sigma}^2] =\\sigma^2\\] The distribution of \\(\\hat{\\sigma}^2\\) is independent of \\(\\hat{\\boldsymbol{\\beta}}\\). Corollary: \\[\\frac{\\hat{\\beta}_j - \\beta_j }{\\hat{\\sigma} \\sqrt{c_{jj}}} \\sim t_{n-p}\\] So we can do tests and obtain CIs for \\(\\beta_j\\) 4.6 The hat matrix The vector of fitted values: \\[\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y} = \\mathbf{H}\\mathbf{Y}.\\] The hat matrix (also known as the projection matrix): \\[\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\] has dimension \\(n \\times n\\) is symmetric (\\(\\mathbf{H}^T = \\mathbf{H}\\)) and is idempotent (\\(\\mathbf{H}^2 = \\mathbf{H}\\mathbf{H} = \\mathbf{H}\\)). We have \\[\\hat{\\mathbf{Y}}= \\mathbf{H}\\mathbf{Y}\\] \\[\\mathbf{e}= \\mathbf{Y} - \\hat{\\mathbf{Y}} =\\mathbf{Y} - \\mathbf{H}\\mathbf{Y} = (\\mathbf{I} - \\mathbf{H})\\mathbf{Y}\\] \\[\\mbox{SSE} = \\mathbf{e}^T\\mathbf{e} = \\mathbf{Y}^T (\\mathbf{I} - \\mathbf{H})\\mathbf{Y}\\] 4.6.1 The QR Decomposition of a matrix We have seen that OLS estimates for \\(\\boldsymbol{\\beta}\\) can be found by using: \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{Y}.\\] Inverting the \\(\\mathbf{X}^T\\mathbf{X}\\) matrix can sometimes introduce significant rounding errors into the calculations and most software packages use QR decomposition of the design matrix \\(\\mathbf{X}\\) to compute the parameter estimates. E.g. take a look at the documentation for the lm method in R. How does this work? We need to find an \\(n \\times p\\) matrix \\(\\mathbf{Q}\\) and a \\(p \\times p\\) matrix \\(\\mathbf{R}\\) such that: \\[\\mathbf{X}=\\mathbf{Q}\\mathbf{R}\\] and \\(\\mathbf{Q}\\) has orthonormal columns, i.e. \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}_p\\) \\(\\mathbf{R}\\) is an upper triangular matrix. There are several methods for computing the \\(\\mathbf{Q}\\mathbf{R}\\) factorization (we won’t study them, but high quality code for the computation exists in publicly available Lapack package {http://www.netlib.org/lapack/lug/}). We can show that: \\[\\begin{align*} \\mathbf{X} &amp;=\\mathbf{Q}\\mathbf{R} \\\\ \\mathbf{X}^T\\mathbf{X} &amp;=(\\mathbf{Q}\\mathbf{R})^T(\\mathbf{Q}\\mathbf{R}) = \\mathbf{R}^T\\mathbf{R}\\\\ \\end{align*}\\] Then: \\[\\begin{align*} (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} &amp; =\\mathbf{X}^T\\mathbf{Y}\\\\ (\\mathbf{R}^T\\mathbf{R})\\hat{\\boldsymbol{\\beta}} &amp; =\\mathbf{R}^T\\mathbf{Q}^T\\mathbf{Y}\\\\ \\mathbf{R}\\hat{\\boldsymbol{\\beta}} &amp; = \\mathbf{Q}^T\\mathbf{Y}\\\\ \\end{align*}\\] Since \\(\\mathbf{R}\\) is a triangular matrix we can use backsolving and this is an easy equation to solve. We can also show that the hat matrix becomes: \\[\\mathbf{H} = \\mathbf{Q}\\mathbf{Q}^T\\] 4.7 ANOVA for multiple regression Recap: ANOVA decomposition \\[\\begin{align*} \\mbox{SSR} &amp; = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2 = \\sum_{i=1}^n \\hat{y}_i ^2- n\\bar{y}^2 \\\\ \\mbox{SSE} &amp; = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n e_i^2\\\\ \\mbox{SST} &amp; = \\sum_{i=1}^n(y_i - \\bar{y})^2 = \\sum_{i=1}^n y_i ^2- n\\bar{y}^2 \\\\ \\end{align*}\\] Theorem 4.3 \\(\\mbox{SST} = \\mbox{SSR} + \\mbox{SSE}\\) Proof: this follows from the decomposition of response = fit + residual. \\[\\begin{align*} \\mathbf{Y} &amp; = \\hat{\\mathbf{Y}} + \\mathbf{e}\\\\ \\mathbf{Y}^T\\mathbf{Y} &amp; = (\\hat{\\mathbf{Y}} + \\mathbf{e})^T (\\hat{\\mathbf{Y}} + \\mathbf{e})\\\\ &amp; = \\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} + \\mathbf{e}^T\\mathbf{e}+ 2\\hat{\\mathbf{Y}}^T\\mathbf{e} \\\\ \\end{align*}\\] But \\(\\hat{\\mathbf{Y}}^T = \\hat{\\boldsymbol{\\beta}}^T\\mathbf{X}^T\\) and \\(\\mathbf{X}^T\\mathbf{e} = 0\\), from normal equations, so \\(\\hat{\\mathbf{Y}}^T\\mathbf{e} = 0\\). Alternatively: \\(\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}\\), \\(\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{Y}\\), so \\(\\hat{\\mathbf{Y}}^T\\mathbf{e} = \\mathbf{Y}^T\\mathbf{H}(\\mathbf{I} - \\mathbf{H})\\mathbf{Y} = 0\\), since \\(\\mathbf{H}^2=\\mathbf{H}\\). Therefore, \\[\\mathbf{Y}^T\\mathbf{Y} = \\hat{\\mathbf{Y}}^T\\hat{\\mathbf{Y}} + \\mathbf{e}^T\\mathbf{e}\\] \\[\\sum_{i=1}^n y_i^2 =\\sum_{i=1}^n \\hat{y}_i^2 + \\sum_{i=1}^n e_i^2\\] and substracting \\(n\\bar{y}^2\\) from both sides completes the proof. ANOVA table: \\[\\begin{align*} \\mbox{SSR} &amp; = \\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2, \\;\\;\\;\\; df =p-1 \\\\ \\mbox{SSE} &amp; = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2, \\;\\;\\;\\; df = n-p \\\\ \\mbox{SST} &amp; = \\sum_{i=1}^n(y_i - \\bar{y})^2, \\;\\;\\;\\; df =n-1. \\end{align*}\\] SOURCE df SS MS F Regression p-1 SSR MSR = SSR/(p-1) MSR/MSE Error n-p SSE MSE = SSE/(n-p) Total n-1 SST If \\(\\beta_1 = \\beta_2 = ... = \\beta_k = 0\\) then \\(\\hat{\\beta}_j \\approx 0\\) for \\(j = 1,...,k\\) and \\(\\hat{y}_i \\approx \\bar{y}\\). Then, \\(\\mbox{SSE} \\approx \\mbox{SST}\\) and \\(\\mbox{SSR} \\approx 0\\). Small values of \\(\\mbox{SSR}\\) relative to \\(\\mbox{SSE}\\) provide indication that \\(\\beta_1 = \\beta_2 = ... = \\beta_k = 0\\). \\[\\begin{align*} &amp;H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\\\ &amp;H_A: \\mbox{ not all }\\beta_j = 0 \\mbox{ for }j = 1,...,k \\end{align*}\\] Under \\(H_0\\): \\[F= \\frac{\\mbox{SSR}/(p-1)}{\\mbox{SSE}/(n-p)} \\sim F_{(p-1, n-p)}\\] P-value is \\(P( F_{(p-1, n-p)} \\geq F_{obs})\\), where \\(F_{obs}\\) is the observed \\(F\\)-value. Coefficient of determination \\(R^2 = \\frac{\\mbox{SSR}}{\\mbox{SST}}\\), \\(0 \\leq R^2 \\leq 1\\). \\(R^2\\) is the proportion of variability in \\(Y\\) explained by regression on \\(X_1,...,X_k\\). Adjusted \\(R^2\\) is the modified version of \\(R^2\\) adjusted for the number of predictors in the model. R uses: \\[R^2_{Adj} = 1-(1- R^2)\\frac{n-1}{n-p-1}.\\] 4.8 1-way ANOVA model 4.8.1 Example: A study was carried out to examine the effects of caffeine. Thirty students were randomly assigned to one of: control, no caffeine low dose caffeine low dose caffeine plus sugar The response \\(y\\) is an index measuring unrest 2 hrs later. (Example from Draper and Smith (1966).) Let \\(y_{ij}\\) be the response for the \\(j^{th}\\) person in the \\(i^{th}\\) group, \\(j=1,...,10\\), \\(i=1,2,3\\). Let \\(n_i\\) be the number assigned to group \\(i\\). Model: \\(y_{ij} = \\mu_{i} + \\epsilon_{ij}\\), \\(\\quad\\) \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\), where \\(\\mu_i\\) is the population mean for those at dose \\(i\\). Or equivalently: \\(y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij}\\), \\(\\quad\\) \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\), where \\(\\mu\\) is the overall population mean and \\(\\alpha_i\\) is the effect of receiving treatment \\(i\\). O.L.S estimates for Model 1 are: \\[\\begin{align*} S(\\mu_1, ..., \\mu_g)&amp; =\\sum_{i=1}^g\\sum_{j=1}^{n_i}\\epsilon_{ij}^2 = \\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\mu_i)^2,\\\\ \\frac{\\delta S(\\mu_1, ..., \\mu_g)}{\\delta \\mu_i}&amp; = -2\\sum_{j=1}^{n_i}(y_{ij}-\\mu_i), \\quad \\forall i = 1,...,g \\\\ \\end{align*}\\] Setting these equal to 0 and evaluating at \\(\\hat{\\mu}_i\\) gives: \\[\\begin{align*} \\sum_{j=1}^{n_i}(y_{ij}-\\hat{\\mu}_i) &amp; =0.\\\\ \\sum_{j=1}^{n_i}y_{ij}-n_i\\hat{\\mu}_i &amp; =0.\\\\ \\hat{\\mu}_i =\\sum_{j=1}^{n_i}y_{ij}/n_i &amp; =\\bar{y}_{i.}\\\\ \\end{align*}\\] NOTE: \\(\\bar{y}_{i.}\\) is the average of responses at level \\(i\\) of \\(X\\). Model 1 has \\(g=3\\) parameters but model 2 has 4 parameters and is over-parameterised (\\(\\mu_i = \\mu + \\alpha_{i}\\)). Usually the constraint \\(\\sum \\alpha_i = 0\\) or \\(\\alpha_3 = 0\\) is imposed. The hypothesis of interest in this model is: \\[\\begin{align*} &amp; H_0: \\mu_1=\\mu_2 = ...= \\mu_g\\\\ &amp; H_A: \\mbox{not all } \\mu_i \\mbox{ are the same.}\\\\ \\end{align*}\\] Equivalently: \\[\\begin{align*} &amp; H_0: \\alpha_i=0, \\hspace{1cm}\\forall i = 1,...,g\\\\ &amp; H_A: \\mbox{not all } \\alpha_i = 0.\\\\ \\end{align*}\\] Calculations can be summarised in the ANOVA table: SOURCE df SS MS F Group g-1 SSG MSG = SSG/(g-1) MSG/MSE Error n-g SSE MSE = SSE/(n-g) Total n-1 SST where: \\(\\mbox{SSG} = \\sum_{i = 1}^gn_{i}(\\bar{y}_{i.} - \\bar{y}_{..})^{2}\\) \\(\\mbox{SSE} = \\sum_{i=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{i.})^{2}\\) \\(\\mbox{SST} = \\sum_{i=1}^g\\sum_{j = 1}^{n_i}(y_{ij} - \\bar{y}_{..})^{2}\\) Under \\(H_{0}\\): \\[F_{obs} = \\frac{\\mbox{MSG}}{\\mbox{MSE}} \\sim F_{g-1, n-g}.\\] We reject \\(H_{0}\\) for large values of \\(F_{obs}\\). 4.9 One way ANOVA in regression notation First we have to set up dummy variables: \\[X_i= \\{ \\begin{array}{ll} 1 &amp; \\quad\\mbox{if observation in gp }i\\\\ 0 &amp; \\quad\\mbox{ow}\\end{array}\\] Model: (effects model \\(y_{ij} = \\mu + \\alpha_{i} + \\epsilon_{ij}\\)) \\[Y = \\mu + \\alpha_1X_1 + \\alpha_2X_2 + \\alpha_3X_3 + \\epsilon\\] \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] where \\(\\mathbf{Y}\\) is a \\(30 \\times 1\\) vector of responses and \\[\\boldsymbol{\\beta} = \\begin{bmatrix} \\mu \\\\ \\alpha_{1} \\\\ \\alpha_{2} \\\\ \\alpha_{3} \\end{bmatrix} \\quad \\quad \\quad \\mathbf{X} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] Note that in the \\(\\mathbf{X}\\) matrix the first column equals the sum of the second, third and fourth columns, therefore it is not of full rank so \\(\\mathbf{X}^T\\mathbf{X}\\) does not have an inverse and there is not unique \\(\\hat{\\boldsymbol{\\beta}}\\). We could require \\(\\sum\\alpha_i = 0\\) and then the solution would be unique. Or, we could require that \\(\\alpha_3 = 0\\) and drop the last column of \\(\\mathbf{X}\\). We could also derive a solution where the first column of \\(\\mathbf{X}\\) is omitted. Then the model becomes: \\[Y = \\mu_1X_1 + \\mu_2X_2 + \\mu_3X_3 + \\epsilon\\] \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\] where \\[\\boldsymbol{\\beta} =\\begin{bmatrix} \\mu_{1} \\\\ \\mu_{2}\\\\ \\mu_{3} \\end{bmatrix}\\quad \\quad \\quad \\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] This is the means model \\(y_{ij} = \\mu_i + \\epsilon_{ij}\\). \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\ &amp; = \\begin{bmatrix} 10 &amp; 0 &amp; 0 \\\\ 0 &amp; 10 &amp; 0 \\\\ 0 &amp; 0 &amp; 10 \\end{bmatrix}^{-1}\\begin{bmatrix} \\sum_{j=1}^{10} y_{1j} \\\\ \\sum_{j=1}^{10} y_{2j}\\\\ \\sum_{j=1}^{10} y_{3j} \\end{bmatrix}\\\\ &amp; = \\frac{1}{10}\\mathbf{I}_3\\begin{bmatrix} y_{1.} \\\\ y_{2.}\\\\ y_{3.} \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} \\bar{y}_{1.} \\\\ \\bar{y}_{2.}\\\\ \\bar{y}_{3.} \\end{bmatrix}\\\\ &amp; = \\begin{bmatrix} \\hat{\\mu}_{1} \\\\ \\hat{\\mu}_{2}\\\\ \\hat{\\mu}_{3} \\end{bmatrix} \\end{align*}\\] The fitted values are then: \\[\\hat{Y} = \\hat{\\mu}_1X_1 + \\hat{\\mu}_2X_2 + \\hat{\\mu}_3X_3\\] or \\[\\hat{Y} = \\hat{\\mu}_i = \\bar{y}_{i.}\\] if \\(Y\\) comes from group \\(i\\). 4.9.1 Fitting the model in MTB and R One way anova, \\(Y\\) is response \\(X\\) is group. Using regression, make dummy variables \\(X_1\\), \\(X_2\\), \\(X_3\\) and: use predictors \\(X_1\\), \\(X_2\\), \\(X_3\\) with no intercept, or use predictors \\(X_1\\), \\(X_2\\) with intercept. The second regression on dummy variables gives the same ANOVA table as the one way ANOVA table. When the model does not include an intercept, the ANOVA table shows the uncorrected SS, i.e. \\(\\bar{y}\\) not substracted: SOURCE df SS Regression p SSR Error n-p SSE Total n SST Where \\(SSR = \\sum \\hat{y}^2\\), \\(SSE = \\sum(y - \\hat{y})^{2}\\) and \\(SST = \\sum y^{2}\\). 4.9.1.1 Example: Caffeine in MTB In this example x is a factor, not a continuous variable, x1, x2 and x3 are the dummy variables that identify the groups in x. One-way ANOVA: y versus x Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value x 2 61.40 30.700 6.18 0.006 Error 27 134.10 4.967 Total 29 195.50 Model Summary S R-sq R-sq(adj) R-sq(pred) 2.22860 31.41% 26.33% 15.32% Means x N Mean StDev 95% CI 1 10 244.800 2.394 (243.354, 246.246) 2 10 246.400 2.066 (244.954, 247.846) 3 10 248.300 2.214 (246.854, 249.746) Pooled StDev = 2.22860 Regression Analysis: y versus x1, x2, x3 The following terms cannot be estimated and were removed: x3 Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 2 61.40 30.700 6.18 0.006 x1 1 43.35 43.350 8.73 0.006 x2 1 18.05 18.050 3.63 0.067 Error 27 134.10 4.967 Total 29 195.50 Model Summary S R-sq R-sq(adj) R-sq(pred) 2.22860 31.41% 26.33% 15.32% Coefficients Term Coef SE Coef T-Value P-Value Constant 248.3 0.705 352.33 0.000 x1 -3.50 0.997 -3.51 0.002 x2 -1.90 0.997 -1.91 0.067 Regression Equation y = 248.300 - 3.500 x1 - 1.900 x2 \\(X_3\\) is dropped. Compare ANOVA table with the 1-way ANOVA results. It is also possible to fit the model without explicitly specifying the dummy variables, but specifing that \\(x\\) is a categorical variable. The same model is fitted, but \\(X_1\\) is dropped. Regression Analysis: y versus x1, x2, x3 Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 3 1822929 607643 122344.22 0.000 x1 1 599270 599270 120658.47 0.000 x2 1 607130 607130 122240.86 0.000 x3 1 616529 616529 124133.34 0.000 Error 27 134 5 Total 30 1823063 Model Summary S R-sq R-sq(adj) R-sq(pred) 2.22860 99.99% 99.99% 99.99% Coefficients Term Coef SE Coef T-Value P-Value VIF x1 244.800 0.705 347.36 0.000 1.00 x2 246.400 0.705 349.63 0.000 1.00 x3 248.300 0.705 352.33 0.000 1.00 Regression Equation y = 244.800 x1 + 246.400 x2 + 248.300 x3 Model with intercept = 0. 4.9.1.2 Example: Caffeine in R #Tell R this is a categorical variable coffee.data$x &lt;- as.factor(coffee.data$x) fit.oneway.anova &lt;- aov(y~x, data = coffee.data) summary(fit.oneway.anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 61.4 30.700 6.181 0.00616 ** ## Residuals 27 134.1 4.967 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fit.oneway.anova) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 61.4 30.7000 6.1812 0.006163 ** ## Residuals 27 134.1 4.9667 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 model.tables(fit.oneway.anova, &quot;means&quot;) ## Tables of means ## Grand mean ## ## 246.5 ## ## x ## x ## 1 2 3 ## 244.8 246.4 248.3 # plot(fit.oneway.anova) #diagnostic plots coefficients(fit.oneway.anova) ## (Intercept) x2 x3 ## 244.8 1.6 3.5 #TukeyHSD(fit.oneway.anova, &quot;x&quot;, ordered = TRUE) #plot(TukeyHSD(fit.oneway.anova, &quot;x&quot;)) fit.coffee &lt;- lm(y~x, data = coffee.data) summary(fit.coffee) ## ## Call: ## lm(formula = y ~ x, data = coffee.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 244.8000 0.7047 347.359 &lt; 2e-16 *** ## x2 1.6000 0.9967 1.605 0.12005 ## x3 3.5000 0.9967 3.512 0.00158 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.3141, Adjusted R-squared: 0.2633 ## F-statistic: 6.181 on 2 and 27 DF, p-value: 0.006163 anova(fit.coffee) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 2 61.4 30.7000 6.1812 0.006163 ** ## Residuals 27 134.1 4.9667 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # or using dummy variables d1 &lt;- as.numeric(coffee.data$x == 1) d2 &lt;- as.numeric(coffee.data$x == 2) d3 &lt;- as.numeric(coffee.data$x == 3) fit.coffee.dummy &lt;- lm(coffee.data$y ~ d1 +d2) summary(fit.coffee.dummy) ## ## Call: ## lm(formula = coffee.data$y ~ d1 + d2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 248.3000 0.7047 352.326 &lt; 2e-16 *** ## d1 -3.5000 0.9967 -3.512 0.00158 ** ## d2 -1.9000 0.9967 -1.906 0.06730 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.3141, Adjusted R-squared: 0.2633 ## F-statistic: 6.181 on 2 and 27 DF, p-value: 0.006163 #no intercept fit.coffee.dummy2 &lt;- lm(coffee.data$y ~ d1 +d2 +d3 - 1) summary(fit.coffee.dummy2) ## ## Call: ## lm(formula = coffee.data$y ~ d1 + d2 + d3 - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.400 -2.075 -0.300 1.675 3.700 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## d1 244.8000 0.7047 347.4 &lt;2e-16 *** ## d2 246.4000 0.7047 349.6 &lt;2e-16 *** ## d3 248.3000 0.7047 352.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.229 on 27 degrees of freedom ## Multiple R-squared: 0.9999, Adjusted R-squared: 0.9999 ## F-statistic: 1.223e+05 on 3 and 27 DF, p-value: &lt; 2.2e-16 4.10 Confidence intervals and hypothesis tests for linear combinations of \\(\\boldsymbol{\\beta}\\) From the theory of OLS: \\[\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta},(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2)\\] and \\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} \\sim N(\\mathbf{c}^T\\boldsymbol{\\beta},\\mathbf{c}^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{c}\\sigma^2)\\] For the caffeine example (1 way ANOVA model): suppose we want to compare the treatment means with the control mean, that is, we want a CI for: \\[\\frac{\\mu_2+\\mu_3}{2}-\\mu_1\\] Let \\(\\mathbf{c}^T= (-1, 1/2, 1/2)\\), \\(\\boldsymbol{\\beta}^T = (\\mu_1, \\mu_2, \\mu_3)\\). \\[\\mathbf{c}^T\\boldsymbol{\\beta} = -\\mu_1+\\mu_2/2+ \\mu_3/2\\] is estimated by: \\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} = -\\bar{y}_{1.}+\\bar{y}_{2.}/2+ \\bar{y}_{3.}/2\\] and the variance is: \\[\\begin{bmatrix} -1 &amp; 1/2 &amp; 1/2 \\end{bmatrix} \\frac{1}{10}\\mathbf{I}_3 \\begin{bmatrix} -1 \\\\ 1/2 \\\\ 1/2\\end{bmatrix}\\sigma^2 = \\frac{3}{20}\\sigma^2\\] So, the \\(100 \\times (1- \\alpha) \\%\\)CI is: \\[\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}} \\pm t_{27}(\\alpha/2) \\sqrt{\\frac{3}{20}\\hat{\\sigma}^2}\\] The df is: \\(n-g = 30-3 = 27\\). We could also test hypotheses e.g. \\(H_o: \\mathbf{c}^T\\boldsymbol{\\beta} = 0\\). The test statistic: \\[\\frac{\\mathbf{c}^T\\hat{\\boldsymbol{\\beta}}}{\\sqrt{\\frac{3}{20}\\hat{\\sigma}^2}} \\sim t_{27}.\\] References "],
["model-comparisons-and-testing-for-lack-of-fit.html", "5 Model comparisons and testing for lack of fit 5.1 F-tests for comparing two models 5.2 Sequential sums of squares 5.3 Testing for lack of fit 5.4 Added variable plots 5.5 Visualising Models in Hdim: added variable plots for the bodyfat data.", " 5 Model comparisons and testing for lack of fit 5.1 F-tests for comparing two models 5.1.1 Example: Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) i.e. in Model A, \\(\\beta_2=\\beta_3=0\\). Model A is the reduced or simpler model and model B is the full model. The \\(\\mbox{SSE}\\) for Model B will be smaller than the \\(\\mbox{SSE}\\) for Model A but is the reduction enough to justify the two extra parameters? We have: Model A: \\[\\mbox{SST} = \\mbox{SSR}(A) + \\mbox{SSE}(A)\\] Model B: \\[\\mbox{SST} = \\mbox{SSR}(B) + \\mbox{SSE}(B)\\] Note: \\[\\mbox{SSE}(A)-\\mbox{SSE}(B)=\\mbox{SSR}(B)-\\mbox{SSR}(A)\\] 5.1.2 F-test to compare models: Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + ... + \\beta_q x_q\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k\\) where \\(q&lt;k\\) and Model A is nested within Model B. \\(H_0\\): \\(\\beta_{q+1} = \\beta_{q+2} = ... = \\beta_k = 0\\) \\(H_A\\): At least one \\(\\beta_{q+1}, ... , \\beta_k \\neq 0.\\) \\[F =\\frac{(\\mbox{SSE}(A)-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}.\\] Under \\(H_0\\), \\[F \\sim F_{(k-q),(n-p)},\\] where \\(p = (k+1).\\) Note: Equivalently, the F-test can be written as: \\[F =\\frac{(\\mbox{SSR}(B)-\\mbox{SSR}(A))/(k-q)}{\\mbox{SSE}(B)/(n-p)}.\\] Note: Models A and B must be hierarchical for the F-test to be valid. 5.1.3 Example: Steam data This data is from a study undertaken to understand the factors that caused energy consumption in detergent manufacturing over a 25 month period. Example from Draper and Smith (1966). The data variables are: y = STEAM Pounds of steam used monthly. x1 = TEMP Average atmospheric temperature (\\(^o\\)F). x2 = INV Inventory: pounds of real fatty acid in storage per month. x3 = PROD Pounds of crude glycerin made. x4 = WIND Average wind velocity (in mph). x5 = CDAY Calendar days per month. x6 = OPDAY Operating days per month. x7 = FDAY Days below \\(32^o\\)F. x8 = WIND2 Average wind velocity squared. x9 = STARTS Number of production start-ups during the month. Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) where \\(x_1\\) = TEMP, \\(x_2\\) = INV, \\(x_3\\) = PROD. modelA &lt;- lm(STEAM ~ TEMP) modelB &lt;- lm(STEAM ~ TEMP + INV + PROD) summary(modelA) ## ## Call: ## lm(formula = STEAM ~ TEMP) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6789 -0.5291 -0.1221 0.7988 1.3457 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.62299 0.58146 23.429 &lt; 2e-16 *** ## TEMP -0.07983 0.01052 -7.586 1.05e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8901 on 23 degrees of freedom ## Multiple R-squared: 0.7144, Adjusted R-squared: 0.702 ## F-statistic: 57.54 on 1 and 23 DF, p-value: 1.055e-07 summary(modelB) ## ## Call: ## lm(formula = STEAM ~ TEMP + INV + PROD) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2348 -0.4116 0.1240 0.3744 1.2979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.514814 1.062969 8.951 1.30e-08 *** ## TEMP -0.079928 0.007884 -10.138 1.52e-09 *** ## INV 0.713592 0.502297 1.421 0.17 ## PROD 0.330497 3.267694 0.101 0.92 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.652 on 21 degrees of freedom ## Multiple R-squared: 0.8601, Adjusted R-squared: 0.8401 ## F-statistic: 43.04 on 3 and 21 DF, p-value: 3.794e-09 anova(modelA) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TEMP 1 45.592 45.592 57.543 1.055e-07 *** ## Residuals 23 18.223 0.792 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(modelB) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TEMP 1 45.592 45.592 107.2523 1.046e-09 *** ## INV 1 9.292 9.292 21.8588 0.0001294 *** ## PROD 1 0.004 0.004 0.0102 0.9203982 ## Residuals 21 8.927 0.425 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 \\(H_0\\): \\(\\beta_2 = \\beta_3 = 0\\) \\(H_A\\): At least one \\(\\beta_2, \\beta_3 \\neq 0\\) \\[\\begin{align*} F_{obs} &amp; = \\frac{(\\mbox{SSE}(A)-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}\\\\ &amp; = \\frac{(18.223-8.927)/(3-1)}{8.927/(25-4)}=10.93.\\\\ \\end{align*}\\] \\(F_{(0.05,2,21)} = 3.467\\), \\(F_{(0.01,2,21)} = 5.780\\) P-value \\(&lt;0.01\\), we reject \\(H_0\\) and conclude that at least one of \\(\\beta_2\\), \\(\\beta_3\\) differ from 0. anova(modelA, modelB) ## Analysis of Variance Table ## ## Model 1: STEAM ~ TEMP ## Model 2: STEAM ~ TEMP + INV + PROD ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 23 18.223 ## 2 21 8.927 2 9.2964 10.934 0.0005569 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2 Sequential sums of squares 5.2.1 Example: Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2+ \\beta_3 x_3\\) As noted earlier, the reduction in \\(\\mbox{SSE}\\) going from Model A to B, is equivalent to the increase in \\(\\mbox{SSR}\\), i.e. \\[\\mbox{SSE}(A)-\\mbox{SSE}(B)=\\mbox{SSR}(B)-\\mbox{SSR}(A).\\] We can denote: \\[\\mbox{SSR}(B|A)=\\mbox{SSR}(B)-\\mbox{SSR}(A).\\] These are the sequential sums of squares. We can write: \\[\\begin{align*} \\mbox{SST} &amp; = \\mbox{SSR}(B) + \\mbox{SSE}(B)\\\\ &amp; = \\mbox{SSR}(A) +\\mbox{SSR}(B) - \\mbox{SSR}(A) + \\mbox{SSE}(B)\\\\ &amp; = \\mbox{SSR}(A) + \\mbox{SSR}(B|A) + \\mbox{SSE}(B).\\\\ \\mbox{SST} - \\mbox{SSE}(B) &amp;= \\mbox{SSR}(A) + \\mbox{SSR}(B|A)\\\\ \\mbox{SSR}(B) &amp;= \\mbox{SSR}(A) + \\mbox{SSR}(B|A).\\\\ \\end{align*}\\] If model A is appropriate, \\(\\mbox{SSR}(B|A)\\) should be small. 5.2.2 Example: Steam data Model A: \\(\\mathbb{E}[y] = \\beta_0\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1\\) Model C: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) SOURCE df seqSS Notation TEMP 1 45.592 SSR(B|A) INV 1 9.292 SSR(C|B) PROD 1 0.004 SSR(D|C) From the ANOVA table, \\[\\begin{align*} \\mbox{SSR}(D)&amp; =54.889\\\\ &amp; = \\mbox{SSR}(B|A) + \\mbox{SSR}(C|B) + \\mbox{SSR}(D|C)\\\\ \\end{align*}\\] We can use the F-test for comparing two models to test Seq SS. 1): Model A: \\(\\mathbb{E}[y] = \\beta_0\\) Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) \\(H_0\\): \\(\\beta_1 = \\beta_2 = \\beta_3 = 0\\) \\(H_a\\): Not all \\(\\beta_i\\) are 0 \\(\\mbox{SSR}(A) = 0\\) \\(\\mbox{SSR}(D|A) = \\mbox{SSR}(D) = 54.889.\\) \\[F_{obs} = \\frac{\\mbox{SSR}(D|A)/(k-q)}{\\mbox{SSE}(D)/(n-p)} = \\frac{54.889/(3-0)}{8.927/(25-4)}=43.04 \\] P-value \\(&lt; 0.001\\), we reject \\(H_0\\) and conclude that not all \\(\\beta_i\\) are 0. 2): Model C: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\) Model D: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) \\(H_0\\): \\(\\beta_3 = 0\\) \\(H_a\\): \\(\\beta_3 \\neq 0\\) \\(\\mbox{SSR}(D|C) = 0.004\\) \\[F_{obs} = \\frac{\\mbox{SSR}(D|C)/(k-q)}{\\mbox{SSE}(D)/(n-p)} = \\frac{0.004/1}{8.927/21} = 0.01\\] \\(F_{(0.1,1,21)} = 2.96096\\), so P-value \\(&gt;0.05\\). We fail to reject \\(H_0\\) and conclude there is no evidence \\(\\beta_3 \\neq 0\\), i.e. \\(x_3\\) is not needed in the model. This F-test is equivalent to a t-test for \\(\\beta_3\\): \\[T = 0.1\\] \\[F = (0.1)(0.1) = 0.01\\] The p-value for both tests \\(= 0.92\\). Note: The Seq SS values depend on the order in which the variables are added to the model (unless the variables are uncorrelated). 5.2.3 Example: Steam cont’d in MTB Regression Analysis: STEAM versus TEMP Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value Regression 1 45.5924 45.5924 57.54 0.000 TEMP 1 45.5924 45.5924 57.54 0.000 Error 23 18.2234 0.7923 Lack-of-Fit 22 17.4042 0.7911 0.97 0.680 Pure Error 1 0.8192 0.8192 Total 24 63.8158 Model Summary S R-sq R-sq(adj) R-sq(pred) 0.890125 71.44% 70.20% 66.32% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 13.623 0.581 23.43 0.000 TEMP -0.0798 0.0105 -7.59 0.000 1.00 Regression Equation STEAM = 13.623 -0.0798TEMP Regression Analysis: STEAM versus TEMP, INV, PROD Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value Regression 3 54.8888 18.2963 43.04 0.000 TEMP 1 43.6895 43.6895 102.78 0.000 INV 1 0.8580 0.8580 2.02 0.170 PROD 1 0.0043 0.0043 0.01 0.920 Error 21 8.9270 0.4251 Total 24 63.8158 Model Summary S R-sq R-sq(adj) R-sq(pred) 0.651993 86.01% 84.01% 79.77% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 9.51 1.06 8.95 0.000 TEMP -0.07993 0.00788 -10.14 0.000 1.05 INV 0.714 0.502 1.42 0.170 9.51 PROD 0.33 3.27 0.10 0.920 9.55 Regression Equation STEAM = 9.51 -0.07993TEMP +0.714INV +0.33PROD Regression \\(&gt;\\) Options \\(&gt;\\) sum of squares tests \\(&gt;\\) sequential Regression Analysis: STEAM versus TEMP, INV, PROD Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 3 54.8888 18.2963 43.04 0.000 TEMP 1 45.5924 45.5924 107.25 0.000 INV 1 9.2921 9.2921 21.86 0.000 PROD 1 0.0043 0.0043 0.01 0.920 Error 21 8.9270 0.4251 Total 24 63.8158 Model Summary S R-sq R-sq(adj) R-sq(pred) 0.651993 86.01% 84.01% 79.77% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 9.51 1.06 8.95 0.000 TEMP -0.07993 0.00788 -10.14 0.000 1.05 INV 0.714 0.502 1.42 0.170 9.51 PROD 0.33 3.27 0.10 0.920 9.55 Regression Equation STEAM = 9.51 - 0.07993TEMP + 0.714INV + 0.33PROD Regression \\(&gt;\\) Options \\(&gt;\\) sum of squares tests \\(&gt;\\) sequential, but change the order in which the predictors are input in MTB. Output below is MTB17, MTB18 rearranges them in the order TEMP INV PROD. Regression Analysis: STEAM versus PROD, INV, TEMP Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 3 54.889 18.2963 43.04 0.000 PROD 1 5.958 5.9577 14.02 0.001 INV 1 5.242 5.2415 12.33 0.002 TEMP 1 43.690 43.6895 102.78 0.000 Error 21 8.927 0.4251 Total 24 63.816 Model Summary S R-sq R-sq(adj) R-sq(pred) 0.651993 86.01% 84.01% 79.77% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 9.51 1.06 8.95 0.000 PROD 0.33 3.27 0.10 0.920 9.55 INV 0.714 0.502 1.42 0.170 9.51 TEMP -0.07993 0.00788 -10.14 0.000 1.05 Regression Equation STEAM = 9.51 + 0.33PROD + 0.714INV - 0.07993TEMP The anova and aov functions in R implement a sequential sum of squares (type I). Function Anova(, type= 2) in library(car) gives the adjusted SS (type II) modelB &lt;- lm(STEAM ~ TEMP + INV + PROD) anova(modelB) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TEMP 1 45.592 45.592 107.2523 1.046e-09 *** ## INV 1 9.292 9.292 21.8588 0.0001294 *** ## PROD 1 0.004 0.004 0.0102 0.9203982 ## Residuals 21 8.927 0.425 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(STEAM ~ PROD + INV + TEMP)) ## Analysis of Variance Table ## ## Response: STEAM ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## PROD 1 5.958 5.958 14.015 0.001197 ** ## INV 1 5.242 5.242 12.330 0.002076 ** ## TEMP 1 43.690 43.690 102.776 1.524e-09 *** ## Residuals 21 8.927 0.425 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #library(car) Anova(modelB, type= 2) ## Anova Table (Type II tests) ## ## Response: STEAM ## Sum Sq Df F value Pr(&gt;F) ## TEMP 43.690 1 102.7760 1.524e-09 *** ## INV 0.858 1 2.0183 0.1701 ## PROD 0.004 1 0.0102 0.9204 ## Residuals 8.927 21 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.3 Testing for lack of fit When replicate values of response are available at some or all of the \\(X\\) values, a formal test of model adequacy is available. The test is based on comparing the fitted value to the average response for that level of \\(X\\). NOTATION: Suppose there are \\(g\\) different values of \\(X\\) and at the \\(i^{th}\\) of these, there are \\(n_i\\) observations of \\(Y\\). Let \\(\\bar{y}_{i.}=\\frac{1}{n_i}\\sum_{j=1}^{n_i} y_{ij}\\), \\(\\quad i=1, ..., g.\\) Note: this is the estimate of the group means in the 1-way ANOVA model (means model): \\(y_{ij} = \\mu_{i} + \\epsilon_{ij}\\), where \\(\\epsilon_{ij}\\) iid \\(N(0, \\sigma^2)\\). Then the pure error sums of squares, \\[\\begin{align*} \\mbox{SS}_{\\mbox{PE}}&amp; =\\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij}- \\bar{y}_{i.})^2\\\\ df_{PE} &amp; = \\sum_{i=1}^g (n_i-1)=n-g, \\hspace{1cm} \\mbox{where } n=n_1+...+n_g.\\\\ \\end{align*}\\] Therefore \\[\\frac{\\sum_{i=1}^g \\sum_{j=1}^{n_i} (y_{ij}- \\bar{y}_{i.})^2}{n-g}\\] is an estimator of \\(\\sigma^2\\). NOTE: Here we use the replicates to obtain an estimate of \\(\\sigma^2\\) which is independent of the fitted model (SLR). *This estimator of \\(\\sigma^2\\) corresponds to the \\(\\mbox{MSE}\\) in the ANOVA table for the 1-way ANOVA model. The 1-way ANOVA model has \\(g\\) parameters. The SLR model has \\(2\\) parameters. The latter is more restrictive as it requires linearity. \\(df_{PE} = n-g\\), \\(df_{SLR} = n-2\\). The SLR model has a residual SS which is \\(\\geq\\) residual SS from the means model, i.e. \\(\\mbox{SSE} \\geq \\mbox{SS}_{\\mbox{PE}}\\). A large difference \\(\\mbox{SSE} - \\mbox{SS}_{\\mbox{PE}}\\) indicates lack of fit of the regression line. \\(\\mbox{SS}(\\mbox{lack of fit})= \\mbox{SSE} - \\mbox{SS}_{\\mbox{PE}} = \\sum_{i,j} (\\hat{y}_{i,j} - \\bar{y}_i)^2\\), the sum of squared distances of between the SLR estimate and the means model estimate of \\(\\mathbb{E}(Y_{i,j})\\). Lack of fit is tested by the statistic: \\[F_{obs}=\\frac{\\left ( \\mbox{SSE}-\\mbox{SS}_{\\mbox{PE}} \\right )/(g-2)}{\\mbox{SS}_{\\mbox{PE}}/(n-g)}.\\] \\(H_0\\): Regression model fits well \\(H_A\\): Regression model displays lack of fit Under \\(H_0\\), \\(F_{obs} \\sim F_{g-2,n-g}\\). Note: This generalises to multiple predictors - the pure error estimate of \\(\\sigma^2\\) is based on SS between \\(y_i\\) for cases with the same values on all predictors. \\(df_{SLR} = p\\) instead of 2. Reject for large values of \\(F_{obs}\\). 5.3.1 Example: Voltage Example from Ramsey and Schafer (2002) (case0802 in library(Sleuth3)). Batches of electrical fluids were subjected to constant voltages until the insulating properties of the fluid broke down. \\(Y\\): time to breakdown \\(X\\): Voltage The scatterplot of \\(Y\\) vs. \\(X\\) shows evidence of non-linearity and non-constant variance. The response was log transformed to resolve this. \\(H_0: \\beta_1=0\\) \\(H_A: \\beta_1 \\neq 0\\) \\(F = 78.4\\), \\(p&lt;0.001\\). We reject \\(H_0\\) and conclude that \\(\\beta_1 \\neq 0\\). \\(H_0:\\) S.L.R model is appropriate/correct model \\(H_A:\\) S.L.R model has lack of fit. \\[F=\\frac{(180.07-173.75)/(7-2)}{173.75/(76-7)}=0.5\\] \\(F=0.50, p=0.773\\). We conclude that there is no evidence of lack of fit. One-way ANOVA: LOG_TIME versus CODE Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value CODE 6 196.5 32.746 13.00 0.000 Error 69 173.7 2.518 Total 75 370.2 Model Summary S R-sq R-sq(adj) R-sq(pred) 1.58685 53.07% 48.99% 38.72% Regression Analysis: LOG_TIME versus VOLTAGE Analysis of Variance Source DF Adj SS Adj MS F-Value P-Value Regression 1 190.151 190.151 78.14 0.000 VOLTAGE 1 190.151 190.151 78.14 0.000 Error 74 180.075 2.433 Lack-of-Fit 5 6.326 1.265 0.50 0.773 Pure Error 69 173.749 2.518 Total 75 370.226 Model Summary S R-sq R-sq(adj) R-sq(pred) 1.55995 51.36% 50.70% 48.50% Regression Equation LOG_TIME = 18.96 - 0.5074 VOLTAGE R code anova(lm(log(TIME)~VOLTAGE)) ## Analysis of Variance Table ## ## Response: log(TIME) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## VOLTAGE 1 190.15 190.151 78.141 3.34e-13 *** ## Residuals 74 180.07 2.433 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(lm(log(TIME)~as.factor(VOLTAGE))) ## Analysis of Variance Table ## ## Response: log(TIME) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(VOLTAGE) 6 196.48 32.746 13.004 8.871e-10 *** ## Residuals 69 173.75 2.518 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.4 Added variable plots In simple linear regression we can assess the importance of a predictor by: t-statistic \\(\\mbox{SSR}\\) \\(R^2\\) \\(Y\\)-\\(X\\) plot. The analogues in multiple regression for assessing the importance of a predictor in the presence of other predictors are: t-statistic Seq/Extra SS partial \\(R^2\\) added variable plot. 5.4.1 Example: STEAM vs. TEMP, INV, PROD Model A: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1+ \\beta_2 x_2\\) Model B: \\(\\mathbb{E}[y] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) where \\(x_1\\) = TEMP, \\(x_2\\) = INV, \\(x_3\\) = PROD. The t-statistic for PROD is small: \\(T=0.10, p=0.920\\) \\(\\mbox{SSR}(B|A) = 0.004\\) is also small. The partial \\(R^2\\) for PROD is the proportion of variability in the response unexplained by TEMP and INV that is explained by PROD \\[\\begin{align*} R^2(\\mbox{PROD|TEMP, INV})&amp; =\\frac{\\mbox{SSR}(B|A)}{\\mbox{SSE}(A)} &amp; = \\frac{0.004}{8.931} = 0.00045=0.045\\%\\\\ \\end{align*}\\] The added variable plot shows the relationship between a response and a predictor, adjusting for other predictors in the model. ‘Adjusting’ \\(Y\\) for predictors \\(X_1,...,X_k\\) is achieved by computing the residuals from the regression of \\(Y\\) on \\(X_1,...,X_k\\). The resulting residuals can be thought of as \\(Y\\) with the effect of \\(X_1,...,X_k\\) removed. 5.4.2 Example: Added variable plot for PROD. i.e. should we add PROD to the model containing the predictors TEMP and INV? (Response is STEAM). Compute \\(e\\)(STEAM\\(|\\) TEMP, INV), i.e. the residuals from regression of STEAM on TEMP and INV. Compute \\(e\\)(PROD\\(|\\) TEMP, INV), i.e. the residuals from regression of PROD on TEMP and INV. AVP for PROD: Plot \\(e\\)(STEAM\\(|\\) TEMP, INV) vs. \\(e\\)(PROD\\(|\\) TEMP, INV). We can also do: AVP INV: Plot \\(e\\)(STEAM\\(|\\) TEMP, PROD) vs. \\(e\\)(INV\\(|\\) TEMP, PROD) AVP TEMP: Plot \\(e\\)(STEAM\\(|\\) INV, PROD) vs. \\(e\\)(TEMP\\(|\\) INV, PROD) 5.4.3 Example: Steam data cont’d fit1 &lt;- lm(STEAM ~ TEMP + INV) fit2 &lt;- lm(PROD ~ TEMP + INV) summary(lm(resid(fit1)~ resid(fit2))) ## ## Call: ## lm(formula = resid(fit1) ~ resid(fit2)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2348 -0.4116 0.1240 0.3744 1.2979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.487e-17 1.246e-01 0.000 1.000 ## resid(fit2) 3.305e-01 3.122e+00 0.106 0.917 ## ## Residual standard error: 0.623 on 23 degrees of freedom ## Multiple R-squared: 0.0004869, Adjusted R-squared: -0.04297 ## F-statistic: 0.0112 on 1 and 23 DF, p-value: 0.9166 Alternatively you can use the avPlots function in the library(car). In minitab use STORAGE option to save the residuals of both models and make a scatterplot. 5.4.4 Properties of AVPs: Estimated intercept is 0. Slope of the line in AVP for PROD equals \\(\\hat{\\beta}\\) (the coefficient of PROD in the model with TEMP, INV and PROD as predictors. Residuals in AVP equal residuals from regression of STEAM on TEMP, INV and PROD. \\(R^2\\) in AVP for PROD is the partial \\(R^2\\) for PROD, i.e. \\(R^2\\)(PROD\\(|\\)TEMP,INV). \\(\\hat{\\sigma}^2\\) from AVP for PROD \\(\\approx \\hat{\\sigma}^2\\) from full model. \\[\\hat{\\sigma}^2_{AVP}(n-2) = \\hat{\\sigma}^2_{full}(n-p)\\] The points in an AVP are clustered tightly around a line if and only if the variable is important. AV plots may also show outliers, or if the apparent adjusted association between \\(Y\\) and \\(X_j\\) is due to an influence point. 5.5 Visualising Models in Hdim: added variable plots for the bodyfat data. Bodyfat data from assignment 3: http://rpubs.com/kdomijan/431176 References "],
["diagnostic-methods-in-more-details.html", "6 Diagnostic methods (in more details) 6.1 Model assumptions 6.2 Residuals 6.3 Leverage values 6.4 Standardised residuals 6.5 Leave-one-out methods 6.6 Other influence measures 6.7 Testing outliers 6.8 Diagnostics examples (two case studies)", " 6 Diagnostic methods (in more details) 6.1 Model assumptions The assumptions can be stated in terms of the error vector: \\(\\mathbb{E}(\\boldsymbol{\\epsilon}) = \\mathbf{0}\\) \\(\\mbox{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2\\mathbf{I}_n\\) \\(\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I}_n)\\) Since we do not observe \\(\\boldsymbol{\\epsilon}\\) we cannot check assumptions directly. Instead we observe residuals \\(\\mathbf{e}\\). 6.2 Residuals Residuals are the key to assessing model problems. \\[\\mathbf{e} = \\mathbf{Y} - \\mathbf{\\hat{Y}} = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} = (\\mathbf{I} - \\mathbf{H})\\mathbf{Y} \\] How do \\(\\boldsymbol{\\epsilon}\\) and \\(\\mathbf{e}\\) differ? If the model is correct, \\(\\mathbb{E}(\\mathbf{e}) = \\mathbb{E}(\\mathbf{Y}) - \\mathbb{E}(\\mathbf{\\hat{Y}})= \\mathbf{0}\\) (the same) \\(\\mbox{Var}(\\mathbf{e}) = (\\mathbf{I} - \\mathbf{H})\\sigma^2\\) (different). Like errors, residuals have mean 0, but Var(\\(e_i) = (1-h_{ii})\\sigma^2\\) so their variance is not quite constant (variance is smaller for \\(h_{ii}\\) close to 1). Note: this results shows that the residuals may have different variances even when \\(y_i\\)s have the same variance (\\(\\sigma^2\\)) because the precision of the fitted values depends on the pattern of \\(X_i\\)s. Cov(\\(e_i, e_j) = -h_{ii}\\sigma^2\\), for \\(i \\neq j\\). So the residuals are correlated, but in practice this correlation is generally not important or visible in residual plots. We plot: \\(e_i\\) vs \\(\\hat{y}_i\\) (residual vs fit), \\(e_i\\) vs \\(X_{ij}\\) (residual vs predictor \\(j\\)). The SLR conclusions are clear cut. In multiple regression deviations from the ideal pattern indicate model problems but precise diagnosis is more difficult. Figures above: ideal pattern: random scatter of points around 0 line non-constant variance, variability of residuals is changing curvature suggests that: \\(\\mathbb{E}(e_i) \\neq 0\\), thus, \\(\\mathbb{E}(\\epsilon_i)\\neq 0\\) curvature and non-constant variance 6.3 Leverage values \\[\\begin{align*} \\mathbf{\\hat{Y}} &amp; = \\mathbf{H}\\mathbf{Y}\\\\ \\mathbf{H} &amp; = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\\\ h_{ii} &amp; = \\mathbf{X}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}_i\\\\ \\end{align*}\\] \\(h_{ii}\\) is known as the leverage of case \\(i\\). where \\(\\mathbf{X}_i\\) is row \\(i\\) of \\(\\mathbf{X}\\) matrix in a column vector. In SLR, \\[\\begin{align*} h_{ii} &amp; = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}. \\end{align*}\\] A similar formula can be derived for multiple regression: \\[h_{ii} = \\frac{1}{n} + (\\mathbf{X}_i^* -\\mathbf{M})(\\mathbf{\\tilde{X}}^T\\mathbf{\\tilde{X}})^{-1}(\\mathbf{X}_i^* - \\mathbf{M}),\\] where \\(\\mathbf{X}_i^*\\) is row \\(i\\) of \\(\\mathbf{X}\\) matrix without the column of \\(\\mathbf{1}\\)s. i.e. \\(\\mathbf{X}_i =\\begin{bmatrix} 1 &amp; \\mathbf{X}_i^* \\end{bmatrix}\\) \\(\\mathbf{M}\\) is the mean vector \\[\\mathbf{M} =\\begin{bmatrix} \\bar{X}_{.1}\\\\ \\vdots \\\\ \\bar{X}_{.k} \\end{bmatrix}\\] \\(\\tilde{\\mathbf{X}}\\) is the matrix of centered \\(x\\) - data values, i.e. \\[\\tilde{\\mathbf{X}} =\\begin{bmatrix} (X_{11} - \\bar{X}_{.1}) &amp; \\dots &amp; (X_{1k} - \\bar{X}_{.k}) \\\\ (X_{21} - \\bar{X}_{.1}) &amp; \\dots &amp; (X_{2k} - \\bar{X}_{.k}) \\\\ \\vdots &amp; \\vdots &amp;\\vdots \\\\ (X_{n1} - \\bar{X}_{.1}) &amp; \\dots &amp; (X_{nk} - \\bar{X}_{.k}) \\end{bmatrix} \\] \\(\\bar{X}_{.1}, \\dots, \\bar{X}_{.k}\\) are the means of the \\(k\\) predictors. Therefore \\(h_{ii}\\) measures the `distance’ of case \\(i\\) from the average case \\(\\mathbf{M}\\). Example \\(k=2\\): contours of constant \\(h_{ii}\\) All points on an ellipse have the same \\(h_{ii}\\) value. 6.3.1 Properties of \\(h_{ii}\\): \\(0 \\leq h_{ii} \\leq 1\\) or if the model has an intercept \\(\\frac{1}{n} \\leq h_{ii} \\leq 1\\) \\(\\sum_{i=1}^n h_{ii} =p\\) where \\(p\\) is the number of parameters \\(\\sum_{i=1}^n h_{ij} =\\sum_{j=1}^n h_{ij} = 1\\), i.e. row sum = col sum = 1. If \\(h_{ii}\\) is large (rule of thumb: large if \\(\\geq 2 \\times \\mbox{average } h_{ii} = 2p/n\\)), then the case may be: a mistake a high influence case (i.e. has a big impact on results). If \\(h_{ii}\\) is extremely large (i.e. close to 1): \\(\\hat{y}_i \\approx y_i\\) so \\(e_i \\approx 0\\) and Var(\\(\\hat{y}_i)= h_{ii}\\sigma^2 \\approx \\sigma^2 \\approx\\) Var(\\(y_i\\)). In this case \\(\\mathbb{E}(y_i)\\) is imprecisely estimated. In practice, this rarely happens. 6.4 Standardised residuals When we compare residuals (\\(e_i\\)) for different observations we should take into account that their variances may differ: \\[\\mbox{Var}(e_i) = (1-h_{ii})\\sigma^2.\\] We can standardise them by dividing by \\(\\sqrt{(1-h_{ii})}\\hat{\\sigma}\\), where \\(\\hat{\\sigma}\\) is the estimate based on the SSE. We get standardised residuals: \\[r_i=\\frac{e_i}{\\sqrt{(1-h_{ii})}\\hat{\\sigma}}.\\] Then we have: \\(\\mathbb{E}(r_i) = 0\\) and \\(\\mbox{Var}(r_i)\\approx 1\\) (constant). Since \\(r_i\\), unlike \\(e_i\\), are on a common scale it is easier/fairer to compare them. Standardised residuals are useful in detecting anomalous observations or outliers. Note: Cases with \\(|r_i| \\geq 2\\) are not well fit. \\(r_i\\) are often used in place of \\(e_i\\) in residual plots and normal probability plots. \\(r_i\\) (and \\(e_i\\)) are not independent. 6.5 Leave-one-out methods Remember that as \\(h_{ii}\\) approaches 1, the variance of the residual approaches 0, indicating that the fitted value \\(\\hat{y}_i\\) is pulled close to the observed value \\(y_i\\). So leverage \\(h_{ii}\\) is the potential influence of the \\(i^{th}\\) observation. Observations with high leverage need to be inspected carefully as they might have a large influence on the fit. Note that potential influence is not necessarily the same thing as actual influence, since it is might be the case that the observation is in line with rest of the data, and fitting the model without this observation would give a prediction close to the observed \\(y_i\\) anyhow. One way to examine actual influence of case \\(i\\) is to compare the regression results with case \\(i\\) to those without case \\(i\\). Denote the fitted values with all cases included as \\(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_n\\) as usual. Denote the fitted values with case \\(i\\) removed as \\(\\hat{y}_{1(i)}, \\hat{y}_{2(i)}, ..., \\hat{y}_{n(i)}\\), \\(i = 1,...,n\\). Cook’s distance measures the influence of the \\(i^{th}\\) case by: \\[D_i = \\sum_{j=1}^n \\frac{(\\hat{y}_{j}-\\hat{y}_{j(i)})^2}{p \\hat{\\sigma}^2}.\\] Note this is the Euclidean distance (SS) between fitted values obtained by omitting the \\(i^{th}\\) observation \\(\\hat{y}_{j(i)}\\) and fitted values based on all the data \\(\\hat{y}_{j}\\). Typically we examine the case with the largest \\(D_i\\) further or, in the case of large datasets, the few cases with the largest \\(D_i\\) values, as these have the largest influence. So does the computation of Cook’s distance \\(D_i\\) for all observations require refitting the model \\(n\\) times? This would be computationally expensive! It turns out that we can rewrite the above formula for \\(D_i\\) as a function of the standardised residual \\(r_i\\) and the leverage \\(h_{ii}\\). So in the end we only need tro fit the mopdel once and then we can compute \\(D_i\\) from the complete data regression results. Shortcut formula: \\[\\hat{y}_{j(i)}=\\hat{y}_{j}- \\frac{h_{ij}}{1-h_{ii}}e_i.\\] Thus: \\[\\hat{y}_{j}-\\hat{y}_{j(i)}= \\frac{h_{ij}}{1-h_{ii}}e_i\\] and \\[\\begin{align*} \\sum_j(\\hat{y}_{j}-\\hat{y}_{j(i)})^2 &amp; = \\frac{e_i^2}{(1-h_{ii})^2}\\sum_j h_{ij}^2\\\\ &amp; = \\frac{e_i^2 h_{ii}}{(1-h_{ii})^2},\\\\ \\end{align*}\\] from the properties of the H matrix (symmetric and idempotent). Hence: \\[\\begin{align*} D_i&amp; = \\frac{e_i^2 h_{ii}}{p \\hat{\\sigma}^2(1-h_{ii})^2}\\\\ &amp; = \\frac{r_i^2h_{ii}}{p(1-h_{ii})}.\\\\ \\end{align*}\\] In general, high influence or outlier cases have either: Big \\(|r_i|\\) and big \\(h_{ii}\\). (RX) Big \\(|r_i|\\) and moderate \\(h_{ii}\\). (R) Moderate \\(|r_i|\\) and big \\(h_{ii}\\). (X) 6.6 Other influence measures \\(\\hat{\\beta_j}-\\hat{\\beta}_{j(i)}\\) (the effect of leaving case \\(i\\) out) \\(\\hat{\\sigma}_{(i)}\\) (estimate of \\(\\sigma\\) with case \\(i\\) omitted). Used in: “studentised residual” = \\(\\frac{\\hat{\\epsilon_i}}{\\sqrt{1-h_{ii}}\\hat{\\sigma}_{(i)}}\\) 6.7 Testing outliers If an individual case is suspected of being an outlier a formal hypothesis test can be performed to verify. Details of the test are in Chapter 9 of Weisberg (2005). For more on leverages and influence see Chapter 2 of Rodríguez (2007). For a very detailed exposition on linear model diagnostics see Chapter 11 and 12 of Fox (2016). 6.8 Diagnostics examples (two case studies) 6.8.1 Example 1: Brain size versus body gestation period and litter Example from Ramsey and Schafer (2002) (case0902 in library(Sleuth3)). It is known that body size of mammals is a good predictor of brain size but it was of interest to know if gestation period and litter size were also good predictors. This data contains average values of brain weight, body weight, gestation length and litter size in 96 species of mammals. Does the model fit well? Outliers, nonlinearity? NOTE: in MTB you can use brushing (set species as ID variables) and link the graphs to explore the data. #library(car) #library(MASS) fit1 &lt;- lm(BRAIN ~ BODY + GESTATION + LITTER) summary(fit1) ## ## Call: ## lm(formula = BRAIN ~ BODY + GESTATION + LITTER) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1026.68 -62.08 17.29 51.73 988.76 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -225.29213 83.05875 -2.712 0.00797 ** ## BODY 0.98588 0.09428 10.457 &lt; 2e-16 *** ## GESTATION 1.80874 0.35445 5.103 1.79e-06 *** ## LITTER 27.64864 17.41429 1.588 0.11579 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 224.6 on 92 degrees of freedom ## Multiple R-squared: 0.81, Adjusted R-squared: 0.8038 ## F-statistic: 130.7 on 3 and 92 DF, p-value: &lt; 2.2e-16 # anova(fit1) Clearly the model is defective. The normal probability plot shows that the residuals come from a heavy tailed distribution. The residual vs fit plot shows a linear pattern for the majority of data and a few outliers. Most of the pattern is hidden because the data is clumped together. We also observe unequal variance. African elephant, Hippopotamus, Dolphin, Human have large standardised residuals. African elephant, Hippopotamus and Dolphin, have large influence. #################### # influence measures #################### inflm.SR &lt;- influence.measures(fit1) int &lt;- which(apply(inflm.SR$is.inf, 1, any)) cbind(brain.data[int ,], influence(fit1)$hat[int],# large leverage stdres(fit1)[int],# large std residual abs(D)[int]) # large Cook&#39;s D ## SPECIES BRAIN BODY GESTATION LITTER ## 24 Human being 1300.00 65.00 270 1.0 ## 39 Rat I 0.72 0.05 23 7.3 ## 40 Rat II 2.38 0.34 21 8.0 ## 52 Dolphin 1600.00 160.00 360 1.0 ## 72 African elephant 4480.00 2800.00 655 1.0 ## 75 Tapir 250.00 230.00 390 1.0 ## 77 Domestic pig 180.00 190.00 115 8.0 ## 78 Hippopotamus 590.00 1400.00 240 1.0 ## influence(fit1)$hat[int] stdres(fit1)[int] abs(D)[int] ## 24 0.03522907 4.28526198 1.676377e-01 ## 39 0.10218313 -0.08211966 1.918783e-04 ## 40 0.13397456 -0.15234064 8.975596e-04 ## 52 0.07906017 4.58816129 4.517975e-01 ## 72 0.71937767 6.15725313 2.429678e+01 ## 75 0.09448789 -2.26740230 1.341155e-01 ## 77 0.17045813 -1.03271364 5.478722e-02 ## 78 0.25103327 -5.28288412 2.338573e+00 Added-variable plots can be used for detecting influential data. For example, AVP for body: Hippo and African elephant have a large body given the other variables (litter and gestation). Humans, dolphins and African elephant have a large brain for their litter and gestation. Together Humans, dolphins and hippos reduce the body slope, African elephant, while a high-leverage point, is more in line with the rest of the data. The raw data have a lot of skeweness. This naturally generates a lot of outliers and obscures patterns in the data. Transform the data by taking the logs to improve the model and fit. The skeweness is gone. The plot of log(brain) vs log(body) shows a strong linear pattern - not apparent with the untransformed variables. Therefore the log transformation of these two variables seems appropriate. The other variables are also less skewed than before. fit2 &lt;- lm(log(BRAIN) ~ log(BODY) + log(GESTATION) + log(LITTER)) summary(fit2) ## ## Call: ## lm(formula = log(BRAIN) ~ log(BODY) + log(GESTATION) + log(LITTER)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.95415 -0.29639 -0.03105 0.28111 1.57491 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.85482 0.66167 1.292 0.19962 ## log(BODY) 0.57507 0.03259 17.647 &lt; 2e-16 *** ## log(GESTATION) 0.41794 0.14078 2.969 0.00381 ** ## log(LITTER) -0.31007 0.11593 -2.675 0.00885 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4748 on 92 degrees of freedom ## Multiple R-squared: 0.9537, Adjusted R-squared: 0.9522 ## F-statistic: 631.6 on 3 and 92 DF, p-value: &lt; 2.2e-16 #anova(fit2) The biggest standardised residual is now case 24 (Human) who have a larger brain than predicted by the model. Highest leverage is case 1 (quokka), but it doesn’t have high influence (this also goes for all cases with \\(h_{ii} &gt; 0.083 = 2p/n\\), where average \\(h_{ii} = p/n\\), except for case 73 which has longer gestation). The residual distribution shows some skeweness, but is closer to normal than before. The residual vs fit plot shows some evidence of non-constant variance. Perhaps some other transformation would be better? To make these plots in minitab use the storage option for standardised residuals, Cook’s dist and leverages. To answer the original question, clearly gestation time and litter size are important predictors of brain size in the presence of body size. 6.8.2 Example 2: Rat data Example from Weisberg (2005). Experiment to investigate amount of drug present in liver of rat. BodyWt = body weight of the rat LiverWt = measured after sacrifice Dose = dose given, proportional to body weight EndDose = dose of drug recovered after sacrifice of the animal Experimantal hypothesis: no relationship between EndDose and 3 predictors. #library(car) #library(MASS) fit1 &lt;- lm(EndDose ~ BodyWt + LiverWt + Dose, data = rats.data) summary(fit1) ## ## Call: ## lm(formula = EndDose ~ BodyWt + LiverWt + Dose, data = rats.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.100557 -0.063233 0.007131 0.045971 0.134691 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.265922 0.194585 1.367 0.1919 ## BodyWt -0.021246 0.007974 -2.664 0.0177 * ## LiverWt 0.014298 0.017217 0.830 0.4193 ## Dose 4.178111 1.522625 2.744 0.0151 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07729 on 15 degrees of freedom ## Multiple R-squared: 0.3639, Adjusted R-squared: 0.2367 ## F-statistic: 2.86 on 3 and 15 DF, p-value: 0.07197 #anova(fit1) It appears as if a combination of bodyweight and dose are relevant to EndDose. However, they are not individually related to EndDose even though \\(X_1 \\propto X_3\\) Case 3 has large influence (see Cook’s distance plot). Case 3 is an unusual combination of dose and bodyweight. head(rats.data[sort(influence(fit1)$hat, decreasing = TRUE, index.return = TRUE)$ix,]) ## BodyWt LiverWt Dose EndDose ## 3 190 9.0 1.00 0.56 ## 5 200 7.2 1.00 0.23 ## 13 149 5.2 0.75 0.21 ## 8 195 10.0 0.98 0.41 ## 16 186 6.8 0.94 0.28 ## 17 146 7.3 0.73 0.30 A rat was given a dose that was too high for his bodyweight. avPlots(fit1) Redo the analysis with case 3 removed: fit1 &lt;- lm(EndDose ~ BodyWt + LiverWt + Dose, data = rats.data[-3,]) summary(fit1) ## ## Call: ## lm(formula = EndDose ~ BodyWt + LiverWt + Dose, data = rats.data[-3, ## ]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.102154 -0.056486 0.002838 0.046519 0.137059 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.311427 0.205094 1.518 0.151 ## BodyWt -0.007783 0.018717 -0.416 0.684 ## LiverWt 0.008989 0.018659 0.482 0.637 ## Dose 1.484877 3.713064 0.400 0.695 ## ## Residual standard error: 0.07825 on 14 degrees of freedom ## Multiple R-squared: 0.02106, Adjusted R-squared: -0.1887 ## F-statistic: 0.1004 on 3 and 14 DF, p-value: 0.9585 #anova(fit1) Experimental hypothesis is validated as all coefficients have large P-values. 6.8.2.1 Should Unusual Data Be Discarded? Although problematic data should not be ignored, they also should not be deleted automatically. It is important to investigate why an observation is unusual. Truly bad data (e.g.rats) can be corrected or thrown away. When a discrepant data-point is correct, we may be able to understand why the observation is unusual. For Species Brain data, it makes sense that humans enjoy brain size not accounted for by the other variables. In a case like this, we may choose to deal separately with an outlying observation. Outliers or influential data may motivate model respecification e.g. the introduction of additional explanatory variables. However, we must be careful to avoid overfitting the data i.e. permitting a small portion of the data to determine the form of the model. A more extensive discussion can be found in Fox (2016) Chapter 11.7, pg 288-289. 6.8.2.2 Demonstrate effect of omiting cases on regression line Cigarette data from 3.7.4. Select a point to remove and refit the model. https://rstudioserver.maths.nuim.ie:3838/churley/cig.Rmd References "],
["special-cases-of-multiple-regression.html", "7 Special cases of multiple regression 7.1 Categorical and continuous predictors (binary categories) 7.2 Categorical and continuous predictors (more than two categories) 7.3 Quadratic terms and interactions 7.4 An example with two continuous and two categorical predictors", " 7 Special cases of multiple regression 7.1 Categorical and continuous predictors (binary categories) 7.1.0.1 Example (from Ramsey and Schafer (2002) pg 236, 245): \\(Y\\): average number of flowers per plant (meadowfoam). Light intensity: 150, 300, 450, 600, 750, 900 (\\(\\mu\\) mol/\\(m^2\\)/sec) Timing: Timing of onset of light treatment Early/Late. Coded 0/1. Suppose data is in the table below (every 2nd row) and consider the following models: Parallel lines model (model A): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light)\\] Separate lines model (model B): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light) + \\beta_3 (timing \\times light)\\] Give the design matrix and the parameter vector for both models Test \\(H_0: \\beta_3 = 0\\). Flowers Timing Time Intensity 62.3 Early 0 150 77.4 Early 0 150 55.3 Early 0 300 54.2 Early 0 300 49.6 Early 0 450 61.9 Early 0 450 Parallel lines model: \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light)\\] \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 150 \\\\ 1 &amp; 0 &amp; 300\\\\ 1 &amp; 0 &amp; 450 \\\\ 1 &amp; 0 &amp; 600\\\\ 1 &amp; 0 &amp; 750\\\\ 1 &amp; 0 &amp; 900\\\\ 1 &amp; 1 &amp; 150 \\\\ 1 &amp; 1 &amp; 300\\\\ 1 &amp; 1 &amp; 450 \\\\ 1 &amp; 1 &amp; 600 \\\\ 1 &amp; 1 &amp; 750\\\\ 1 &amp; 1 &amp; 900 \\\\ \\end{bmatrix}\\] \\(\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\\\\ \\end{bmatrix}\\) Separate lines model: \\[\\mathbb{E}(y)= \\beta_0+\\beta_1(timing)+ \\beta_2 (light) + \\beta_3 (timing \\times light)\\] \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; 0 &amp; 150 &amp;0 \\\\ 1 &amp; 0 &amp; 300 &amp;0 \\\\ 1 &amp; 0 &amp; 450 &amp;0 \\\\ 1 &amp; 0 &amp; 600 &amp;0 \\\\ 1 &amp; 0 &amp; 750 &amp;0 \\\\ 1 &amp; 0 &amp; 900 &amp;0 \\\\ 1 &amp; 1 &amp; 150 &amp;150 \\\\ 1 &amp; 1 &amp; 300 &amp;300 \\\\ 1 &amp; 1 &amp; 450 &amp;450 \\\\ 1 &amp; 1 &amp; 600 &amp;600 \\\\ 1 &amp; 1 &amp; 750 &amp;750 \\\\ 1 &amp; 1 &amp; 900 &amp;900 \\\\ \\end{bmatrix}\\] \\[\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\\\\ \\beta_3\\\\ \\end{bmatrix}\\] To test \\(H_0: \\beta_3 = 0\\), P-value = 0.910, so cannot reject \\(H_0\\) (See table of coefficients, output below). Model A Regression Analysis: Flowers versus Time, Intensity Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 2 3466.7 1733.35 41.78 0.000 Time 1 887.0 886.95 21.38 0.000 Intensity 1 2579.8 2579.75 62.18 0.000 Error 21 871.2 41.49 Lack-of-Fit 9 215.3 23.92 0.44 0.889 Pure Error 12 655.9 54.66 Total 23 4337.9 Model Summary S R-sq R-sq(adj) R-sq(pred) 6.44107 79.92% 78.00% 73.84% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 71.31 3.27 21.78 0.000 Time 12.16 2.63 4.62 0.000 1.00 Intensity -0.04047 0.00513 -7.89 0.000 1.00 Regression Equation Flowers = 71.31 + 12.16 Time - 0.04047 Intensity Model B Regression Analysis: Flowers versus Time, Intensity, TxI Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 3 3467.28 1155.76 26.55 0.000 Time 1 886.95 886.95 20.37 0.000 Intensity 1 2579.75 2579.75 59.26 0.000 TxI 1 0.58 0.58 0.01 0.910 Error 20 870.66 43.53 Lack-of-Fit 8 214.73 26.84 0.49 0.841 Pure Error 12 655.93 54.66 Total 23 4337.94 Model Summary S R-sq R-sq(adj) R-sq(pred) 6.59795 79.93% 76.92% 70.95% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant 71.62 4.34 16.49 0.000 Time 11.52 6.14 1.88 0.075 5.20 Intensity -0.04108 0.00744 -5.52 0.000 2.00 TxI 0.0012 0.0105 0.12 0.910 6.20 Regression Equation Flowers = 71.62 + 11.52 Time - 0.04108 Intensity + 0.0012 TxI Model A: fit1 &lt;- lm(Flowers ~ Intensity + Time) summary(fit1) ## ## Call: ## lm(formula = Flowers ~ Intensity + Time) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.652 -4.139 -1.558 5.632 12.165 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.305833 3.273772 21.781 6.77e-16 *** ## Intensity -0.040471 0.005132 -7.886 1.04e-07 *** ## Time 12.158333 2.629557 4.624 0.000146 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.441 on 21 degrees of freedom ## Multiple R-squared: 0.7992, Adjusted R-squared: 0.78 ## F-statistic: 41.78 on 2 and 21 DF, p-value: 4.786e-08 Model B: fit2 &lt;- lm(Flowers ~ Intensity * Time) summary(fit2) ## ## Call: ## lm(formula = Flowers ~ Intensity * Time) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.516 -4.276 -1.422 5.473 11.938 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 71.623333 4.343305 16.491 4.14e-13 *** ## Intensity -0.041076 0.007435 -5.525 2.08e-05 *** ## Time 11.523333 6.142360 1.876 0.0753 . ## Intensity:Time 0.001210 0.010515 0.115 0.9096 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.598 on 20 degrees of freedom ## Multiple R-squared: 0.7993, Adjusted R-squared: 0.7692 ## F-statistic: 26.55 on 3 and 20 DF, p-value: 3.549e-07 model.matrix(fit2) ## (Intercept) Intensity Time Intensity:Time ## 1 1 150 0 0 ## 2 1 150 0 0 ## 3 1 300 0 0 ## 4 1 300 0 0 ## 5 1 450 0 0 ## 6 1 450 0 0 ## 7 1 600 0 0 ## 8 1 600 0 0 ## 9 1 750 0 0 ## 10 1 750 0 0 ## 11 1 900 0 0 ## 12 1 900 0 0 ## 13 1 150 1 150 ## 14 1 150 1 150 ## 15 1 300 1 300 ## 16 1 300 1 300 ## 17 1 450 1 450 ## 18 1 450 1 450 ## 19 1 600 1 600 ## 20 1 600 1 600 ## 21 1 750 1 750 ## 22 1 750 1 750 ## 23 1 900 1 900 ## 24 1 900 1 900 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 3 7.2 Categorical and continuous predictors (more than two categories) 7.2.0.1 Example: (from Ramsey and Schafer (2002)): \\(Y\\): Measure of energy \\(X_1\\): Measure of weight Group: Type of flyer (1,2,3). Z1, Z2, Z3 (dummy variables). Parallel lines model (model A): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1 z_2+ \\beta_2 z_3 + \\beta_3 x_1\\] Separate lines model (model B): \\[\\mathbb{E}(y)= \\beta_0+\\beta_1 z_2+ \\beta_2 z_3 + \\beta_3 x_1 + \\beta_4 x_1 z_2+ \\beta_5 x_1 z_3\\] Hypothesis testing: Test \\(H_0: \\beta_4 = \\beta_5 = 0\\) by comparing the two models using an F-test. Test \\(H_0: \\beta_1 = \\beta_2 = 0\\) by comparing the parallel lines model to the model \\(\\mathbb{E}(y)= \\beta_0+\\beta_3 x_1\\) using an F-test. Give the design matrix and the parameter vector for both models. Test \\(H_0: \\beta_4 = \\beta_5 = 0\\), i.e. \\(H_0:\\) Model A is correct \\(H_A:\\) Model B is preferable to Model A \\[\\begin{align*} F &amp; =\\frac{(\\mbox{SSE}(A)-\\mbox{SSE}(B))/(k-q)}{\\mbox{SSE}(B)/(n-p)}\\\\ &amp; =\\frac{(0.5533- 0.5049)/(5-3)}{0.5049/(20-6)}\\\\ &amp; =\\frac{0.0242}{0.0361}\\\\ &amp; = 0.67.\\\\ \\end{align*}\\] \\(F_{(2,14)}(0.95) = 3.73 &gt; 0.67\\) so we cannot reject \\(H_0\\), model A is OK. Test \\(H_0: \\beta_1 = \\beta_2 = 0\\), i.e. let model C = one group model: \\[\\mathbb{E}(y)= \\beta_0+ \\beta_3 x_1 \\] \\(H_0:\\) Model C is correct \\(H_A:\\) Model A is preferable to Model C \\[\\begin{align*} F &amp; =\\frac{(\\mbox{SSE}(C)-\\mbox{SSE}(A))/(k-q)}{\\mbox{SSE}(A)/(n-p)}\\\\ &amp; =\\frac{\\mbox{SSR}(A|C)/(3-1)}{0.5533/(20-4)}\\\\ &amp; =\\frac{(0.0008+0.0288)/(3-1)}{0.5533/(20-4)}\\\\ &amp; =\\frac{0.0296/2}{0.0346}\\\\ &amp; = 0.43\\\\ \\end{align*}\\] We don’t need to see the fit for Model C, take Seq SS. \\(F_{(2,16)}(0.95) = 3.63 &gt; 0.43\\) so we cannot reject \\(H_0\\), model C is adequate. OUTPUT: Model A Regression Analysis: y versus x1, Z2, Z3 Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 3 29.4215 9.8072 283.59 0.000 x1 1 29.3919 29.3919 849.91 0.000 Z2 1 0.0288 0.0288 0.83 0.375 Z3 1 0.0008 0.0008 0.02 0.883 Error 16 0.5533 0.0346 Total 19 29.9748 Model Summary S R-sq R-sq(adj) R-sq(pred) 0.185963 98.15% 97.81% 97.30% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant -1.498 0.150 -9.99 0.000 x1 0.8150 0.0445 18.30 0.000 2.58 Z2 -0.079 0.203 -0.39 0.703 3.80 Z3 0.024 0.158 0.15 0.883 3.45 Regression Equation y = -1.498 +0.8150x1 -0.079Z2 +0.024Z3 OUTPUT: Model B Regression Analysis: y versus x1, Z2, Z3, Z2*x1, Z3*x1 Analysis of Variance Source DF Seq SS Seq MS F-Value P-Value Regression 5 29.4699 5.8940 163.44 0.000 x1 1 29.3919 29.3919 815.04 0.000 Z2 1 0.0288 0.0288 0.80 0.387 Z3 1 0.0008 0.0008 0.02 0.886 Z2*x1 1 0.0452 0.0452 1.25 0.282 Z3*x1 1 0.0032 0.0032 0.09 0.770 Error 14 0.5049 0.0361 Total 19 29.9748 Model Summary S R-sq R-sq(adj) R-sq(pred) 0.189900 98.32% 97.71% 96.29% Coefficients Term Coef SE Coef T-Value P-Value VIF Constant -1.471 0.248 -5.94 0.000 x1 0.8047 0.0867 9.28 0.000 9.37 Z2 1.27 1.29 0.99 0.341 146.62 Z3 -0.110 0.385 -0.29 0.779 19.70 Z2*x1 -0.215 0.224 -0.96 0.353 166.38 Z3*x1 0.031 0.103 0.30 0.770 41.94 Regression Equation y = -1.471 +0.8047 x1 +1.27 Z2 -0.110 Z3- - 0.215Z2 x1 +0.031Z3 x1 x1G &lt;- flying.data$x1 * flying.data$G1 x2G &lt;- flying.data$x1 * flying.data$G2 fitA &lt;- lm(y ~ x1 + G1 + G2, data = flying.data) summary(fitA) ## ## Call: ## lm(formula = y ~ x1 + G1 + G2, data = flying.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.23224 -0.12199 -0.03637 0.12574 0.34457 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.49770 0.14987 -9.993 2.77e-08 *** ## x1 0.81496 0.04454 18.297 3.76e-12 *** ## G1 -0.07866 0.20268 -0.388 0.703 ## G2 0.02360 0.15760 0.150 0.883 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.186 on 16 degrees of freedom ## Multiple R-squared: 0.9815, Adjusted R-squared: 0.9781 ## F-statistic: 283.6 on 3 and 16 DF, p-value: 4.464e-14 anova(fitA) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 29.3919 29.3919 849.9108 2.691e-15 *** ## G1 1 0.0288 0.0288 0.8327 0.3750 ## G2 1 0.0008 0.0008 0.0224 0.8828 ## Residuals 16 0.5533 0.0346 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fitB &lt;- lm(y ~ x1 + G1 + G2 + x1G + x2G, data = flying.data) summary(fitB) ## ## Call: ## lm(formula = y ~ x1 + G1 + G2 + x1G + x2G, data = flying.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.25152 -0.12643 -0.00954 0.08124 0.32840 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.47052 0.24767 -5.937 3.63e-05 *** ## x1 0.80466 0.08668 9.283 2.33e-07 *** ## G1 1.26807 1.28542 0.987 0.341 ## G2 -0.11032 0.38474 -0.287 0.779 ## x1G -0.21487 0.22362 -0.961 0.353 ## x2G 0.03071 0.10283 0.299 0.770 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1899 on 14 degrees of freedom ## Multiple R-squared: 0.9832, Adjusted R-squared: 0.9771 ## F-statistic: 163.4 on 5 and 14 DF, p-value: 6.696e-12 anova(fitB) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 29.3919 29.3919 815.0383 8.265e-14 *** ## G1 1 0.0288 0.0288 0.7986 0.3866 ## G2 1 0.0008 0.0008 0.0215 0.8855 ## x1G 1 0.0452 0.0452 1.2543 0.2816 ## x2G 1 0.0032 0.0032 0.0892 0.7696 ## Residuals 14 0.5049 0.0361 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fitA, fitB) ## Analysis of Variance Table ## ## Model 1: y ~ x1 + G1 + G2 ## Model 2: y ~ x1 + G1 + G2 + x1G + x2G ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 16 0.55332 ## 2 14 0.50487 2 0.04845 0.6718 0.5265 fitC &lt;- lm(y ~ x1, data = flying.data) #summary(fitC) #anova(fitC) 7.3 Quadratic terms and interactions Example from Ramsey and Schafer (2002) pg 252. The data on corn yields and rainfall are in `RainfallData.csv’, or library(Sleuth3) in ‘ex0915’. Variables: Yield: corn yield (bushels/acre) Rainfall: rainfall (inches/year) Year: year. Link: http://www.rpubs.com/kdomijan/332466 7.4 An example with two continuous and two categorical predictors FEV data - for a full description see http://ww2.amstat.org/publications/jse/v13n2/datasets.kahn.html. Response variable: fev (forced expiratory volume) measures respiratory function. Predictors: age, height, gender and smoke. The dataset is in library(covreg). Link: http://www.rpubs.com/kdomijan/325930 References "],
["references.html", "References", " References "]
]
