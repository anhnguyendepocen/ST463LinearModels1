<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Notes for ST463/ST683 Linear Models 1</title>
  <meta name="description" content="These are the notes for ST463/ST683 Linear Models 1 module at Maynooth University. The output format for this example is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Notes for ST463/ST683 Linear Models 1" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the notes for ST463/ST683 Linear Models 1 module at Maynooth University. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="domijank/ST463" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Notes for ST463/ST683 Linear Models 1" />
  
  <meta name="twitter:description" content="These are the notes for ST463/ST683 Linear Models 1 module at Maynooth University. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Katarina Domijan">


<meta name="date" content="2018-11-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="multiple-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Module Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#recommended-texts"><i class="fa fa-check"></i><b>1.1</b> Recommended texts</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i><b>1.2</b> Software</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#r-scripts"><i class="fa fa-check"></i><b>1.3</b> R Scripts</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#introductory-examples"><i class="fa fa-check"></i><b>2.1</b> Introductory Examples</a><ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#mother-and-daughter-heights"><i class="fa fa-check"></i><b>2.1.1</b> Mother and daughter heights</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#bacterial-count-and-storage-temperature"><i class="fa fa-check"></i><b>2.1.2</b> Bacterial count and storage temperature</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#yield-and-rainfall"><i class="fa fa-check"></i><b>2.1.3</b> Yield and Rainfall</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#driving"><i class="fa fa-check"></i><b>2.1.4</b> Driving</a></li>
<li class="chapter" data-level="2.1.5" data-path="intro.html"><a href="intro.html#fuel"><i class="fa fa-check"></i><b>2.1.5</b> Fuel Consumption</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="SLR.html"><a href="SLR.html"><i class="fa fa-check"></i><b>3</b> Simple Linear regression</a><ul>
<li class="chapter" data-level="3.1" data-path="SLR.html"><a href="SLR.html#OLS"><i class="fa fa-check"></i><b>3.1</b> Ordinary least squares</a><ul>
<li class="chapter" data-level="3.1.1" data-path="SLR.html"><a href="SLR.html#residuals"><i class="fa fa-check"></i><b>3.1.1</b> Residuals</a></li>
<li class="chapter" data-level="3.1.2" data-path="SLR.html"><a href="SLR.html#some-algebraic-implications-of-the-ols-fit"><i class="fa fa-check"></i><b>3.1.2</b> Some algebraic implications of the OLS fit</a></li>
<li class="chapter" data-level="3.1.3" data-path="SLR.html"><a href="SLR.html#ols-estimates-for-the-fuel-consumption-example"><i class="fa fa-check"></i><b>3.1.3</b> OLS Estimates for the Fuel Consumption Example</a></li>
<li class="chapter" data-level="3.1.4" data-path="SLR.html"><a href="SLR.html#interpretation-of-the-fitted-simple-linear-regression-line-parameter-estimates"><i class="fa fa-check"></i><b>3.1.4</b> Interpretation of the fitted simple linear regression line: Parameter estimates</a></li>
<li class="chapter" data-level="3.1.5" data-path="SLR.html"><a href="SLR.html#predicting"><i class="fa fa-check"></i><b>3.1.5</b> Predicting</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="SLR.html"><a href="SLR.html#the-formal-simple-linear-regression-model"><i class="fa fa-check"></i><b>3.2</b> The formal simple linear regression model</a><ul>
<li class="chapter" data-level="3.2.1" data-path="SLR.html"><a href="SLR.html#model"><i class="fa fa-check"></i><b>3.2.1</b> Model</a></li>
<li class="chapter" data-level="3.2.2" data-path="SLR.html"><a href="SLR.html#assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.3" data-path="SLR.html"><a href="SLR.html#estimation-of-sigma2"><i class="fa fa-check"></i><b>3.2.3</b> Estimation of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="3.2.4" data-path="SLR.html"><a href="SLR.html#review-of-some-probability-results"><i class="fa fa-check"></i><b>3.2.4</b> Review of some probability results</a></li>
<li class="chapter" data-level="3.2.5" data-path="SLR.html"><a href="SLR.html#prop"><i class="fa fa-check"></i><b>3.2.5</b> Properties of the estimates</a></li>
<li class="chapter" data-level="3.2.6" data-path="SLR.html"><a href="SLR.html#special-cases"><i class="fa fa-check"></i><b>3.2.6</b> Special cases</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="SLR.html"><a href="SLR.html#simple-linear-regression-models-in-r-and-minitab"><i class="fa fa-check"></i><b>3.3</b> Simple linear regression models in R and Minitab</a><ul>
<li class="chapter" data-level="3.3.1" data-path="SLR.html"><a href="SLR.html#minitab"><i class="fa fa-check"></i><b>3.3.1</b> Minitab</a></li>
<li class="chapter" data-level="3.3.2" data-path="SLR.html"><a href="SLR.html#r"><i class="fa fa-check"></i><b>3.3.2</b> R</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="SLR.html"><a href="SLR.html#statistical-inference"><i class="fa fa-check"></i><b>3.4</b> Statistical inference</a><ul>
<li class="chapter" data-level="3.4.1" data-path="SLR.html"><a href="SLR.html#r-simulation-paraminference.r"><i class="fa fa-check"></i><b>3.4.1</b> R simulation: ParamInference.R</a></li>
<li class="chapter" data-level="3.4.2" data-path="SLR.html"><a href="SLR.html#inference-for-beta_1"><i class="fa fa-check"></i><b>3.4.2</b> Inference for <span class="math inline">\(\beta_1\)</span></a></li>
<li class="chapter" data-level="3.4.3" data-path="SLR.html"><a href="SLR.html#inference-for-beta_0"><i class="fa fa-check"></i><b>3.4.3</b> Inference for <span class="math inline">\(\beta_0\)</span></a></li>
<li class="chapter" data-level="3.4.4" data-path="SLR.html"><a href="SLR.html#inference-for-mean-response"><i class="fa fa-check"></i><b>3.4.4</b> Inference for mean response</a></li>
<li class="chapter" data-level="3.4.5" data-path="SLR.html"><a href="SLR.html#inference-for-prediction"><i class="fa fa-check"></i><b>3.4.5</b> Inference for prediction</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="SLR.html"><a href="SLR.html#analysis-of-variance-for-s.l.r."><i class="fa fa-check"></i><b>3.5</b> Analysis of variance (for s.l.r.)</a><ul>
<li class="chapter" data-level="3.5.1" data-path="SLR.html"><a href="SLR.html#anova-decomposition"><i class="fa fa-check"></i><b>3.5.1</b> ANOVA decomposition</a></li>
<li class="chapter" data-level="3.5.2" data-path="SLR.html"><a href="SLR.html#anova-table"><i class="fa fa-check"></i><b>3.5.2</b> ANOVA table</a></li>
<li class="chapter" data-level="3.5.3" data-path="SLR.html"><a href="SLR.html#special-cases-1"><i class="fa fa-check"></i><b>3.5.3</b> Special cases</a></li>
<li class="chapter" data-level="3.5.4" data-path="SLR.html"><a href="SLR.html#does-regression-on-x-explain-y"><i class="fa fa-check"></i><b>3.5.4</b> Does regression on x explain y?</a></li>
<li class="chapter" data-level="3.5.5" data-path="SLR.html"><a href="SLR.html#notes-on-the-anova-table"><i class="fa fa-check"></i><b>3.5.5</b> Notes on the ANOVA table</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="SLR.html"><a href="SLR.html#sample-correlation"><i class="fa fa-check"></i><b>3.6</b> Sample correlation</a><ul>
<li class="chapter" data-level="3.6.1" data-path="SLR.html"><a href="SLR.html#examples-of-correlation"><i class="fa fa-check"></i><b>3.6.1</b> Examples of correlation</a></li>
<li class="chapter" data-level="3.6.2" data-path="SLR.html"><a href="SLR.html#comparison-of-the-correlation-and-coefficient-of-determination-for-two-data-sets."><i class="fa fa-check"></i><b>3.6.2</b> Comparison of the correlation and coefficient of determination for two data sets.</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="SLR.html"><a href="SLR.html#assessing-the-simple-linear-regression-model-assumptions"><i class="fa fa-check"></i><b>3.7</b> Assessing the simple linear regression model assumptions</a><ul>
<li class="chapter" data-level="3.7.1" data-path="SLR.html"><a href="SLR.html#assumptions-1"><i class="fa fa-check"></i><b>3.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="3.7.2" data-path="SLR.html"><a href="SLR.html#violations-and-consequences"><i class="fa fa-check"></i><b>3.7.2</b> Violations and consequences</a></li>
<li class="chapter" data-level="3.7.3" data-path="SLR.html"><a href="SLR.html#graphical-tools-for-assessment"><i class="fa fa-check"></i><b>3.7.3</b> Graphical tools for assessment</a></li>
<li class="chapter" data-level="3.7.4" data-path="SLR.html"><a href="SLR.html#cigarette"><i class="fa fa-check"></i><b>3.7.4</b> Cigarette Data</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="SLR.html"><a href="SLR.html#a-note-on-the-galton-paradox"><i class="fa fa-check"></i><b>3.8</b> A note on the Galton paradox</a><ul>
<li class="chapter" data-level="3.8.1" data-path="SLR.html"><a href="SLR.html#the-galton-paradox"><i class="fa fa-check"></i><b>3.8.1</b> The Galton paradox</a></li>
<li class="chapter" data-level="3.8.2" data-path="SLR.html"><a href="SLR.html#two-regressions"><i class="fa fa-check"></i><b>3.8.2</b> Two regressions</a></li>
<li class="chapter" data-level="3.8.3" data-path="SLR.html"><a href="SLR.html#regression-vs-orthogonal-regression"><i class="fa fa-check"></i><b>3.8.3</b> Regression vs orthogonal regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple regression</a><ul>
<li class="chapter" data-level="4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#introductory-examples-1"><i class="fa fa-check"></i><b>4.1</b> Introductory examples</a><ul>
<li class="chapter" data-level="4.1.1" data-path="multiple-regression.html"><a href="multiple-regression.html#example-1-fuel-use"><i class="fa fa-check"></i><b>4.1.1</b> Example 1: Fuel Use</a></li>
<li class="chapter" data-level="4.1.2" data-path="multiple-regression.html"><a href="multiple-regression.html#example-2-categorical-predictors"><i class="fa fa-check"></i><b>4.1.2</b> Example 2: Categorical predictors</a></li>
<li class="chapter" data-level="4.1.3" data-path="multiple-regression.html"><a href="multiple-regression.html#example-3-polynomials"><i class="fa fa-check"></i><b>4.1.3</b> Example 3: Polynomials</a></li>
<li class="chapter" data-level="4.1.4" data-path="multiple-regression.html"><a href="multiple-regression.html#example-4-nonlinear-relationships"><i class="fa fa-check"></i><b>4.1.4</b> Example 4: Nonlinear relationships</a></li>
<li class="chapter" data-level="4.1.5" data-path="multiple-regression.html"><a href="multiple-regression.html#cigarette-data-continued"><i class="fa fa-check"></i><b>4.1.5</b> Cigarette Data continued</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#least-squares-estimation-for-multiple-regression"><i class="fa fa-check"></i><b>4.2</b> Least squares estimation for multiple regression</a><ul>
<li class="chapter" data-level="4.2.1" data-path="multiple-regression.html"><a href="multiple-regression.html#estimation-of-sigma2-varepsilon"><i class="fa fa-check"></i><b>4.2.1</b> Estimation of <span class="math inline">\(\sigma^2\)</span> = Var<span class="math inline">\((\epsilon)\)</span></a></li>
<li class="chapter" data-level="4.2.2" data-path="multiple-regression.html"><a href="multiple-regression.html#estimation-of-varhatbeta"><i class="fa fa-check"></i><b>4.2.2</b> Estimation of Var<span class="math inline">\((\hat{\beta})\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="multiple-regression.html"><a href="multiple-regression.html#prediction-from-multiple-linear-regression-model"><i class="fa fa-check"></i><b>4.3</b> Prediction from multiple linear regression model</a></li>
<li class="chapter" data-level="4.4" data-path="multiple-regression.html"><a href="multiple-regression.html#regression-models-in-matrix-notation-examples"><i class="fa fa-check"></i><b>4.4</b> Regression models in matrix notation: examples</a><ul>
<li class="chapter" data-level="4.4.1" data-path="multiple-regression.html"><a href="multiple-regression.html#example-1-slr"><i class="fa fa-check"></i><b>4.4.1</b> Example 1: SLR</a></li>
<li class="chapter" data-level="4.4.2" data-path="multiple-regression.html"><a href="multiple-regression.html#example-2"><i class="fa fa-check"></i><b>4.4.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="multiple-regression.html"><a href="multiple-regression.html#the-formal-multiple-regression-model-and-properties"><i class="fa fa-check"></i><b>4.5</b> The formal multiple regression model and properties</a><ul>
<li class="chapter" data-level="4.5.1" data-path="multiple-regression.html"><a href="multiple-regression.html#concepts-random-vectors-covariance-matrix-multivariate-normal-distribution-mvn."><i class="fa fa-check"></i><b>4.5.1</b> Concepts: random vectors, covariance matrix, multivariate normal distribution (MVN).</a></li>
<li class="chapter" data-level="4.5.2" data-path="multiple-regression.html"><a href="multiple-regression.html#multiple-regression-model"><i class="fa fa-check"></i><b>4.5.2</b> Multiple regression model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="multiple-regression.html"><a href="multiple-regression.html#the-hat-matrix"><i class="fa fa-check"></i><b>4.6</b> The hat matrix</a><ul>
<li class="chapter" data-level="4.6.1" data-path="multiple-regression.html"><a href="multiple-regression.html#the-qr-decomposition-of-a-matrix"><i class="fa fa-check"></i><b>4.6.1</b> The QR Decomposition of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="multiple-regression.html"><a href="multiple-regression.html#anova-for-multiple-regression"><i class="fa fa-check"></i><b>4.7</b> ANOVA for multiple regression</a></li>
<li class="chapter" data-level="4.8" data-path="multiple-regression.html"><a href="multiple-regression.html#way-anova-model"><i class="fa fa-check"></i><b>4.8</b> 1-way ANOVA model</a><ul>
<li class="chapter" data-level="4.8.1" data-path="multiple-regression.html"><a href="multiple-regression.html#example"><i class="fa fa-check"></i><b>4.8.1</b> Example:</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="multiple-regression.html"><a href="multiple-regression.html#one-way-anova-in-regression-notation"><i class="fa fa-check"></i><b>4.9</b> One way ANOVA in regression notation</a><ul>
<li class="chapter" data-level="4.9.1" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-the-model-in-mtb-and-r"><i class="fa fa-check"></i><b>4.9.1</b> Fitting the model in MTB and R</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="multiple-regression.html"><a href="multiple-regression.html#confidence-intervals-and-hypothesis-tests-for-linear-combinations-of-boldsymbolbeta"><i class="fa fa-check"></i><b>4.10</b> Confidence intervals and hypothesis tests for linear combinations of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html"><i class="fa fa-check"></i><b>5</b> Model comparisons and testing for lack of fit</a><ul>
<li class="chapter" data-level="5.1" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#f-tests-for-comparing-two-models"><i class="fa fa-check"></i><b>5.1</b> F-tests for comparing two models</a><ul>
<li class="chapter" data-level="5.1.1" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-1"><i class="fa fa-check"></i><b>5.1.1</b> Example:</a></li>
<li class="chapter" data-level="5.1.2" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#f-test-to-compare-models"><i class="fa fa-check"></i><b>5.1.2</b> F-test to compare models:</a></li>
<li class="chapter" data-level="5.1.3" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-steam-data"><i class="fa fa-check"></i><b>5.1.3</b> Example: Steam data</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#sequential-sums-of-squares"><i class="fa fa-check"></i><b>5.2</b> Sequential sums of squares</a><ul>
<li class="chapter" data-level="5.2.1" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-3"><i class="fa fa-check"></i><b>5.2.1</b> Example:</a></li>
<li class="chapter" data-level="5.2.2" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-steam-data-1"><i class="fa fa-check"></i><b>5.2.2</b> Example: Steam data</a></li>
<li class="chapter" data-level="5.2.3" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-steam-contd-in-mtb"><i class="fa fa-check"></i><b>5.2.3</b> Example: Steam cont’d in MTB</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#testing-for-lack-of-fit"><i class="fa fa-check"></i><b>5.3</b> Testing for lack of fit</a><ul>
<li class="chapter" data-level="5.3.1" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-voltage"><i class="fa fa-check"></i><b>5.3.1</b> Example: Voltage</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#added-variable-plots"><i class="fa fa-check"></i><b>5.4</b> Added variable plots</a><ul>
<li class="chapter" data-level="5.4.1" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-steam-vs.temp-inv-prod"><i class="fa fa-check"></i><b>5.4.1</b> Example: STEAM vs. TEMP, INV, PROD</a></li>
<li class="chapter" data-level="5.4.2" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-added-variable-plot-for-prod."><i class="fa fa-check"></i><b>5.4.2</b> Example: Added variable plot for PROD.</a></li>
<li class="chapter" data-level="5.4.3" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#example-steam-data-contd"><i class="fa fa-check"></i><b>5.4.3</b> Example: Steam data cont’d</a></li>
<li class="chapter" data-level="5.4.4" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#properties-of-avps"><i class="fa fa-check"></i><b>5.4.4</b> Properties of AVPs:</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="model-comparisons-and-testing-for-lack-of-fit.html"><a href="model-comparisons-and-testing-for-lack-of-fit.html#visualising-models-in-hdim-added-variable-plots-for-the-bodyfat-data."><i class="fa fa-check"></i><b>5.5</b> Visualising Models in Hdim: added variable plots for the bodyfat data.</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html"><i class="fa fa-check"></i><b>6</b> Diagnostic methods (in more details)</a><ul>
<li class="chapter" data-level="6.1" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#model-assumptions"><i class="fa fa-check"></i><b>6.1</b> Model assumptions</a></li>
<li class="chapter" data-level="6.2" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#residuals-1"><i class="fa fa-check"></i><b>6.2</b> Residuals</a></li>
<li class="chapter" data-level="6.3" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#leverage-values"><i class="fa fa-check"></i><b>6.3</b> Leverage values</a><ul>
<li class="chapter" data-level="6.3.1" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#properties-of-h_ii"><i class="fa fa-check"></i><b>6.3.1</b> Properties of <span class="math inline">\(h_{ii}\)</span>:</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#standardised-residuals"><i class="fa fa-check"></i><b>6.4</b> Standardised residuals</a></li>
<li class="chapter" data-level="6.5" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#leave-one-out-methods"><i class="fa fa-check"></i><b>6.5</b> Leave-one-out methods</a></li>
<li class="chapter" data-level="6.6" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#other-influence-measures"><i class="fa fa-check"></i><b>6.6</b> Other influence measures</a></li>
<li class="chapter" data-level="6.7" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#testing-outliers"><i class="fa fa-check"></i><b>6.7</b> Testing outliers</a></li>
<li class="chapter" data-level="6.8" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#diagnostics-examples-two-case-studies"><i class="fa fa-check"></i><b>6.8</b> Diagnostics examples (two case studies)</a><ul>
<li class="chapter" data-level="6.8.1" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#example-1-brain-size-versus-body-gestation-period-and-litter"><i class="fa fa-check"></i><b>6.8.1</b> Example 1: Brain size versus body gestation period and litter</a></li>
<li class="chapter" data-level="6.8.2" data-path="diagnostic-methods-in-more-details.html"><a href="diagnostic-methods-in-more-details.html#example-2-rat-data"><i class="fa fa-check"></i><b>6.8.2</b> Example 2: Rat data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="special-cases-of-multiple-regression.html"><a href="special-cases-of-multiple-regression.html"><i class="fa fa-check"></i><b>7</b> Special cases of multiple regression</a><ul>
<li class="chapter" data-level="7.1" data-path="special-cases-of-multiple-regression.html"><a href="special-cases-of-multiple-regression.html#categorical-and-continuous-predictors-binary-categories"><i class="fa fa-check"></i><b>7.1</b> Categorical and continuous predictors (binary categories)</a></li>
<li class="chapter" data-level="7.2" data-path="special-cases-of-multiple-regression.html"><a href="special-cases-of-multiple-regression.html#categorical-and-continuous-predictors-more-than-two-categories"><i class="fa fa-check"></i><b>7.2</b> Categorical and continuous predictors (more than two categories)</a></li>
<li class="chapter" data-level="7.3" data-path="special-cases-of-multiple-regression.html"><a href="special-cases-of-multiple-regression.html#quadratic-terms-and-interactions"><i class="fa fa-check"></i><b>7.3</b> Quadratic terms and interactions</a></li>
<li class="chapter" data-level="7.4" data-path="special-cases-of-multiple-regression.html"><a href="special-cases-of-multiple-regression.html#an-example-with-two-continuous-and-two-categorical-predictors"><i class="fa fa-check"></i><b>7.4</b> An example with two continuous and two categorical predictors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for ST463/ST683 Linear Models 1</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="SLR" class="section level1">
<h1><span class="header-section-number">3</span> Simple Linear regression</h1>
<div id="OLS" class="section level2">
<h2><span class="header-section-number">3.1</span> Ordinary least squares</h2>
<p>We have seen some introductory examples in Section <a href="intro.html#fuel">2.1.5</a>.</p>
<p>Fuel consumption example, what is the `best fitting line’ to summarise the linear trend? <span class="math display">\[y_i =\beta_{0} + \beta_{1}x_i + \epsilon_i.\]</span></p>
<p>The method of ordinary least squares chooses <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span> to minimise:</p>
<span class="math display">\[\begin{align*}
S(\beta_{0}, \beta_{1}) &amp; =\sum_{i=1}^{n}\epsilon_i^2 \\
 &amp; = \sum_{i=1}^{n}(y_i-\beta_0-\beta_1x_i)^2\\
\end{align*}\]</span>
<p><img src="_main_files/figure-html/unnamed-chunk-19-1.png" width="432" style="display: block; margin: auto;" /></p>
<p>The least squares estimators <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> must satisfy: <span class="math inline">\(\frac{\delta S}{\delta \beta_0} = 0\)</span> and <span class="math inline">\(\frac{\delta S}{\delta \beta_1} = 0\)</span>.</p>
<span class="math display">\[\begin{align*}
\frac{\delta S}{\delta \beta_0} &amp; = - 2\sum_{i=1}^{n} (y_i-\beta_0-\beta_1x_i) \\
\frac{\delta S}{\delta \beta_1} &amp; = - 2\sum_{i=1}^{n} x_i(y_i-\beta_0-\beta_1x_i).
\end{align*}\]</span>
<p>Setting these to 0 at <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> gives:</p>
<span class="math display" id="eq:normal2" id="eq:normal1">\[\begin{align}
\sum_{i=1}^{n} (y_i-\hat{\beta}_0-\hat{\beta}_1x_i) &amp; =0 \tag{3.1}\\
\sum_{i=1}^{n} x_i(y_i-\hat{\beta}_0-\hat{\beta}_1x_i)&amp; =0. \tag{3.2}
\end{align}\]</span>
<p>These equations (<a href="SLR.html#eq:normal1">(3.1)</a> and <a href="SLR.html#eq:normal2">(3.2)</a>) are called the <strong>normal equations</strong>. From <a href="SLR.html#eq:normal1">(3.1)</a>:</p>
<span class="math display">\[\begin{align*}
\sum_{i=1}^{n} y_i-n\hat{\beta}_0-\hat{\beta}_1\sum_{i=1}^{n}x_i &amp; =0\\
\hat{\beta}_0&amp; =\bar{y}-\hat{\beta}_1\bar{x}.
\end{align*}\]</span>
<p>Substitute into <a href="SLR.html#eq:normal2">(3.2)</a>:</p>
<span class="math display">\[\begin{align*}
\sum_{i=1}^{n}x_i( y_i-\bar{y}+\hat{\beta}_1\bar{x}-\hat{\beta}_1x_i) &amp; =0\\
\sum_{i=1}^{n}x_i( y_i-\bar{y}) &amp; =\hat{\beta}_1\sum_{i=1}^{n}x_i(x_i-\bar{x})\\
\hat{\beta}_1&amp; = \frac{\sum_{i=1}^{n}x_i( y_i-\bar{y})}{\sum_{i=1}^{n}x_i(x_i-\bar{x})}\\
&amp; =\frac{\sum_{i=1}^{n}(x_i-\bar{x})( y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}\\
&amp; =\frac{S_{xy}}{S_{xx}}.
\end{align*}\]</span>
Some notation:
<span class="math display">\[\begin{align*}
S_{xx} &amp; =\sum_{i=1}^{n}(x_{i} - \bar{x})^{2} =\sum_{i=1}^{n}x_{i}^{2} - n\bar{x}^2 \\
S_{yy} &amp; =\sum_{i=1}^{n}(y_{i} - \bar{y})^{2}   =\sum_{i=1}^{n}y_{i}^{2} - n\bar{y}^2 \\
S_{xy} &amp; =\sum_{i=1}^{n}(x_{i} - \bar{x})(y_{i} - \bar{y}) = \sum_{i=1}^{n}x_{i}y_{i} - n\bar{x}\bar{y}
\end{align*}\]</span>
<p>So, the equation of the OLS fitted line is given by: <span class="math display">\[\hat{y} =\hat{\beta}_{0} + \hat{\beta}_{1}x,\]</span></p>
<p>where <span class="math display">\[\hat{\beta}_{1} = \frac{S_{xy}}{S_{xx}}\]</span> and <span class="math display">\[\hat{\beta}_{0} = \bar{y}-\hat{\beta}_1\bar{x}.\]</span></p>
<div id="residuals" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Residuals</h3>
<p>The <strong>fitted value</strong> at each observation is: <span class="math display">\[\hat{y}_i =\hat{\beta}_{0} + \hat{\beta}_{1}x_i\]</span></p>
<p>The <strong>residuals</strong> are computed as: <span class="math display">\[e_i = y_i-\hat{y}_i\]</span></p>
</div>
<div id="some-algebraic-implications-of-the-ols-fit" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Some algebraic implications of the OLS fit</h3>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\sum_{i=1}^n e_i = \sum_{i=1}^n (y_i - \hat{y}_i) = 0\)</span> (residuals sum to 0)</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n x_i e_i = \sum_{i=1}^n x_i(y_i - \hat{y}_i) = 0\)</span></p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n y_i = \sum_{i=1}^n \hat{y}_i\)</span> (from <a href="SLR.html#eq:normal1">(3.1)</a>)</p></li>
<li><p><span class="math inline">\(\bar{y} = \hat{\beta}_0+\hat{\beta}_1\bar{x}\)</span> (OLS line always goes through the mean of the sample)</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n \hat{y}_ie_i = 0\)</span> (from <a href="SLR.html#eq:normal1">(3.1)</a> and <a href="SLR.html#eq:normal2">(3.2)</a>).</p></li>
</ol>
</div>
<div id="ols-estimates-for-the-fuel-consumption-example" class="section level3">
<h3><span class="header-section-number">3.1.3</span> OLS Estimates for the Fuel Consumption Example</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>Temp
y &lt;-<span class="st"> </span>Fuel
n &lt;-<span class="st"> </span><span class="dv">8</span>

<span class="kw">cbind</span>(x, y, <span class="dt">xsq =</span> x<span class="op">^</span><span class="dv">2</span>, <span class="dt">ysq =</span> y<span class="op">^</span><span class="dv">2</span>, <span class="dt">xy =</span> x <span class="op">*</span><span class="st"> </span>y)</code></pre></div>
<pre><code>##         x    y     xsq    ysq     xy
## [1,] 28.0 12.4  784.00 153.76 347.20
## [2,] 28.0 11.7  784.00 136.89 327.60
## [3,] 32.5 12.4 1056.25 153.76 403.00
## [4,] 39.0 10.8 1521.00 116.64 421.20
## [5,] 45.9  9.4 2106.81  88.36 431.46
## [6,] 57.8  9.5 3340.84  90.25 549.10
## [7,] 58.1  8.0 3375.61  64.00 464.80
## [8,] 62.5  7.5 3906.25  56.25 468.75</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(x)</code></pre></div>
<pre><code>## [1] 351.8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(y)</code></pre></div>
<pre><code>## [1] 81.7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(x)</code></pre></div>
<pre><code>## [1] 43.975</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(y)</code></pre></div>
<pre><code>## [1] 10.2125</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 16874.76</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(y<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 859.91</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span>y)</code></pre></div>
<pre><code>## [1] 3413.11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sxx &lt;-<span class="st"> </span><span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(x)<span class="op">^</span><span class="dv">2</span>
Sxx</code></pre></div>
<pre><code>## [1] 1404.355</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Syy &lt;-<span class="st"> </span><span class="kw">sum</span>(y<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(y)<span class="op">^</span><span class="dv">2</span>
Syy</code></pre></div>
<pre><code>## [1] 25.54875</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sxy &lt;-<span class="st"> </span><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span>y) <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(x) <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(y)
Sxy</code></pre></div>
<pre><code>## [1] -179.6475</code></pre>
<p>Calculate <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>:</p>
<span class="math display">\[\begin{align*}
\hat{\beta}_{1} &amp; = \frac{S_{xy}}{S_{xx}} =   \frac{\sum_{i=1}^{n}x_{i}y_{i} - n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_{i}^{2} - n\bar{x}^{2}} \\
 &amp; =\frac{-179.65}{1404.355} = -0.128
\end{align*}\]</span>
<p><span class="math inline">\(\hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x} = 10.212 - ( - 0.128)(43.98) =15.84\)</span></p>
<p>The equation of the fitted line is <span class="math inline">\(\hat{y}= 15.84 - 0.128 x\)</span>.</p>
</div>
<div id="interpretation-of-the-fitted-simple-linear-regression-line-parameter-estimates" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Interpretation of the fitted simple linear regression line: Parameter estimates</h3>
<p>-0.128 is the estimated change in mean fuel use for a 1<span class="math inline">\(^oF\)</span> increase in temperature.</p>
<p>In theory, 15.84 is the estimated mean fuel use at a temperature of 0<span class="math inline">\(^oF\)</span>.</p>
<p>However, we have no reason to believe this is a good estimate because our data contains no information about the fuel-temperature relationship below 28<span class="math inline">\(^oF\)</span>.</p>
</div>
<div id="predicting" class="section level3">
<h3><span class="header-section-number">3.1.5</span> Predicting</h3>
<p>The fitted line allows us to predict fuel use at any temperature within the range of the data.</p>
<p>For example, at <span class="math inline">\(x=30^oF\)</span>: <span class="math display">\[\hat{y}_i = 15.84 - 0.128 \times 30 = 12.\]</span> 12 units of fuel is the estimated fuel use at <span class="math inline">\(30^oF\)</span>.</p>
<p>E.g. at <span class="math inline">\(x=40\)</span>; <span class="math inline">\(\hat{y} = 10.721\)</span>, at <span class="math inline">\(x=50\)</span>; <span class="math inline">\(\hat{y} = 9.442\)</span>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-21-1.png" width="288" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="the-formal-simple-linear-regression-model" class="section level2">
<h2><span class="header-section-number">3.2</span> The formal simple linear regression model</h2>
<p>The SLR model tries to capture two features:</p>
<ul>
<li>a linear trend and</li>
<li><p>fluctuations (scatter about that trend).</p>
<!-- In using $\hat{y}_i$ to predict the $y_i$ we make a prediction error: $e_{i}=y_i-\hat{y}_i$. This is the vertical line from the fitted line to the data point $(x_i, y_i)$. Remember that least squares criterion chooses the values of the parameters to minimize the sum of squared prediction errors. --></li>
</ul>
<p>Because of random variations in experimental conditions we do not expect to get the same value of <span class="math inline">\(y\)</span> even if we keep repeating the experiment at various fixed <span class="math inline">\(x\)</span> values.</p>
<p>SLR model tries to model the scatter about the regression line. We will have to make some assumptions about the behaviour of these chance fluctuations.</p>
<div id="model" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Model</h3>
<p>The SLR model is of the form:</p>
<p><span class="math display">\[y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}, \hspace{0.5cm}  \epsilon_{i} \sim N(0, \sigma^{2}), \hspace{0.5cm} i=1,...,n. \]</span></p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown parameters</p></li>
<li><p><span class="math inline">\(y\)</span> and <span class="math inline">\(\epsilon\)</span> are random</p></li>
<li><p><span class="math inline">\(x\)</span> is assumed non-random</p></li>
</ul>
<p>We use errors <span class="math inline">\(\epsilon_{i}\)</span> to model the chance fluctuations about the regression line (i.e. the underlying true line).</p>
<p>So the SLR model assumes that these errors, i.e. vertical distances from the observed point to the regression line, are, on average, equal to zero. It also assumes that they are normally distributed.</p>
<p>Another assumption is that the <span class="math inline">\(\epsilon_{i}\)</span> values are <strong>independent</strong> and <strong>identically distributed</strong> (IID).</p>
<p><img src="_main_files/figure-html/unnamed-chunk-22-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="assumptions" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Assumptions</h3>
<ul>
<li><p><span class="math inline">\(\mathbb{E}[\epsilon_{i}] = 0\)</span>, so <span class="math inline">\(\mathbb{E}[y_{i}] = \beta_0 + \beta_1x_i+ \mathbb{E}[\epsilon_i] = \beta_0 + \beta_1x_i\)</span>.</p></li>
<li><p>Var(<span class="math inline">\(\epsilon_i\)</span>) = <span class="math inline">\(\sigma^2\)</span>. Equivalently Var(<span class="math inline">\(y_{i}\)</span>) = <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p><span class="math inline">\(\epsilon_i\)</span> are independent (therefore <span class="math inline">\(y_{i}\)</span> also are).</p></li>
<li><p><span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>. Equivalently <span class="math inline">\(y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)\)</span>.</p></li>
</ul>
<p>NOTE: if <span class="math inline">\(x_i\)</span> are random then the model says that <span class="math inline">\(\mathbb{E}[y_{i}|x_i] = \beta_0 + \beta_1x_i\)</span> and Var(<span class="math inline">\(y_{i}|x_i\)</span>) = <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="estimation-of-sigma2" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Estimation of <span class="math inline">\(\sigma^2\)</span></h3>
<p>NOTE: <span class="math inline">\(\sigma^2\)</span> = Var(<span class="math inline">\(\epsilon_i\)</span>)</p>
<p>The <strong>errors</strong> <span class="math inline">\(\epsilon_i\)</span> are not observable, but the <strong>residuals</strong>, <span class="math inline">\(e_i\)</span> should have similar properties.</p>
<p>We estimate <span class="math inline">\(\sigma^2\)</span> by</p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{\sum_{i=1}^n e_i^2}{n-2}.\]</span></p>
<p><span class="math inline">\(n-2\)</span> is the degrees of freedom and <span class="math inline">\(\sum_{i=1}^n e_i^2\)</span> is called the residual sum of squares, denoted <span class="math inline">\(\mbox{SSE}\)</span>.</p>
</div>
<div id="review-of-some-probability-results" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Review of some probability results</h3>
<p>Let <span class="math inline">\(U\)</span>, <span class="math inline">\(W\)</span> and <span class="math inline">\(Z\)</span> be three random variables:</p>
<p><span class="math inline">\(\mathbb{E}[U]\)</span> = mean of the distribution of <span class="math inline">\(U\)</span></p>
<p>Var <span class="math inline">\((U) = \mathbb{E}[U^2] - (\mathbb{E}[U])^2\)</span></p>
<p>Cov(<span class="math inline">\(U,U\)</span>) = Var(<span class="math inline">\(U\)</span>)</p>
<p>Cov(<span class="math inline">\(U, W\)</span>) = <span class="math inline">\(\mathbb{E}[UW] - \mathbb{E}[U]\mathbb{E}[W]\)</span></p>
<p>If <span class="math inline">\(U\)</span> and <span class="math inline">\(W\)</span> are uncorrelated or independent then Cov(<span class="math inline">\(U,W\)</span>) = 0</p>
<p><span class="math inline">\(\mbox{Corr}(U,W) = \frac{\mbox{Cov}(U,W)}{\sqrt{\mbox{Var}(U)\mbox{Var}(W)}}\)</span></p>
<p>For constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:</p>
<p><span class="math inline">\(\mathbb{E}[aU+bW] = a\mathbb{E}[U] + b\mathbb{E}[W]\)</span></p>
<p>Var(<span class="math inline">\(aU \pm bW\)</span>) = <span class="math inline">\(a^2\)</span>Var[<span class="math inline">\(U\)</span>] + <span class="math inline">\(b^2\)</span>Var[<span class="math inline">\(W\)</span>] <span class="math inline">\(\pm\)</span> <span class="math inline">\(2ab\)</span>Cov(<span class="math inline">\(U\)</span>,<span class="math inline">\(W\)</span>)</p>
<p>Cov(<span class="math inline">\(aU+bW, cZ\)</span>) = <span class="math inline">\(ac\)</span>Cov(<span class="math inline">\(U\)</span>,<span class="math inline">\(Z\)</span>) + <span class="math inline">\(bc\)</span>Cov(<span class="math inline">\(W\)</span>,<span class="math inline">\(Z\)</span>)</p>
</div>
<div id="prop" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Properties of the estimates</h3>
<p>When the model holds:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat{\beta}_1 \sim N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)\)</span></p></li>
<li><p><span class="math inline">\(\hat{\beta}_0 \sim N\left(\beta_0, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\right)\)</span></p></li>
<li><p>Cov<span class="math inline">\((\hat{\beta}_0, \hat{\beta}_1) = -\sigma^2\frac{\bar{x}}{S_{xx}}\)</span></p></li>
<li><p><span class="math inline">\(\hat{y} \sim N\left(\beta_0 + \beta_1x, \sigma^2\left(\frac{1}{n}+ \frac{(x-\bar{x})^2}{S_{xx}}\right)\right)\)</span></p></li>
<li><p><span class="math inline">\((n-2)\frac{\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{(n-2)}\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{E}[\hat{\sigma}^2] = \sigma^2\)</span></p></li>
</ol>
<p>Proof of (1):</p>
<p>First show that <span class="math inline">\(\mathbb{E}[\hat{\beta}_1] = \beta_1\)</span>.</p>
<span class="math display">\[\begin{align*}
\hat{\beta}_1 &amp; = \frac{S_{xy}}{S_{xx}} \\
 &amp; = \frac{\sum_{i=1}^{n}(x_i-\bar{x})( y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}  \\
 &amp; = \frac{\sum_{i=1}^{n}(x_i-\bar{x})y_i}{\sum_{i=1}^{n}(x_i-\bar{x})^2}  \\
 &amp; =  \sum_{i = 1}^n a_iy_i
\end{align*}\]</span>
<!-- %\mbox{ (since} \sum_{i=1}^{n}(x_i-\bar{x}) = 0) -->
<p>where <span class="math inline">\(a_i\)</span> depend only on <span class="math inline">\(x\)</span> and are NOT random.</p>
<p>By linearity of expectation:</p>
<span class="math display">\[\begin{align*}
\mathbb{E}[\hat{\beta}_1] &amp; = \sum_{i = 1}^n a_i\mathbb{E}[y_i]\\
&amp; = \sum_{i = 1}^n a_i (\beta_0+\beta_1x_i) \mbox{  (from the model assumptions)}\\
&amp; = \beta_0\sum_{i = 1}^n a_i+\beta_1\sum_{i = 1}^n a_i x_i
\end{align*}\]</span>
<p>But</p>
<p><span class="math display">\[\sum_{i = 1}^n a_i = \frac{\sum_{i=1}^{n}(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2} = 0,\]</span></p>
<p>And</p>
<span class="math display">\[\begin{align*}
\sum_{i = 1}^n a_i x_i &amp; =  \frac{\sum_{i=1}^{n}x_i(x_i-\bar{x})}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \\
&amp; =  \frac{\sum_{i=1}^{n}x_i^2-n\bar{x}^2}{S_{xx}} \\
&amp; = \frac{S_{xx}}{S_{xx}} = 1.
\end{align*}\]</span>
<p>So <span class="math inline">\(\mathbb{E}[\hat{\beta}_1] = \beta_1\)</span> as required.</p>
<p>Second, show that Var(<span class="math inline">\(\hat{\beta}_1\)</span>) = <span class="math inline">\(\frac{\sigma^2}{S_{xx}}\)</span></p>
<span class="math display">\[\begin{align*}
\mbox{Var}(\hat{\beta}_1) &amp; =  \mbox{Var} \left( \sum_{i = 1}^n a_i y_i \right)\\
 &amp; =  \sum_{i = 1}^n a_i^2 \mbox{Var}(y_i) \mbox{(since $y_i$s are independent)}\\
  &amp; =  \sigma^2 \sum_{i = 1}^n a_i^2\\
  &amp; =  \sigma^2 \sum_{i = 1}^n \left( \frac{x_i-\bar{x}}{\sum_{i=1}^{n}(x_i-\bar{x})^2} \right)^2\\
 &amp; =  \sigma^2  \frac{\sum_{i = 1}^n (x_i-\bar{x})^2}{(\sum_{i=1}^{n}(x_i-\bar{x})^2)^2} \\
  &amp; =  \sigma^2  \frac{S_{xx}}{(S_{xx})^2} \\
  &amp; =   \frac{ \sigma^2}{S_{xx}} \mbox{   (as required)}
\end{align*}\]</span>
<p>Finally, the normality assumption follows as <span class="math inline">\(\hat{\beta}_1\)</span> is a linear combination of normal random variables (<span class="math inline">\(y_i\)</span>s).</p>
<p>Proof of (2):</p>
<p>First show that <span class="math inline">\(\mathbb{E}[\hat{\beta}_0] = \beta_0\)</span>.</p>
<span class="math display">\[\begin{align*}
\mathbb{E}[\hat{\beta}_0] &amp; = \mathbb{E}[\bar{y} - \hat{\beta}_1\bar{x}]\\
&amp; = \mathbb{E}[\bar{y}] - \beta_1 \bar{x}\\
&amp; = \frac{1}{n}\sum_{i = 1}^n\mathbb{E}[y_i] - \beta_1 \bar{x}\\
&amp; = \frac{1}{n}\sum_{i = 1}^n (\beta_0+ \beta_1 x_i) - \beta_1 \bar{x}\\
&amp; = \frac{1}{n}( n\beta_0 + \beta_1 \sum_{i = 1}^n x_i) - \beta_1 \bar{x}\\
&amp; = \beta_0 + \beta_1 \bar{x} - \beta_1 \bar{x}\\
&amp; = \beta_0 \mbox{ (as required)}
\end{align*}\]</span>
<p>Second, show that Var(<span class="math inline">\(\hat{\beta}_0\)</span>) = <span class="math inline">\(\sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\)</span></p>
<span class="math display">\[\begin{align*}
\mbox{Var}(\hat{\beta}_0) &amp; =\mbox{Var}(\bar{y} - \hat{\beta}_1\bar{x}) \\
&amp; = \mbox{Var}(\bar{y}) + \bar{x}^2 \mbox{Var}(\hat{\beta}_1) - 2\bar{x}\mbox{Cov}(\bar{y}, \hat{\beta}_1)
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
\mbox{Cov}(\bar{y}, \hat{\beta}_1) &amp; =\mbox{Cov}\left( \frac{1}{n}\sum_{i = 1}^n y_i, \sum_{i = 1}^n a_iy_i \right)\\
 &amp; =\sum_{i = 1}^n  \sum_{j = 1}^n \frac{1}{n} a_i\mbox{Cov}(y_i, y_j)\\
 &amp; =\frac{1}{n} \sum_{i = 1}^n  \sum_{j = 1}^n a_i\mbox{Cov}(y_i, y_j)\\
 &amp; =\frac{1}{n} \sum_{i = 1}^n   a_i\mbox{Cov}(y_i, y_i) \mbox{ (since $y_i$ are indep.)} \\
&amp; =\frac{\sigma^2}{n} \sum_{i = 1}^n   a_i\\
&amp; = 0 
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
\mbox{Var}(\hat{\beta}_0) &amp; = \mbox{Var}(\bar{y}) + \bar{x}^2 \mbox{Var}(\hat{\beta}_1) \\
&amp; =  \frac{1}{n^2} \sum_{i = 1}^n  \mbox{Var}(y_i) + \bar{x}^2 \frac{\sigma^2}{S_{xx}}\\
&amp; =  \frac{1}{n^2} n\sigma^2 + \bar{x}^2 \frac{\sigma^2}{S_{xx}}\\
&amp; =  \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right) \mbox{ (as required)}
\end{align*}\]</span>
<p>Finally, the normality assumption follows as <span class="math inline">\(\hat{\beta}_0\)</span> is a linear combination of normal random variables (<span class="math inline">\(y_i\)</span>s and <span class="math inline">\(\hat{\beta}_1\)</span>).</p>
<p>Proof of (3):</p>
<span class="math display">\[\begin{align*}
\mbox{Cov}(\hat{\beta}_0, \hat{\beta}_1) &amp; =\mbox{Cov}(\bar{y} - \hat{\beta}_1\bar{x}, \hat{\beta}_1)\\
&amp; =\mbox{Cov}(\bar{y}, \hat{\beta}_1) - \mbox{Cov}(\hat{\beta}_1\bar{x}, \hat{\beta}_1) \\
&amp; = 0 - \bar{x}\mbox{Cov}(\hat{\beta}_1, \hat{\beta}_1) \\
&amp; = -\bar{x} \frac{\sigma^2}{S_{xx}} 
\end{align*}\]</span>
<p>Proof of (4):</p>
<p>First show that <span class="math inline">\(\mathbb{E}[\hat{y}] = \beta_0 + \beta_1 x\)</span>.</p>
<span class="math display">\[\begin{align*}
\mathbb{E}[\hat{y}] &amp; = \mathbb{E}[\hat{\beta}_0 + \hat{\beta}_1x]\\
&amp; = \mathbb{E}[\hat{\beta}_0] + \mathbb{E}[\hat{\beta}_1]x\\
&amp; = \beta_0 + \beta_1 x \mbox{ (as required)}
\end{align*}\]</span>
<p>Second, show that Var(<span class="math inline">\(\hat{y}\)</span>) = <span class="math inline">\(\sigma^2\left(\frac{1}{n} + \frac{(x-\bar{x})^2}{S_{xx}}\right)\)</span></p>
<span class="math display">\[\begin{align*}
\mbox{Var}(\hat{y}) &amp; =\mbox{Var}(\hat{\beta}_0 + \hat{\beta}_1 x) \\
   &amp; =\mbox{Var}(\bar{y} - \hat{\beta}_1\bar{x} + \hat{\beta}_1 x) \\
   &amp; =\mbox{Var}(\bar{y} + \hat{\beta}_1(x - \bar{x})) \\
    &amp; =\mbox{Var}(\bar{y}) + (x - \bar{x})^2 \mbox{Var}(\hat{\beta}_1) + 2(x - \bar{x})\mbox{Cov}(\bar{y}, \hat{\beta}_1)\\
        &amp; =\frac{\sigma^2}{n} + (x - \bar{x})^2 \frac{\sigma^2}{S_{xx}}\\
        &amp; =\sigma^2\left(\frac{1}{n} +  \frac{(x - \bar{x})^2}{S_{xx}}\right) \quad \mbox{ (as required)}.
\end{align*}\]</span>
<p>Finally, the normality assumption follows as <span class="math inline">\(\hat{y}\)</span> is a linear combination of <span class="math inline">\(y_i\)</span>s.</p>
</div>
<div id="special-cases" class="section level3">
<h3><span class="header-section-number">3.2.6</span> Special cases</h3>
<ol style="list-style-type: decimal">
<li><p>At <span class="math inline">\(x = 0\)</span>, <span class="math inline">\(\hat{y} = \hat{\beta}_0\)</span>.</p></li>
<li><p>At <span class="math inline">\(x = x_i\)</span>, <span class="math inline">\(\hat{y} = \hat{y}_i\)</span>.</p></li>
</ol>
<span class="math display">\[\begin{align*}
\mbox{Var}(\hat{y}_i) &amp; =\sigma^2\left(\frac{1}{n} +  \frac{(x_i - \bar{x})^2}{S_{xx}}\right)\\
     &amp; =\sigma^2h_{ii}
\end{align*}\]</span>
<p>NOTE: <span class="math inline">\(h_{ii} = \left(\frac{1}{n} + \frac{(x_i - \bar{x})^2}{S_{xx}}\right)\)</span> is a distance value (see later!)</p>
</div>
</div>
<div id="simple-linear-regression-models-in-r-and-minitab" class="section level2">
<h2><span class="header-section-number">3.3</span> Simple linear regression models in R and Minitab</h2>
<div id="minitab" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Minitab</h3>
<pre><code>
Regression Analysis: Fuel versus Temp

Analysis of Variance

Source         DF   Adj SS   Adj MS  F-Value  P-Value
Regression      1  22.9808  22.9808    53.69    0.000
  Temp          1  22.9808  22.9808    53.69    0.000
Error           6   2.5679   0.4280
  Lack-of-Fit   5   2.3229   0.4646     1.90    0.500
  Pure Error    1   0.2450   0.2450
Total           7  25.5488


Model Summary

       S    R-sq  R-sq(adj)  R-sq(pred)
0.654209  89.95%     88.27%      81.98%


Coefficients

Term         Coef  SE Coef  T-Value  P-Value   VIF
Constant   15.838    0.802    19.75    0.000
Temp      -0.1279   0.0175    -7.33    0.000  1.00


Regression Equation

Fuel = 15.838 -0.1279Temp
</code></pre>
</div>
<div id="r" class="section level3">
<h3><span class="header-section-number">3.3.2</span> R</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">lm</span>(Fuel<span class="op">~</span>Temp)
<span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Fuel ~ Temp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5663 -0.4432 -0.1958  0.2879  1.0560 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 15.83786    0.80177  19.754 1.09e-06 ***
## Temp        -0.12792    0.01746  -7.328  0.00033 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6542 on 6 degrees of freedom
## Multiple R-squared:  0.8995, Adjusted R-squared:  0.8827 
## F-statistic: 53.69 on 1 and 6 DF,  p-value: 0.0003301</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(fit)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Fuel
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Temp       1 22.9808  22.981  53.695 0.0003301 ***
## Residuals  6  2.5679   0.428                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
<div id="statistical-inference" class="section level2">
<h2><span class="header-section-number">3.4</span> Statistical inference</h2>
<div id="r-simulation-paraminference.r" class="section level3">
<h3><span class="header-section-number">3.4.1</span> R simulation: ParamInference.R</h3>
<p>Reminder: the linear relationship <span class="math inline">\(\mathbb{E}[y_{i}] = \beta_{0} + \beta_{1}x_{i}\)</span> is the <strong>underlying true line</strong> and the values of its parameters (intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span>) are unknown. We estimate the parameters with <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>. These parameter estimates have sampling distributions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#~~~~~~~~~~~~~~~ Simulation</span>

<span class="co">#~~~~~~~~~~~~~~~ True model parameters:</span>
beta_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="dv">1</span>
beta_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="dv">2</span>
sigma_sq &lt;-<span class="st"> </span><span class="dv">2</span> 

<span class="co"># Simulate a dataset:</span>
n &lt;-<span class="st"> </span><span class="dv">100</span> <span class="co">#number of observations in the sample</span>

x &lt;-<span class="st"> </span><span class="kw">runif</span>(n, <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)
e &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="kw">sqrt</span>(sigma_sq)) 
y &lt;-<span class="st"> </span>beta_<span class="dv">0</span> <span class="op">+</span><span class="st"> </span>x <span class="op">*</span><span class="st"> </span>beta_<span class="dv">1</span>  <span class="op">+</span><span class="st"> </span>e

<span class="kw">plot</span>(x, y)
<span class="kw">abline</span>(beta_<span class="dv">0</span>, beta_<span class="dv">1</span>, <span class="dt">col =</span> <span class="dv">2</span> ) <span class="co"># true model</span>
<span class="kw">points</span>(<span class="kw">mean</span>(x), <span class="kw">mean</span>(y), <span class="dt">col =</span> <span class="dv">3</span>)
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x) <span class="co">#fit the model</span>
<span class="kw">abline</span>(<span class="kw">coef</span>(fit)[<span class="dv">1</span>], <span class="kw">coef</span>(fit)[<span class="dv">2</span>], <span class="dt">col =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimated coefficients</span>
<span class="kw">coef</span>(fit)</code></pre></div>
<pre><code>## (Intercept)           x 
##   0.8221711   2.0056686</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(<span class="kw">residuals</span>(fit)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(n<span class="op">-</span><span class="dv">2</span>))
s<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 2.28467</code></pre>
<!-- # can also obtain s^2 from: anova(fit)[["Mean Sq"]][2] or summary(fit) gives s -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># some useful calculations for dist of estimates</span>

x_bar &lt;-<span class="st"> </span><span class="kw">mean</span>(x)
Sxx &lt;-<span class="st"> </span><span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>(x_bar<span class="op">^</span><span class="dv">2</span>)

var_beta_<span class="dv">0</span> &lt;-<span class="st"> </span>sigma_sq <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n <span class="op">+</span><span class="st"> </span>x_bar<span class="op">^</span><span class="dv">2</span><span class="op">/</span>Sxx)
var_beta_<span class="dv">1</span> &lt;-<span class="st"> </span>sigma_sq <span class="op">/</span>Sxx

est_cov &lt;-<span class="st"> </span><span class="op">-</span>sigma_sq <span class="op">*</span><span class="st">  </span>x_bar<span class="op">/</span>Sxx <span class="co"># estimated covariance from one sample</span>

se_fit &lt;-<span class="st"> </span><span class="kw">sqrt</span>(sigma_sq <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n <span class="op">+</span><span class="st"> </span>(x <span class="op">-</span><span class="st"> </span>x_bar)<span class="op">^</span><span class="dv">2</span><span class="op">/</span>Sxx))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#~~~~~~~~~~~~~~~~~~~~ Repeat the simulation</span>

N &lt;-<span class="st"> </span><span class="dv">500</span> <span class="co">#number of simulations</span>
estimates &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, N, <span class="dv">3</span>) <span class="co"># to save param estimates</span>



<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N)
{
  x &lt;-<span class="st"> </span><span class="kw">runif</span>(n, <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)
  e &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="kw">sqrt</span>(sigma_sq)) 
  y &lt;-<span class="st"> </span>beta_<span class="dv">0</span> <span class="op">+</span><span class="st"> </span>x <span class="op">*</span><span class="st"> </span>beta_<span class="dv">1</span>  <span class="op">+</span><span class="st"> </span>e
  
  fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x)
  estimates[i, <span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">coef</span>(fit)[<span class="dv">1</span>]
  estimates[i, <span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">coef</span>(fit)[<span class="dv">2</span>]
  estimates[i, <span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">anova</span>(fit)[[<span class="st">&quot;Mean Sq&quot;</span>]][<span class="dv">2</span>] <span class="co"># sigamsq</span>
}

<span class="kw">plot</span>(x,y)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>)<span class="kw">abline</span>(estimates[i, <span class="dv">1</span>], estimates[i, <span class="dv">2</span>], <span class="dt">col =</span> <span class="st">&quot;grey&quot;</span>) <span class="co"># N is too many points</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-27-1.png" width="672" style="display: block; margin: auto;" /></p>
<!-- cov(estimates[,1], estimates[,2]) # covariance by simulation -->
<!-- est_cov #compare to -->
<!-- plot(estimates[,1], estimates[,2]) #visualise covariance -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#~~~~~~~~~~~~~~ plot the estimates</span>

<span class="co">#beta_0</span>
<span class="kw">hist</span>(estimates[,<span class="dv">1</span>], <span class="dt">breaks =</span> <span class="dv">50</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>, <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">0</span>]), <span class="dt">main =</span>  <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">0</span>]))
<span class="kw">curve</span>(<span class="kw">dnorm</span>(x, beta_<span class="dv">0</span>, <span class="kw">sqrt</span>(var_beta_<span class="dv">0</span>) ), <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#beta_1</span>
<span class="kw">hist</span>(estimates[,<span class="dv">2</span>], <span class="dt">breaks =</span> <span class="dv">50</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>, <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]), <span class="dt">main =</span>  <span class="kw">expression</span>(<span class="kw">hat</span>(beta)[<span class="dv">1</span>]))
<span class="kw">curve</span>( <span class="kw">dnorm</span>(x, beta_<span class="dv">1</span>, <span class="kw">sqrt</span>(var_beta_<span class="dv">1</span>)) , <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-2.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#sigma_sq</span>
<span class="kw">hist</span>((n<span class="op">-</span><span class="dv">2</span>)<span class="op">/</span>sigma_sq <span class="op">*</span>estimates[,<span class="dv">3</span>], <span class="dt">breaks =</span> <span class="dv">50</span>, <span class="dt">freq =</span> <span class="ot">FALSE</span>, <span class="dt">xlab =</span> <span class="kw">expression</span>(<span class="kw">hat</span>(sigma)<span class="op">^</span><span class="dv">2</span>), <span class="dt">main =</span>  <span class="kw">expression</span>(<span class="kw">hat</span>(sigma)<span class="op">^</span><span class="dv">2</span>))
<span class="kw">curve</span>( <span class="kw">dchisq</span>(x, <span class="dt">df =</span> n<span class="op">-</span><span class="dv">2</span>), <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>NOTE: superimposed curves are calculated from one (first) sample of the data and the known parameters. The distributions are from <a href="SLR.html#prop">3.2.5</a>:</p>
<ul>
<li><p><span class="math inline">\(\hat{\beta}_1 \sim N\left(\beta_1,\frac{\sigma^2}{S_{xx}}\right)\)</span></p></li>
<li><p><span class="math inline">\(\hat{\beta}_0 \sim N\left(\beta_0, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\right)\)</span></p></li>
<li><p><span class="math inline">\((n-2)\frac{\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{(n-2)}\)</span></p></li>
</ul>
</div>
<div id="inference-for-beta_1" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Inference for <span class="math inline">\(\beta_1\)</span></h3>
<div id="confidence-interval" class="section level4">
<h4><span class="header-section-number">3.4.2.1</span> Confidence Interval</h4>
<p><span class="math inline">\(\hat{\beta}_1\)</span> estimates <span class="math inline">\(\beta_1\)</span>, for example the change in fuel use for a 1<span class="math inline">\(^oF\)</span> increase in temperature.</p>
<p>We would like to construct a confidence interval for <span class="math inline">\(\beta_1\)</span>. This will give us an interval where we are confident the true <span class="math inline">\(\beta_1\)</span> lies.</p>
<p>The key to obtaining a C.I. is the fact that:</p>
<p><span class="math display">\[\hat{\beta}_1 \sim N(\beta_1, \frac{\sigma^2}{S_{xx}}).\]</span></p>
<p>Equivalently</p>
<p><span class="math display">\[\frac{\hat{\beta}_{1} - \beta_{1}}{\sigma/\sqrt{S_{xx}}} \sim N(0,1).\]</span></p>
<p>And, when we replace <span class="math inline">\(\sigma\)</span> by <span class="math inline">\(\hat{\sigma}\)</span> we have:</p>
<p><span class="math display">\[\frac{\hat{\beta}_{1} - \beta_{1}}{\hat{\sigma}/\sqrt{S_{xx}}} \sim  t_{n-2}.\]</span></p>
<p>The df is <span class="math inline">\(n-2\)</span> because this is the df associated with the estimate of <span class="math inline">\(\sigma\)</span>.</p>
<p>In general, when:</p>
<p><span class="math display">\[\frac{\mbox{Est - parameter}}{\mbox{S.E.(Est)}} \sim \mbox{distribution}.\]</span></p>
<p>A C.I. for the parameter is given by:</p>
<p><span class="math display">\[\mbox{Est} \pm \mbox{(quantile from distribution)} \times \mbox{S.E.(Est)}.\]</span></p>
<p>A <span class="math inline">\((1-\alpha)\times 100\%\)</span> confidence interval for <span class="math inline">\(\beta_{1}\)</span>:</p>
<p><span class="math display">\[\hat{\beta}_{1} \pm t_{n-2}(\alpha/2) \times \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}}.\]</span></p>
<p>For the fuel use data, a <span class="math inline">\(95\%\)</span> C.I. for <span class="math inline">\(\beta_1\)</span> is:</p>
<span class="math display">\[\begin{align*}
\hat{\beta}_1 &amp;\pm t_6(0.025) \times \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}}\\
&amp; = -0.128 \pm 2.45 \times 0.018\\
&amp; = (-0.171, -0.085)
\end{align*}\]</span>
<p>We are <span class="math inline">\(95\%\)</span> confident that the average fuel use drop is between 0.085 and 0.171.</p>
</div>
<div id="hypothesis-test" class="section level4">
<h4><span class="header-section-number">3.4.2.2</span> Hypothesis test</h4>
<p>In some settings we may wish to test:</p>
<p><span class="math inline">\(H_{0}: \beta_{1} = 0\)</span> versus <span class="math inline">\(H_{A}: \beta_{1} \ne 0\)</span>.</p>
<p>The null hypothesis here is that <span class="math inline">\(\mathbb{E}[y] = \beta_0\)</span> i.e. <span class="math inline">\(\mathbb{E}[y]\)</span> is not linearly related to <span class="math inline">\(x\)</span>.</p>
<p>Under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math display">\[t_{obs} = \frac{\hat{\beta}_{1} - 0}{\hat{\sigma}/\sqrt{S_{xx}}} \sim t_{n-2}.\]</span></p>
<p>P-value = <span class="math inline">\(P[T_{n-2} \geq |t_{obs}|]\)</span></p>
<p>Reject <span class="math inline">\(H_0\)</span> for small p-values, typically <span class="math inline">\(p&lt; 0.05\)</span>.</p>
<p>In the fuel use example:</p>
<p><span class="math display">\[t_{obs} = \frac{-0.128-0}{0.018} = -7.33\]</span></p>
<p>and p-value <span class="math inline">\(&lt; 0.001\)</span>, so we reject <span class="math inline">\(H_0\)</span> and conclude that <span class="math inline">\(\beta_{1} \ne 0\)</span>.</p>
<p>We could also test <span class="math inline">\(H_0: \beta_1=b\)</span> by computing:</p>
<p><span class="math display">\[\frac{\hat{\beta}_{1} - b}{\mbox{S.E}.(\hat{\beta}_{1})}.\]</span></p>
</div>
</div>
<div id="inference-for-beta_0" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Inference for <span class="math inline">\(\beta_0\)</span></h3>
<p>A <span class="math inline">\(95\%\)</span> C.I. for <span class="math inline">\(\beta_0\)</span> is:</p>
<p><span class="math display">\[\hat{\beta}_{0} \pm t_{n-2}(\alpha/2) \times \mbox{S.E.}(\hat{\beta}_{0}).\]</span></p>
<p>Note: <span class="math inline">\(\mbox{S.E.}(\hat{\beta}_0) = \sqrt{\hat{\sigma}^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)}\)</span></p>
<p>For the fuel use data:</p>
<span class="math display">\[\begin{align*}
&amp; = 15.84 \pm 2.45 \times 0.8018\\
&amp; = (13.88, 17.80)
\end{align*}\]</span>
<p>We can also test for a particular value of <span class="math inline">\(\beta_0\)</span>, e.g.</p>
<p><span class="math inline">\(H_0: \beta_0 = 0\)</span> vs. <span class="math inline">\(H_A: \beta_0 \neq 0\)</span></p>
<p>The null hypothesis here is that <span class="math inline">\(\mathbb{E}[y] = \beta_1 x\)</span> i.e. the line passes through the origin.</p>
<p>The test statistic is</p>
<p><span class="math display">\[\frac{\hat{\beta}_{0} - \beta_0}{\mbox{S.E.}(\hat{\beta}_{0})} = 19.75.\]</span></p>
<p>for the fuel data.</p>
<p>P-value <span class="math inline">\(= 2P[T_6 \geq 19.75] &lt;0.001.\)</span></p>
<p>Note: This is for illustration, in practice with this particular data we would not do this. Why?</p>
</div>
<div id="inference-for-mean-response" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Inference for mean response</h3>
<p>Suppose we want to estimate <span class="math inline">\(\mu=\mathbb{E}[y]\)</span> at a particular value of <span class="math inline">\(x\)</span>.</p>
<p>At <span class="math inline">\(x_0\)</span> let:</p>
<p><span class="math display">\[\mu_0 = \mathbb{E}[y_0] = \beta_0 + \beta_1 x_0\]</span></p>
<p>We can estimate <span class="math inline">\(\mu_0\)</span> by:</p>
<p><span class="math display">\[\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 x_0.\]</span></p>
<p>e.g. we estimate the mean fuel use at temperature <span class="math inline">\(50^oF\)</span> by</p>
<p><span class="math display">\[\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 \times 50 =15.84-0.128 \times 50 =9.44.\]</span></p>
<p>A <span class="math inline">\(95\%\)</span> C.I. for <span class="math inline">\(\mu_0\)</span> is given by</p>
<span class="math display">\[\begin{align*}
 \mbox{Est} &amp;\pm t_{n-2}(\alpha/2) \times \mbox{S.E.(Est)}\\
 \hat{y}_0 &amp;\pm t_{n-2}(\alpha/2) \times \mbox{S.E.}_{\mbox{fit}}(\hat{y}_0)
 \end{align*}\]</span>
<p>where</p>
<span class="math display">\[\begin{align*}
\mbox{S.E.}_{\mbox{fit}}(\hat{y}_0)&amp; =\hat{\sigma}\sqrt{h_{00}}\\
&amp; = \hat{\sigma}\sqrt{\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}}\\
&amp; = 0.65 \times \sqrt{0.15}=0.254
\end{align*}\]</span>
<p>The <span class="math inline">\(95\%\)</span> C.I. is</p>
<span class="math display">\[\begin{align*}
&amp; = 9.44 \pm 2.45 \times 0.254\\
&amp; = (8.8, 10.1)
\end{align*}\]</span>
<p>This interval contains the true mean fuel use at <span class="math inline">\(50^oF\)</span> with <span class="math inline">\(95\%\)</span> confidence.</p>
</div>
<div id="inference-for-prediction" class="section level3">
<h3><span class="header-section-number">3.4.5</span> Inference for prediction</h3>
<p>Suppose we want to predict the response at a particular value of <span class="math inline">\(x\)</span>.</p>
<p>At <span class="math inline">\(x_0\)</span>, let <span class="math inline">\(y_0\)</span> be the unobserved response.</p>
<p>From our model:</p>
<p><span class="math display">\[y_0 =\mu_0 + \epsilon= \beta_0 + \beta_1 x_0 + \epsilon\]</span></p>
<p>and we estimate (predict) it by:</p>
<p><span class="math display">\[\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 x_0.\]</span></p>
<p>Note: <strong>estimation of a random variable is called prediction</strong>.</p>
<p>In our example, we predict that the fuel use at temp <span class="math inline">\(50^oF\)</span> as:</p>
<p><span class="math display">\[\hat{y}_0 = 15.84-0.128\times 50=9.44\]</span></p>
<p>Note that the prediction of future response equals the estimate of the mean response, however the associated standard errors (and hence confidence intervals) are different.</p>
<p>Confidence intervals for random variables are called <strong>prediction intervals</strong> (PIs).</p>
<p>In our prediction of <span class="math inline">\(y_0\)</span> by <span class="math inline">\(\hat{y}_0\)</span> the prediction error <span class="math inline">\(y_0-\hat{y}_0\)</span> is with variance:</p>
<span class="math display">\[\begin{align*}
 \mbox{Var}(y_0-\hat{y}_0)&amp; = \mbox{Var}(y_0)+ \mbox{Var}(\hat{y}_0)\mbox{       (indep as $y_0$ is out of sample)}\\
&amp; =\sigma^2 + \sigma^2\times h_{00} \\
 &amp; = \sigma^2(1+h_{00})\\
&amp; = \sigma^2\left(1+\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}\right)
\end{align*}\]</span>
<p>The <span class="math inline">\(\mbox{S.E.}\)</span> of the prediction is then:</p>
<p><span class="math display">\[\mbox{S.E.}_{\mbox{pred}} (\hat{y}_0) = \hat{\sigma}\sqrt{1+\frac{1}{n}+\frac{(x_0-\bar{x})^2}{S_{xx}}}\]</span></p>
<p>The <span class="math inline">\(95\%\)</span> prediction interval is</p>
<p><span class="math display">\[\hat{y}_0 \pm t_{n-2}(\alpha/2) \times \mbox{S.E.}_{\mbox{pred}} (\hat{y}_0)\]</span></p>
<p>For <span class="math inline">\(x_0\)</span> = 50, <span class="math inline">\(\hat{y}_0 =9.442\)</span>:</p>
<p><span class="math display">\[\mbox{S.E.}_{\mbox{pred}}= \sqrt{1+0.15} \times 0.654 = 0.702\]</span></p>
<p>So the <span class="math inline">\(95\%\)</span> P.I. is:</p>
<span class="math display">\[\begin{align*}
&amp; = 9.44 \pm 2.45 \times 0.702\\
&amp; = (7.72, 11.16)
\end{align*}\]</span>
<p>We are <span class="math inline">\(95\%\)</span> sure that the interval contains the actual fuel use on a week with temp = <span class="math inline">\(50^oF\)</span>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-29-1.png" width="384" style="display: block; margin: auto;" /></p>
<p><img src="_main_files/figure-html/unnamed-chunk-30-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="analysis-of-variance-for-s.l.r." class="section level2">
<h2><span class="header-section-number">3.5</span> Analysis of variance (for s.l.r.)</h2>
<p>The analysis of variance is a method for comparing the fit of two or more models to the same dataset. It is particularly useful in multiple regression.</p>
<div id="anova-decomposition" class="section level3">
<h3><span class="header-section-number">3.5.1</span> ANOVA decomposition</h3>
<span class="math display">\[\begin{align*}
\mbox{data} &amp; = \mbox{fit} + \mbox{residual} \\
y_i &amp; = \hat{y}_i +e_i\\
y_i - \bar{y} &amp; = \hat{y}_i - \bar{y} +e_i \\
\sum_{i = 1}^n ( y_i - \bar{y})^2 &amp; =\sum_{i = 1}^n(\hat{y}_i - \bar{y} +e_i)^2\\
&amp; =\sum_{i = 1}^n(\hat{y}_i - \bar{y})^2 +\sum_{i = 1}^ne_i^2 + 2\sum_{i = 1}^n (\hat{y}_i - \bar{y})e_i
\end{align*}\]</span>
<p>The last term is zero because (from normal equations):</p>
<p><span class="math inline">\(\sum_{i = 1}^n\hat{y}_ie_i = 0\)</span> and <span class="math inline">\(\sum_{i = 1}^ne_i = 0\)</span>.</p>
<p>The decomposition:</p>
<p><span class="math display">\[\sum_{i = 1}^n ( y_i - \bar{y})^2 =\sum_{i = 1}^n(\hat{y}_i - \bar{y})^2 +\sum_{i = 1}^ne_i^2\]</span></p>
<p>is called the <strong>ANOVA decomposition</strong>.</p>
<p>These calculations are summarised in the <strong>ANOVA table</strong>.</p>
</div>
<div id="anova-table" class="section level3">
<h3><span class="header-section-number">3.5.2</span> ANOVA table</h3>
<p>In general, an ANOVA table is a method for partitioning variability in a response variable into what is explained by the model fitted and what is left over.</p>
<p>The exact form of the ANOVA table will depend on the model that has been fitted.</p>
<p>The hypothesis being tested by the model will also depend on the model that has been fitted.</p>
<p>An ANOVA table for the simple linear regression model:</p>
<table>
<thead>
<tr class="header">
<th align="left">SOURCE</th>
<th align="left">df</th>
<th align="left">SS</th>
<th align="left">MS</th>
<th align="left">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left">1</td>
<td align="left">SSR</td>
<td align="left">MSR = SSR/1</td>
<td align="left">MSR/MSE</td>
</tr>
<tr class="even">
<td align="left">Error</td>
<td align="left">n-2</td>
<td align="left">SSE</td>
<td align="left">MSE = SSE/(n-2)</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left">n-1</td>
<td align="left">SST</td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<!-- \begin{tabular}{l | r l l l } -->
<!-- Source & df & SS & MS & F\\ \hline -->
<!-- &&&& \\ -->
<!-- Regression & $1$ & $\mbox{SSR} = \sum_{i=1}^n (\hat{y}_i-\bar{y})^2$ & $\mbox{MSR} = \mbox{SSR}/1$& $\mbox{MSR}/\mbox{MSE}$\\ -->
<!-- &&&& \\ -->
<!-- Error & $n-2$ & \mbox{SSE} = $\sum_{i=1}^n (y_i-\hat{y}_i)^2$ &$\mbox{MSE} = \mbox{SSE}/(n-2)$ &\\ \hline -->
<!-- &&&& \\ -->
<!-- Total & $n-1$ & $\mbox{SST} = \sum_{i=1}^n (y_i-\bar{y})^2$ && -->
<!-- \end{tabular} -->
<p>NOTE:</p>
<ul>
<li><p>The <strong>total sum of squares</strong>, <span class="math inline">\(\mbox{SST} = \sum_{i=1}^{n}(y_{i} - \bar{y})^{2}\)</span> is the sum of squares of <span class="math inline">\(y\)</span> about the mean. The total sum of squares does not depend on <span class="math inline">\(x\)</span>. (NB: this is <span class="math inline">\(S_{yy}\)</span>)</p></li>
<li><p>The <strong>regression sum of squares</strong>, <span class="math inline">\(\mbox{SSR} = \sum_{i=1}^{n}(\hat{y}_{i} - \bar{y})^{2}\)</span>. Note that <span class="math inline">\(\hat{y}_{i}\)</span> depends on <span class="math inline">\(x\)</span>.</p></li>
<li><p>The <strong>residual/error sum of squares</strong>, <span class="math inline">\(\mbox{SSE} = \sum_{i=1}^{n}e_{i}^{2} = \sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^{2}\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{SST} = \mbox{SSR} + \mbox{SSE}\)</span>.</p></li>
<li><p>The sums of squares have associated degrees of freedom (df). <!-- %The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter is equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself (i.e., the sample variance has N-1 degrees of freedom, since it is computed from N random scores minus the only 1 parameter estimated as intermediate step, which is the sample mean) --></p></li>
<li><p>MS = SS/df</p></li>
<li><p>The <strong>mean squared error</strong> <span class="math inline">\(\mbox{MSE}\)</span> estimates <span class="math inline">\(\sigma^{2}\)</span>.</p></li>
<li><p>The <strong>coefficient of determination</strong> is: <span class="math display">\[R^{2} = \frac{\mbox{SSR}}{\mbox{SST}} = 1 - \frac{\mbox{SSE}}{\mbox{SST}}.\]</span></p></li>
<li><p><span class="math inline">\(R^{2}\)</span> is always between 0 and 1 and it measures the proportion of variation in <span class="math inline">\(y\)</span> that is explained by regression with <span class="math inline">\(x\)</span>.</p></li>
</ul>
</div>
<div id="special-cases-1" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Special cases</h3>
<ul>
<li><span class="math inline">\(R^{2}=1\)</span>, <span class="math inline">\(\mbox{SSR} = \mbox{SST}\)</span>, <span class="math inline">\(\mbox{SSE}\)</span> = 0.</li>
</ul>
<p><span class="math inline">\(e_{i}=0\)</span>, <span class="math inline">\(i = 1, \dots,n\)</span>, data fall on a straight line.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-32-1.png" width="288" style="display: block; margin: auto;" /></p>
<ul>
<li><span class="math inline">\(R^{2}=0\)</span>, <span class="math inline">\(\mbox{SSR} = 0\)</span>, <span class="math inline">\(\mbox{SSE} = \mbox{SST}\)</span>.</li>
</ul>
<p><span class="math inline">\(\hat{y}_{i}=\bar{y}\)</span>, <span class="math inline">\(\hat{\beta}_1=0\)</span>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-33-1.png" width="288" style="display: block; margin: auto;" /></p>
</div>
<div id="does-regression-on-x-explain-y" class="section level3">
<h3><span class="header-section-number">3.5.4</span> Does regression on x explain y?</h3>
<p>In simple linear regression this amounts to testing:</p>
<span class="math display">\[\begin{align*}
H_0&amp;:&amp; \beta_1 = 0\\
H_A&amp;:&amp; \beta_1 \ne 0
\end{align*}\]</span>
<p>We can use a t-test for this, but there is an equivalent test based on the F distribution. As we will see later, F-tests have a wide range of applications.</p>
<p>If <span class="math inline">\(H_0\)</span> holds, then <span class="math inline">\(\mbox{SSR}\)</span> is small and <span class="math inline">\(\mbox{SSE}\)</span> large. Therefore large values of <span class="math inline">\(\mbox{SSR}\)</span> relative to <span class="math inline">\(\mbox{SSE}\)</span> provide evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>The F-statistic is:</p>
<p><span class="math display">\[F=\frac{\mbox{SSR}/df_R}{\mbox{SSE}/df_E}.\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(df_R=1\)</span> is the degrees of freedom of <span class="math inline">\(\mbox{SSR}\)</span> and</li>
<li><span class="math inline">\(df_E=n-2\)</span> is the degrees of freedom of <span class="math inline">\(\mbox{SSE}\)</span>.</li>
</ul>
<p>Under <span class="math inline">\(H_0\)</span>,<span class="math inline">\(F \sim F_{1,n-2}\)</span>.</p>
<p>By dividing each SS by the <span class="math inline">\(df\)</span> we put them on a common scale, so that if <span class="math inline">\(H_0\)</span> is true:</p>
<p><span class="math display">\[\mbox{SSR}/1 \approx \mbox{SSE}/(n-2)\]</span> and <span class="math display">\[F_{obs}\approx 1.\]</span></p>
<p>Large values of <span class="math inline">\(F_{obs}\)</span> provide evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>P-value: <span class="math inline">\(P( F_{1,n-2} \geq F_{obs})\)</span>.</p>
</div>
<div id="notes-on-the-anova-table" class="section level3">
<h3><span class="header-section-number">3.5.5</span> Notes on the ANOVA table</h3>
<ul>
<li><span class="math inline">\(\mathbb{E}[\mbox{MSR}] = \sigma^2 + \beta_1^2 S_{xx}\)</span>.</li>
</ul>
<p>Proof:</p>
<span class="math display">\[\begin{align*}
\mbox{MSR} &amp; = \sum_{i = 1}^n (\hat{y}_i - \bar{y})^2/1\\
&amp; = \sum_{i = 1}^n (\hat{\beta}_0 + \hat{\beta}_1x_i - \bar{y})^2\\
&amp; = \sum_{i = 1}^n (\bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1x_i - \bar{y})^2\\
&amp; =\hat{\beta}_1^2\sum_{i = 1}^n (x_i - \bar{x})^2\\
&amp; =\hat{\beta}_1^2 S_{xx}
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
\mathbb{E}[\mbox{MSR}] &amp; = \mathbb{E}[\hat{\beta}_1^2 S_{xx}]\\
&amp; =S_{xx} \mathbb{E}[\hat{\beta}_1^2]\\
&amp; = S_{xx} \left(\mbox{Var}(\hat{\beta}_1) + \mathbb{E}[\hat{\beta}_1]^2 \right)\\
&amp; = S_{xx} \left(\frac{\sigma^2}{S_{xx}} + \beta_1^2 \right)\\
&amp; = \sigma^2 + \beta_1^2 S_{xx}
\end{align*}\]</span>
<ul>
<li><span class="math inline">\(\mathbb{E}[\mbox{MSE}] = \sigma^2\)</span>.</li>
</ul>
<p>Proof:</p>
<span class="math display">\[\begin{align*}
\mbox{MSE} &amp; = \frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n-2}\\
&amp; = \frac{\sum_{i = 1}^ne_i^2}{n-2} 
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
\mathbb{E}[\mbox{MSE}] &amp; = \frac{1}{n-2} \mathbb{E}\left[\sum_{i = 1}^ne_i^2 \right]\\
 &amp; = \frac{1}{n-2} \sum_{i = 1}^n\mathbb{E}[e_i^2]\\
 &amp; = \frac{1}{n-2} \sum_{i = 1}^n\left( \mbox{Var}(e_i) + \mathbb{E}[e_i]^2 \right)
\end{align*}\]</span>
<p>NOTE: <span class="math inline">\(\mathbb{E}[\epsilon_i] = 0\)</span>, <span class="math inline">\(\mbox{Var}(\epsilon_i) = \sigma^2\)</span>, but <span class="math inline">\(\mathbb{E}[e_i] = 0\)</span>, <span class="math inline">\(\mbox{Var}(e_i)= \sigma^2(1- h_{ii})\)</span>. We will revisit this later in the course.</p>
<span class="math display">\[\begin{align*}
\mathbb{E}[\mbox{MSE}] &amp; = \frac{1}{n-2} \sum_{i = 1}^n \left( \sigma^2( 1 - h_{ii}) + 0 \right)\\
&amp; =\frac{1}{n-2} \sum_{i = 1}^n \sigma^2 \left(1-\left(\frac{1}{n} +\frac{(x_i - \bar{x})^2}{S_{xx}}\right)\right)\\
 &amp; =\frac{1}{n-2} \sum_{i = 1}^n \left(\sigma^2 - \frac{\sigma^2}{n} -\frac{\sigma^2 (x_i - \bar{x})^2}{S_{xx}}\right)\\
&amp; =\frac{1}{n-2} \left(\sigma^2 n- \sigma^2 -\frac{\sigma^2 }{S_{xx}}\sum_{i = 1}^n(x_i - \bar{x})^2\right)\\
 &amp; = \frac{1}{n-2} \left((n-2) \sigma^2\right)\\
 &amp; =\sigma^2 
\end{align*}\]</span>
<ul>
<li><p>Under the <span class="math inline">\(H_0\)</span>, <span class="math inline">\(\beta_1 = 0\)</span> and then <span class="math inline">\(\mathbb{E}[\mbox{MSE}] = \mathbb{E}[\mbox{MSR}]\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{MSE} = \hat{\sigma}^2\)</span> can be computed using the formula:</p></li>
</ul>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{S_{yy} - \hat{\beta}_1^2S_{xx}}{n-2}.\]</span></p>
</div>
</div>
<div id="sample-correlation" class="section level2">
<h2><span class="header-section-number">3.6</span> Sample correlation</h2>
<p>The relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be examined using a scatterplot <span class="math inline">\((x_i, y_i)\)</span>.</p>
<p>Sample correlation measures the strength and direction of the linear association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It is defined as:</p>
<p><span class="math display">\[r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}\]</span></p>
<p><span class="math inline">\(r\)</span> is the estimate of the population correlation (<span class="math inline">\(\rho\)</span>) between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div id="connection-between-correlation-and-regression" class="section level4">
<h4><span class="header-section-number">3.6.0.1</span> Connection between correlation and regression:</h4>
<ul>
<li><p><span class="math inline">\(\hat{\beta}_1=\sqrt{\frac{\mbox{SST}}{S_{xx}}}r=\frac{s_y}{s_x}r\)</span> where <span class="math inline">\(s_y\)</span> and <span class="math inline">\(s_x\)</span> are the standard deviations of the <span class="math inline">\(y_i\)</span>’s and <span class="math inline">\(x_i\)</span>’s. Note that if you change the role of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> the resulting regression line would have slope <span class="math inline">\(\frac{s_x}{s_y}r\)</span>.</p></li>
<li><p><span class="math inline">\(r^2 =R^2\)</span> the coefficient of determination.</p></li>
<li><p>In the SLR model variable <span class="math inline">\(x\)</span> is treated as <strong>fixed</strong> and <span class="math inline">\(y\)</span> and <span class="math inline">\(\epsilon\)</span> are <strong>random</strong>. It is convenient to think of the predictor variable as fixed even if it israndom as we are interested in the behaviour of <span class="math inline">\(y\)</span> at various fixed <span class="math inline">\(x\)</span> values.</p></li>
<li><p>One can calculate <span class="math inline">\(r\)</span> for any pair of variables (see next page), but correlation assumes that variables are bivariately normally distributed.</p></li>
<li><p>Whereas correlation treats both variables symmetrically, in regression, the exploratory variable is used to explain or predict the response variable.</p></li>
</ul>
<p>Father and son heights (Galton data)</p>
<p><img src="_main_files/figure-html/unnamed-chunk-34-1.png" width="480" style="display: block; margin: auto;" /></p>
<p><img src="_main_files/figure-html/unnamed-chunk-35-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="examples-of-correlation" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Examples of correlation</h3>
<p><img src="_main_files/figure-html/unnamed-chunk-36-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Anscombe data</p>
<p><img src="_main_files/figure-html/unnamed-chunk-37-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>In all graphs below, correlation is <span class="math inline">\(r = -0.06\)</span>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-38-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="comparison-of-the-correlation-and-coefficient-of-determination-for-two-data-sets." class="section level3">
<h3><span class="header-section-number">3.6.2</span> Comparison of the correlation and coefficient of determination for two data sets.</h3>
<p><img src="_main_files/figure-html/unnamed-chunk-39-1.png" width="624" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(X1,Y1)<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.6699889</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(Y1 <span class="op">~</span>X1))[<span class="dv">8</span>]</code></pre></div>
<pre><code>## $r.squared
## [1] 0.6699889</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(X2,Y2)<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>## [1] 0.6895371</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(Y2 <span class="op">~</span>X2))[<span class="dv">8</span>]</code></pre></div>
<pre><code>## $r.squared
## [1] 0.6895371</code></pre>
</div>
</div>
<div id="assessing-the-simple-linear-regression-model-assumptions" class="section level2">
<h2><span class="header-section-number">3.7</span> Assessing the simple linear regression model assumptions</h2>
<div id="assumptions-1" class="section level3">
<h3><span class="header-section-number">3.7.1</span> Assumptions</h3>
<p>In the SLR model, we assume that <span class="math inline">\(y_{i} \sim\)</span> N(<span class="math inline">\(\beta_{0} + \beta_{1}x_{i}, \sigma^{2}\)</span>) and that the <span class="math inline">\(y_{i}\)</span>’s are independent.</p>
<p>Equivalently, since <span class="math inline">\(\epsilon_{i} = y_{i} - \beta_{0} - \beta_{1}x_{i}\)</span>, the SLR model assumes that <span class="math inline">\(\epsilon_{i} \sim\)</span> N(0, <span class="math inline">\(\sigma^{2}\)</span>) and the <span class="math inline">\(\epsilon_{i}\)</span>’s are independent and identically distributed.</p>
<p>We want to check the following:</p>
<ol style="list-style-type: decimal">
<li><p>There is a <strong>linear relationship</strong>, i.e. <span class="math inline">\(\mathbb{E}\)</span>[<span class="math inline">\(y_{i}\)</span>] = <span class="math inline">\(\beta_{0} + \beta_{1}x_{i}\)</span>. If the data do not follow a linear relationship then the simple linear regression model is not appropriate.</p></li>
<li><p>The <span class="math inline">\(\epsilon_{i}\)</span>’s have a <strong>constant variance</strong>, i.e. Var(<span class="math inline">\(\epsilon_{i}\)</span>) = <span class="math inline">\(\sigma^{2}\)</span> for all <span class="math inline">\(i\)</span>. If there is not constant variance, the line will summarise the data okay but the parameter estimate standard errors, estimates of <span class="math inline">\(\sigma\)</span> etc, are all based on incorrect assumptions.</p></li>
<li><p>The <span class="math inline">\(\epsilon_{i}\)</span>’s are <strong>independent</strong>.</p></li>
<li><p>The <span class="math inline">\(\epsilon_{i}\)</span>’s are <strong>normally distributed</strong> (with mean 0).</p></li>
</ol>
</div>
<div id="violations-and-consequences" class="section level3">
<h3><span class="header-section-number">3.7.2</span> Violations and consequences</h3>
<ol style="list-style-type: decimal">
<li>Linearity:</li>
</ol>
<ul>
<li>A straight linear relationship may be inadequate.</li>
<li>A straight linear relationship may only be appropriate for most of the data.</li>
<li>When linearity is violated least squares estimates can be biased and standard errors may be inaccurate.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Constant variance:</li>
</ol>
<ul>
<li>When the <strong>variance is not constant</strong> least squares estimate are unbiased but standard errors are inaccurate.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Independence:</li>
</ol>
<ul>
<li>When there is a <strong>lack of independence</strong> least squares estimates are unbiased but standard errors are seriously affected.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Normality:</li>
</ol>
<ul>
<li><strong>Violations of normality</strong> do not have much impact on estimates and standard errors.</li>
<li>Tests and C.I.’s are not usually seriously affected because of the C.L.T.</li>
</ul>
<!-- ### Hypothetical graphs for s.l.r assumption violations -->
<!-- Display below is from @ramsey2002statistical, Section 8.3, page 213.  -->
<!-- ```{r echo = FALSE} -->
<!-- knitr::include_graphics("Graphic1-9.jpg") -->
<!-- ``` -->
</div>
<div id="graphical-tools-for-assessment" class="section level3">
<h3><span class="header-section-number">3.7.3</span> Graphical tools for assessment</h3>
<ol style="list-style-type: decimal">
<li>Plot of <span class="math inline">\(y_i\)</span> versus <span class="math inline">\(x_i\)</span>. If satisfactory use simple linear regression.</li>
</ol>
<p>Sometimes the patterns in the plot of <span class="math inline">\(y_i\)</span> versus <span class="math inline">\(x_i\)</span> are difficult to detect because of the total variability of the response variable is much larger than the variability around the regression line. Scatterplots of residuals vs fits are better at finding patterns because the linear component of the variation in the responses has been removed, leaving a clearer picture about curvature and spread. The plot alerts the user of nonlinearity, non-constant variance and the presence of outliers.</p>
<ol start="2" style="list-style-type: decimal">
<li>Plot of <span class="math inline">\(e_i\)</span> versus <span class="math inline">\(\hat{y}_i\)</span> (or <span class="math inline">\(x_i\)</span>).</li>
</ol>
<ul>
<li>If satisfactory use simple linear regression.</li>
<li>If linearity is violated but <span class="math inline">\(\mathbb{E}[y]\)</span> is monotonic in <span class="math inline">\(x\)</span> and <span class="math inline">\(\mbox{Var}(y)\)</span> is constant, try transforming <span class="math inline">\(x\)</span> and then use simple linear regression.</li>
<li>If linearity is violated and <span class="math inline">\(\mathbb{E}[y]\)</span> is not monotonic, try quadratic regression <span class="math inline">\(\mathbb{E}[y] = \beta_0+\beta_1 x+\beta_2 x^2\)</span> (we will look at this later).</li>
<li>If linearity is violated and <span class="math inline">\(\mbox{Var}(y)\)</span> increases with <span class="math inline">\(\mathbb{E}[y]\)</span>, try transforming y and then use simple linear regression.</li>
<li>If the distribution of <span class="math inline">\(y\)</span> about <span class="math inline">\(\mathbb{E}[y]\)</span> is skewed, i.e. non-normal, then use simple linear regression but report skewness.</li>
<li>If linearity is not violated but <span class="math inline">\(\mbox{Var}(y)\)</span> increases with <span class="math inline">\(\mathbb{E}[y]\)</span>, use weighted regression (we will look at this later).</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Normal probability plot</li>
</ol>
<ul>
<li><p>The model assumes normality of <span class="math inline">\(y\)</span> about <span class="math inline">\(\mathbb{E}[y]\)</span>, or, equivalently, normality of <span class="math inline">\(\epsilon\)</span>.</p></li>
<li><p>The residuals <span class="math inline">\(e_i\)</span> approximate <span class="math inline">\(\epsilon\)</span> and should therefore have a normal distribution.</p></li>
<li><p>The normal probability (quantile) plot is a plot of <span class="math inline">\(z_i\)</span> versus <span class="math inline">\(e_i\)</span>, where <span class="math inline">\(z_i\)</span> are quantiles from the standard normal distribution.</p></li>
<li><p>This plot should roughly follow a straight line pattern.</p></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Residuals vs. time order</li>
</ol>
<ul>
<li><p>If the data are collected over time, serial correlation or a general time trend may occur.</p></li>
<li><p>A plot of <span class="math inline">\(e_i\)</span> vs. <span class="math inline">\(t_i\)</span> (time of the i<span class="math inline">\(^{th}\)</span> observation) may be examined for patterns.</p></li>
</ul>
<p>Everytime you use SLR you should also draw graphs 1) to 3). Also plot 4) when appropriate.</p>
</div>
<div id="cigarette" class="section level3">
<h3><span class="header-section-number">3.7.4</span> Cigarette Data</h3>
<p>FDA data on cigarettes, response is carbon monoxide, predictor is nicotine. Data from <span class="citation">McIntyre (<a href="#ref-mcintyre94">1994</a>)</span>.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-41-1.png" width="384" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = carbon.monoxide ~ nicotine)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3273 -1.2228  0.2304  1.2700  3.9357 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.6647     0.9936   1.675    0.107    
## nicotine     12.3954     1.0542  11.759 3.31e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.828 on 23 degrees of freedom
## Multiple R-squared:  0.8574, Adjusted R-squared:  0.8512 
## F-statistic: 138.3 on 1 and 23 DF,  p-value: 3.312e-11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#studres(fit)</span></code></pre></div>
<p>To assess the fit construct residual plots</p>
<p><img src="_main_files/figure-html/unnamed-chunk-43-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>Plot 1: residuals increasing with the fit, non constant variance.</p>
<p>Plot 2: no indication that the assumption of Normality is unreasonable.</p>
<p>There are 3 unusual observations: 3, 19, 25. Obs 3 has a large negative residual. It is the point on the upper right of the fitted line plot. It is a high influence point, meaning it has a big effect on the fitted line obtained. Obs 19 and 25 have large positive residuals.</p>
<p>We could attempt to improve the fit, refit the model without observation 3.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-44-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Diagnostic plots:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-45-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>Plot 1: no linear pattern, but some hint of non-constant variance.</p>
<div id="transformations" class="section level4">
<h4><span class="header-section-number">3.7.4.1</span> Transformations:</h4>
<p>How can we pick the best transformation?</p>
<ul>
<li>Examine the fitted line plot: linearity, constant variance.</li>
<li>Examine the residual vs fit plot: no relationship, constant variance, no outliers.</li>
<li>Check the normality of residuals.</li>
<li>Check for sensitivity: whether fit would change substantially if extreme points are removed.</li>
<li>One can also compare <span class="math inline">\(R^2\)</span> as long as the <span class="math inline">\(y\)</span> values are on the same scale. Furthermore, <span class="math inline">\(R^2\)</span> doesn’t follow a distribution, so we can’t compare <span class="math inline">\(R^2\)</span> in two models and know that one is meaningfully better.</li>
</ul>
<p>Note: interpretation changes after transformations.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-46-1.png" width="624" style="display: block; margin: auto;" /></p>
<ul>
<li>Row 1: delete outlier</li>
<li>Row 2: use a square root transformation for the predictor. This diminishes the influence of the outlier. The residual plot hints at a small amount of bias.</li>
<li>Row 3: take square root transformations of both the response and the predictor.</li>
<li>Row 4: take log transformations of both the response and the predictor.</li>
</ul>
</div>
</div>
</div>
<div id="a-note-on-the-galton-paradox" class="section level2">
<h2><span class="header-section-number">3.8</span> A note on the Galton paradox</h2>
<!-- ###History of regression -->
<!-- * A. De Moivre (1733) -  the first statement of an approximation to the binomial distribution in terms of the normal (or Gaussian) function. -->
<!-- * A.M. Legendre. Nouvelles m\'ethodes pour la d\'etermination des orbites des com\`etes, (1805). Sur la M\'ethode des moindres carr\'es is an appendix. No link to the theory of probability  Firmin Didot, Paris, -->
<!-- * C.F. Gauss. Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientum. (1809) - connected the method of least squares with the principles of probability and to the normal distribution (which he invented in the process). He specified a mathematical form of the probability density for the observations and defined a method of estimation that minimizes the error of estimation. Gauss showed that if the errors are normally distributed, the least squares estimates give the most probable values for the coefficients. -->
<!-- * S. Laplace (1810-1811)  - proved CLT (motivated by Gauss). Used CLT to give a large sample justification for the method of LS and the normal distribution. -->
<!-- * C.F. Gauss. Theoria combinationis observationum erroribus minimis obnoxiae. (1821/1823) - in a linear model where the errors have a mean of zero, are uncorrelated, and have equal variances, the best linear unbiased estimator of the coefficients is the least-squares estimator. -->
<!-- * R. Adrain (1808) Research concerning the probabilities of the errors which happen in making observations, etc. Independently formulated LS working in survey measurements. -->
<!-- * F. Galton (1886). Regression Towards Mediocrity in Hereditary Stature. - correlation. -->
<!-- ```{r echo = FALSE} -->
<!-- knitr::include_graphics("deMoivre.jpg") -->
<!-- knitr::include_graphics("Legendre.jpg") -->
<!-- knitr::include_graphics("Gauss.jpg") -->
<!-- knitr::include_graphics("Laplace.jpg") -->
<!-- knitr::include_graphics("Adrain.jpg") -->
<!-- knitr::include_graphics("Galton.jpg") -->
<!-- ``` -->
<div id="the-galton-paradox" class="section level3">
<h3><span class="header-section-number">3.8.1</span> The Galton paradox</h3>
<p>Sons are on average taller than their fathers (by 1 inch approx)</p>
<p><img src="_main_files/figure-html/unnamed-chunk-47-1.png" width="288" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">apply</span>(father.son, <span class="dv">2</span>, mean)</code></pre></div>
<pre><code>##   father      son 
## 67.68683 68.68423</code></pre>
<p>Taller than average fathers have taller than average sons. Regression towards the mean: although the above is true, for these tall people, the son’s height was on average less than the father’s.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-49-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>The suggestion is that each generation would have offspring more near average than the previous generation and that over many generations the offspring would be of uniform heigth. However, the observations showed the sons as variable as the fathers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">apply</span>(father.son, <span class="dv">2</span>, sd)</code></pre></div>
<pre><code>##   father      son 
## 2.745827 2.816194</code></pre>
<p>An apparent paradox?</p>
</div>
<div id="two-regressions" class="section level3">
<h3><span class="header-section-number">3.8.2</span> Two regressions</h3>
<p>Regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>, treats <span class="math inline">\(x\)</span> variable as fixed and only vertical distances are minimized. Howevever, regressing <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>, i.e. trying to predict the fathers’ heights from their sons’ treats the sons’ heights <span class="math inline">\(y\)</span> as fixed and the least squares criterion minimizes the horizontal distances.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-51-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="regression-vs-orthogonal-regression" class="section level3">
<h3><span class="header-section-number">3.8.3</span> Regression vs orthogonal regression</h3>
<p><img src="_main_files/figure-html/unnamed-chunk-52-1.png" width="624" style="display: block; margin: auto;" /></p>
<p><img src="_main_files/figure-html/unnamed-chunk-53-1.png" width="624" style="display: block; margin: auto;" /></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-mcintyre94">
<p>McIntyre, Lauren. 1994. “Using Cigarette Data for an Introduction to Multiple Regression.” <em>Journal of Statistics Education</em> 2 (1). Taylor &amp; Francis: null. doi:<a href="https://doi.org/10.1080/10691898.1994.11910468">10.1080/10691898.1994.11910468</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
